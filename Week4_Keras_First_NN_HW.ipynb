{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "import os\n",
    "#url = \"https://www.kaggle.com/uciml/pima-indians-diabetes-database\"  #\"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "#diabetes_df = pd.read_csv(url, names=names)\n",
    "diabetes_df = pd.read_csv('diabetes.csv')\n",
    "diabetes_df.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>5</td>\n",
       "      <td>168</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.135</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>10</td>\n",
       "      <td>148</td>\n",
       "      <td>84</td>\n",
       "      <td>48</td>\n",
       "      <td>237</td>\n",
       "      <td>37.6</td>\n",
       "      <td>1.001</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.803</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0.551</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.491</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "404               5                     168              64               0   \n",
       "458              10                     148              84              48   \n",
       "394               4                     158              78               0   \n",
       "671               1                      99              58              10   \n",
       "50                1                     103              80              11   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "404        0  32.9              0.135   41             1  \n",
       "458      237  37.6              1.001   51             1  \n",
       "394        0  32.9              0.803   31             1  \n",
       "671        0  25.4              0.551   21             0  \n",
       "50        82  19.4              0.491   22             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.760\n",
      "roc-auc is 0.820\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional    # maybe because it is 9 column.... 8 x and 1 y\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"sigmoid\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 390us/step - loss: 0.5538 - acc: 0.7014 - val_loss: 0.5650 - val_acc: 0.7083\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5534 - acc: 0.7031 - val_loss: 0.5646 - val_acc: 0.7083\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5530 - acc: 0.7031 - val_loss: 0.5642 - val_acc: 0.7083\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5526 - acc: 0.7031 - val_loss: 0.5639 - val_acc: 0.7135\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5522 - acc: 0.7049 - val_loss: 0.5635 - val_acc: 0.7135\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5518 - acc: 0.7049 - val_loss: 0.5631 - val_acc: 0.7135\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5514 - acc: 0.7049 - val_loss: 0.5627 - val_acc: 0.7188\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5511 - acc: 0.7049 - val_loss: 0.5624 - val_acc: 0.7240\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5507 - acc: 0.7083 - val_loss: 0.5620 - val_acc: 0.7292\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5503 - acc: 0.7083 - val_loss: 0.5616 - val_acc: 0.7292\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5499 - acc: 0.7101 - val_loss: 0.5613 - val_acc: 0.7292\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5496 - acc: 0.7083 - val_loss: 0.5609 - val_acc: 0.7396\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5492 - acc: 0.7118 - val_loss: 0.5605 - val_acc: 0.7396\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5488 - acc: 0.7083 - val_loss: 0.5602 - val_acc: 0.7448\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5484 - acc: 0.7101 - val_loss: 0.5598 - val_acc: 0.7448\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5481 - acc: 0.7118 - val_loss: 0.5595 - val_acc: 0.7448\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5477 - acc: 0.7135 - val_loss: 0.5591 - val_acc: 0.7448\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5473 - acc: 0.7118 - val_loss: 0.5588 - val_acc: 0.7448\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5470 - acc: 0.7153 - val_loss: 0.5584 - val_acc: 0.7448\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5466 - acc: 0.7170 - val_loss: 0.5581 - val_acc: 0.7448\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5462 - acc: 0.7170 - val_loss: 0.5577 - val_acc: 0.7448\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5459 - acc: 0.7170 - val_loss: 0.5574 - val_acc: 0.7500\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5455 - acc: 0.7170 - val_loss: 0.5570 - val_acc: 0.7500\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5451 - acc: 0.7135 - val_loss: 0.5567 - val_acc: 0.7500\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5448 - acc: 0.7135 - val_loss: 0.5563 - val_acc: 0.7500\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5444 - acc: 0.7170 - val_loss: 0.5560 - val_acc: 0.7448\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5441 - acc: 0.7153 - val_loss: 0.5557 - val_acc: 0.7448\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5438 - acc: 0.7170 - val_loss: 0.5553 - val_acc: 0.7448\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5434 - acc: 0.7153 - val_loss: 0.5550 - val_acc: 0.7448\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5431 - acc: 0.7170 - val_loss: 0.5547 - val_acc: 0.7448\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5427 - acc: 0.7170 - val_loss: 0.5543 - val_acc: 0.7396\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5424 - acc: 0.7170 - val_loss: 0.5540 - val_acc: 0.7396\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5420 - acc: 0.7170 - val_loss: 0.5537 - val_acc: 0.7396\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5417 - acc: 0.7170 - val_loss: 0.5534 - val_acc: 0.7396\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5413 - acc: 0.7170 - val_loss: 0.5530 - val_acc: 0.7448\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5410 - acc: 0.7170 - val_loss: 0.5527 - val_acc: 0.7448\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5407 - acc: 0.7170 - val_loss: 0.5524 - val_acc: 0.7448\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5403 - acc: 0.7188 - val_loss: 0.5521 - val_acc: 0.7500\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5400 - acc: 0.7222 - val_loss: 0.5518 - val_acc: 0.7552\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5397 - acc: 0.7240 - val_loss: 0.5514 - val_acc: 0.7552\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5393 - acc: 0.7240 - val_loss: 0.5511 - val_acc: 0.7552\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5390 - acc: 0.7257 - val_loss: 0.5508 - val_acc: 0.7552\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5387 - acc: 0.7292 - val_loss: 0.5505 - val_acc: 0.7552\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5384 - acc: 0.7292 - val_loss: 0.5502 - val_acc: 0.7552\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5380 - acc: 0.7292 - val_loss: 0.5499 - val_acc: 0.7552\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5377 - acc: 0.7292 - val_loss: 0.5496 - val_acc: 0.7552\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5374 - acc: 0.7292 - val_loss: 0.5493 - val_acc: 0.7552\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5371 - acc: 0.7309 - val_loss: 0.5490 - val_acc: 0.7552\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5368 - acc: 0.7309 - val_loss: 0.5487 - val_acc: 0.7604\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5365 - acc: 0.7309 - val_loss: 0.5484 - val_acc: 0.7656\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5361 - acc: 0.7344 - val_loss: 0.5481 - val_acc: 0.7656\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5358 - acc: 0.7361 - val_loss: 0.5478 - val_acc: 0.7656\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5355 - acc: 0.7361 - val_loss: 0.5475 - val_acc: 0.7656\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5352 - acc: 0.7361 - val_loss: 0.5472 - val_acc: 0.7656\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5349 - acc: 0.7378 - val_loss: 0.5469 - val_acc: 0.7656\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5346 - acc: 0.7361 - val_loss: 0.5466 - val_acc: 0.7656\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5343 - acc: 0.7396 - val_loss: 0.5463 - val_acc: 0.7604\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5339 - acc: 0.7396 - val_loss: 0.5460 - val_acc: 0.7604\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5337 - acc: 0.7396 - val_loss: 0.5457 - val_acc: 0.7604\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5333 - acc: 0.7396 - val_loss: 0.5455 - val_acc: 0.7604\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 49us/step - loss: 0.5330 - acc: 0.7396 - val_loss: 0.5452 - val_acc: 0.7604\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5327 - acc: 0.7396 - val_loss: 0.5449 - val_acc: 0.7656\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5325 - acc: 0.7396 - val_loss: 0.5446 - val_acc: 0.7656\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5321 - acc: 0.7396 - val_loss: 0.5443 - val_acc: 0.7656\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.5319 - acc: 0.7431 - val_loss: 0.5441 - val_acc: 0.7656\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5316 - acc: 0.7413 - val_loss: 0.5438 - val_acc: 0.7656\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5313 - acc: 0.7431 - val_loss: 0.5435 - val_acc: 0.7656\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5310 - acc: 0.7431 - val_loss: 0.5432 - val_acc: 0.7656\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5307 - acc: 0.7448 - val_loss: 0.5430 - val_acc: 0.7656\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5304 - acc: 0.7448 - val_loss: 0.5427 - val_acc: 0.7656\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5301 - acc: 0.7465 - val_loss: 0.5424 - val_acc: 0.7656\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5298 - acc: 0.7465 - val_loss: 0.5421 - val_acc: 0.7604\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5295 - acc: 0.7465 - val_loss: 0.5419 - val_acc: 0.7604\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5292 - acc: 0.7483 - val_loss: 0.5416 - val_acc: 0.7604\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5290 - acc: 0.7483 - val_loss: 0.5414 - val_acc: 0.7604\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5287 - acc: 0.7483 - val_loss: 0.5411 - val_acc: 0.7604\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5284 - acc: 0.7483 - val_loss: 0.5408 - val_acc: 0.7604\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5281 - acc: 0.7500 - val_loss: 0.5406 - val_acc: 0.7604\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5278 - acc: 0.7500 - val_loss: 0.5403 - val_acc: 0.7604\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5276 - acc: 0.7500 - val_loss: 0.5401 - val_acc: 0.7604\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5273 - acc: 0.7483 - val_loss: 0.5398 - val_acc: 0.7604\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5270 - acc: 0.7483 - val_loss: 0.5395 - val_acc: 0.7656\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5268 - acc: 0.7483 - val_loss: 0.5393 - val_acc: 0.7656\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5265 - acc: 0.7483 - val_loss: 0.5390 - val_acc: 0.7604\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5262 - acc: 0.7483 - val_loss: 0.5388 - val_acc: 0.7604\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5259 - acc: 0.7483 - val_loss: 0.5385 - val_acc: 0.7604\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5257 - acc: 0.7500 - val_loss: 0.5383 - val_acc: 0.7604\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5254 - acc: 0.7500 - val_loss: 0.5380 - val_acc: 0.7604\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5251 - acc: 0.7517 - val_loss: 0.5378 - val_acc: 0.7604\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5249 - acc: 0.7517 - val_loss: 0.5375 - val_acc: 0.7604\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5246 - acc: 0.7535 - val_loss: 0.5373 - val_acc: 0.7604\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5243 - acc: 0.7517 - val_loss: 0.5371 - val_acc: 0.7604\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5241 - acc: 0.7517 - val_loss: 0.5368 - val_acc: 0.7604\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5238 - acc: 0.7500 - val_loss: 0.5366 - val_acc: 0.7604\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5236 - acc: 0.7500 - val_loss: 0.5363 - val_acc: 0.7604\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5233 - acc: 0.7517 - val_loss: 0.5361 - val_acc: 0.7604\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5231 - acc: 0.7535 - val_loss: 0.5359 - val_acc: 0.7656\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5228 - acc: 0.7535 - val_loss: 0.5356 - val_acc: 0.7656\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5225 - acc: 0.7517 - val_loss: 0.5354 - val_acc: 0.7656\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5223 - acc: 0.7535 - val_loss: 0.5352 - val_acc: 0.7656\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5220 - acc: 0.7517 - val_loss: 0.5349 - val_acc: 0.7656\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5218 - acc: 0.7535 - val_loss: 0.5347 - val_acc: 0.7656\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5215 - acc: 0.7552 - val_loss: 0.5345 - val_acc: 0.7656\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5213 - acc: 0.7552 - val_loss: 0.5343 - val_acc: 0.7656\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5210 - acc: 0.7569 - val_loss: 0.5340 - val_acc: 0.7656\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5208 - acc: 0.7587 - val_loss: 0.5338 - val_acc: 0.7656\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5205 - acc: 0.7587 - val_loss: 0.5336 - val_acc: 0.7656\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5203 - acc: 0.7622 - val_loss: 0.5334 - val_acc: 0.7656\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5201 - acc: 0.7622 - val_loss: 0.5331 - val_acc: 0.7656\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5198 - acc: 0.7622 - val_loss: 0.5329 - val_acc: 0.7656\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5196 - acc: 0.7622 - val_loss: 0.5327 - val_acc: 0.7656\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5193 - acc: 0.7622 - val_loss: 0.5325 - val_acc: 0.7656\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5191 - acc: 0.7604 - val_loss: 0.5323 - val_acc: 0.7656\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5189 - acc: 0.7622 - val_loss: 0.5320 - val_acc: 0.7656\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5186 - acc: 0.7604 - val_loss: 0.5318 - val_acc: 0.7708\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5184 - acc: 0.7604 - val_loss: 0.5316 - val_acc: 0.7708\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5182 - acc: 0.7604 - val_loss: 0.5314 - val_acc: 0.7708\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5179 - acc: 0.7604 - val_loss: 0.5312 - val_acc: 0.7708\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5177 - acc: 0.7604 - val_loss: 0.5310 - val_acc: 0.7708\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5175 - acc: 0.7604 - val_loss: 0.5308 - val_acc: 0.7708\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.5172 - acc: 0.7587 - val_loss: 0.5306 - val_acc: 0.7708\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5170 - acc: 0.7587 - val_loss: 0.5304 - val_acc: 0.7708\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5168 - acc: 0.7587 - val_loss: 0.5301 - val_acc: 0.7708\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5165 - acc: 0.7587 - val_loss: 0.5299 - val_acc: 0.7708\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5163 - acc: 0.7587 - val_loss: 0.5297 - val_acc: 0.7708\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5161 - acc: 0.7587 - val_loss: 0.5295 - val_acc: 0.7708\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5159 - acc: 0.7587 - val_loss: 0.5293 - val_acc: 0.7708\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5156 - acc: 0.7587 - val_loss: 0.5291 - val_acc: 0.7708\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5155 - acc: 0.7587 - val_loss: 0.5289 - val_acc: 0.7708\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5152 - acc: 0.7604 - val_loss: 0.5287 - val_acc: 0.7708\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5150 - acc: 0.7604 - val_loss: 0.5285 - val_acc: 0.7708\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5148 - acc: 0.7604 - val_loss: 0.5283 - val_acc: 0.7708\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5145 - acc: 0.7622 - val_loss: 0.5281 - val_acc: 0.7708\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5143 - acc: 0.7639 - val_loss: 0.5280 - val_acc: 0.7708\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5141 - acc: 0.7639 - val_loss: 0.5278 - val_acc: 0.7708\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5139 - acc: 0.7639 - val_loss: 0.5276 - val_acc: 0.7708\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5137 - acc: 0.7639 - val_loss: 0.5274 - val_acc: 0.7708\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5135 - acc: 0.7639 - val_loss: 0.5272 - val_acc: 0.7708\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5133 - acc: 0.7656 - val_loss: 0.5270 - val_acc: 0.7708\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5131 - acc: 0.7639 - val_loss: 0.5268 - val_acc: 0.7708\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5129 - acc: 0.7639 - val_loss: 0.5266 - val_acc: 0.7708\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5126 - acc: 0.7656 - val_loss: 0.5264 - val_acc: 0.7708\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5124 - acc: 0.7639 - val_loss: 0.5262 - val_acc: 0.7708\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5122 - acc: 0.7656 - val_loss: 0.5261 - val_acc: 0.7760\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5120 - acc: 0.7656 - val_loss: 0.5259 - val_acc: 0.7760\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5118 - acc: 0.7639 - val_loss: 0.5257 - val_acc: 0.7812\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5116 - acc: 0.7639 - val_loss: 0.5255 - val_acc: 0.7812\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5114 - acc: 0.7656 - val_loss: 0.5253 - val_acc: 0.7812\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5112 - acc: 0.7656 - val_loss: 0.5252 - val_acc: 0.7812\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5110 - acc: 0.7674 - val_loss: 0.5250 - val_acc: 0.7812\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5108 - acc: 0.7656 - val_loss: 0.5248 - val_acc: 0.7812\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5106 - acc: 0.7656 - val_loss: 0.5246 - val_acc: 0.7812\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5104 - acc: 0.7674 - val_loss: 0.5244 - val_acc: 0.7812\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5102 - acc: 0.7674 - val_loss: 0.5243 - val_acc: 0.7812\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5100 - acc: 0.7674 - val_loss: 0.5241 - val_acc: 0.7812\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5098 - acc: 0.7674 - val_loss: 0.5239 - val_acc: 0.7812\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5096 - acc: 0.7674 - val_loss: 0.5237 - val_acc: 0.7812\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5094 - acc: 0.7691 - val_loss: 0.5236 - val_acc: 0.7812\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5092 - acc: 0.7691 - val_loss: 0.5234 - val_acc: 0.7812\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5090 - acc: 0.7691 - val_loss: 0.5232 - val_acc: 0.7812\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5088 - acc: 0.7691 - val_loss: 0.5231 - val_acc: 0.7812\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5086 - acc: 0.7691 - val_loss: 0.5229 - val_acc: 0.7812\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5085 - acc: 0.7691 - val_loss: 0.5227 - val_acc: 0.7812\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5083 - acc: 0.7691 - val_loss: 0.5226 - val_acc: 0.7812\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5081 - acc: 0.7691 - val_loss: 0.5224 - val_acc: 0.7812\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5079 - acc: 0.7691 - val_loss: 0.5222 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5077 - acc: 0.7691 - val_loss: 0.5221 - val_acc: 0.7760\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5075 - acc: 0.7691 - val_loss: 0.5219 - val_acc: 0.7760\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5073 - acc: 0.7691 - val_loss: 0.5217 - val_acc: 0.7760\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5071 - acc: 0.7691 - val_loss: 0.5216 - val_acc: 0.7708\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5069 - acc: 0.7691 - val_loss: 0.5214 - val_acc: 0.7708\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5068 - acc: 0.7691 - val_loss: 0.5213 - val_acc: 0.7708\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5066 - acc: 0.7691 - val_loss: 0.5211 - val_acc: 0.7708\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5064 - acc: 0.7691 - val_loss: 0.5209 - val_acc: 0.7708\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5062 - acc: 0.7691 - val_loss: 0.5208 - val_acc: 0.7708\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5060 - acc: 0.7691 - val_loss: 0.5206 - val_acc: 0.7708\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5059 - acc: 0.7691 - val_loss: 0.5205 - val_acc: 0.7708\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5057 - acc: 0.7691 - val_loss: 0.5203 - val_acc: 0.7708\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5055 - acc: 0.7691 - val_loss: 0.5202 - val_acc: 0.7708\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5053 - acc: 0.7691 - val_loss: 0.5200 - val_acc: 0.7708\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 48us/step - loss: 0.5052 - acc: 0.7691 - val_loss: 0.5199 - val_acc: 0.7708\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5050 - acc: 0.7674 - val_loss: 0.5197 - val_acc: 0.7708\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5048 - acc: 0.7691 - val_loss: 0.5196 - val_acc: 0.7708\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5046 - acc: 0.7674 - val_loss: 0.5194 - val_acc: 0.7708\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5045 - acc: 0.7674 - val_loss: 0.5193 - val_acc: 0.7708\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5043 - acc: 0.7674 - val_loss: 0.5191 - val_acc: 0.7708\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5041 - acc: 0.7674 - val_loss: 0.5190 - val_acc: 0.7708\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5039 - acc: 0.7674 - val_loss: 0.5188 - val_acc: 0.7708\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5038 - acc: 0.7674 - val_loss: 0.5187 - val_acc: 0.7708\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5036 - acc: 0.7674 - val_loss: 0.5185 - val_acc: 0.7708\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5034 - acc: 0.7674 - val_loss: 0.5184 - val_acc: 0.7708\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5033 - acc: 0.7656 - val_loss: 0.5182 - val_acc: 0.7708\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5031 - acc: 0.7656 - val_loss: 0.5181 - val_acc: 0.7708\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5029 - acc: 0.7656 - val_loss: 0.5179 - val_acc: 0.7708\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5028 - acc: 0.7656 - val_loss: 0.5178 - val_acc: 0.7708\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5026 - acc: 0.7656 - val_loss: 0.5177 - val_acc: 0.7708\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5024 - acc: 0.7656 - val_loss: 0.5175 - val_acc: 0.7708\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5023 - acc: 0.7656 - val_loss: 0.5174 - val_acc: 0.7708\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5021 - acc: 0.7656 - val_loss: 0.5172 - val_acc: 0.7708\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5020 - acc: 0.7656 - val_loss: 0.5171 - val_acc: 0.7708\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40169004],\n",
       "       [0.5966177 ],\n",
       "       [0.3196603 ],\n",
       "       [0.2860103 ],\n",
       "       [0.218537  ],\n",
       "       [0.5058101 ],\n",
       "       [0.14239025],\n",
       "       [0.3084938 ],\n",
       "       [0.7081858 ],\n",
       "       [0.27125615]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]    # it is the rounded value of y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.771\n",
      "roc-auc is 0.817\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VOX5//HPza4IYQfZ1YCIaAMNYv2ipu4WK7VWf4AKtlq7SFVQVgHBDRUVsYXWuBZt3JeiouIWURQBMbKjbELYZAtrINvz++MMNsQsk2Rmzizv13XlMpM5mfnkYZx77uc85xxzzgkAAESPGn4HAAAAR6I4AwAQZSjOAABEGYozAABRhuIMAECUoTgDABBlKM5IOGZ2lJm9aWa7zexlv/MkKjN7xszuDnx/ppmtDPL3rjWzz8Kbzl9m1tHMnJnVKuP+8Wb2XKRzIXIoznHOzNaZWa6Z7TOzLYE3xGNKbHOGmX1kZnsDBetNM+taYpuGZvaIma0PPNaqwO1mZTyvmdlNZrbEzPabWbaZvWxmp4Tz7w3S7yS1lNTUOXdFdR/MzNICb6RTS/z8MzO7NvD9tYFthpXYJtvM0qqbIYiMxV8HW83s6cOvAzPLNLPrS/wtr5X4/Z8Ffp5Z4udmZmvMbFl18jnnPnXOnVidxwhGIhR2xAeKc2L4tXPuGEkpkrpLGnX4DjP7haRZkv4rqbWk4yR9I2mOmR0f2KaOpA8lnSzpIkkNJZ0haYek08p4zimSbpZ0k6QmkjpLekNSn8qGL6t7qIYOkr51zhWEMMt+SQPNrGM5v75T0ggza1jZ5w2Rw6+DHpJ6ShpTxnbbJJ1hZk2L/WyQpG9L2fYsSS0kHW9mPUMZNp6F4TWNOENxTiDOuS2S3pNXpA97QNJ059wU59xe59xO59wYSXMljQ9sM1BSe0mXOeeWOeeKnHM/OOfucs7NLPk8ZtZJ0o2S+jvnPnLOHXLOHXDO/cc5d19gmx+7tcDtIzqaQJd2o5l9J+k7M/uXmT1Y4nn+a2ZDA9+3NrNXzWybma01s5tKGwMzmyBpnKT/F+girzOzGmY2xsy+N7MfzGy6mSUFtj88vXidma2X9FEZw5sj6RlJd5RxvyQtl/SFpCHlbFM8a1Igy7ZAtjFmViNw37WBzvxBM9sV+JsvDuZxnXMbJb0jqVsZm+TJ+yDVL/BcNSVdKek/pWw7SN4Hu5mB78v7e7qb2cLADM2LkuoVuy/NzLKL3R5pZqsD2y4zs8t++nD298BMzwozO7fYHUlm9qSZbTazjWZ2t5nVNLOTJP1L0i8C//Y5ge3rBsZxfWBW4V9mdlTgvmZm9paZ5ZjZTjP79PC/QSl/nzNvtmiNmW03s0kl/r3mmNlkM9spaXx5r7ti/mBmmwJ/y63ljO3pZvZ5IOc3Vmw2JvD/2t2B+/eZNzPW1Mz+Y2Z7zGx+BR8q4QOKcwIxs7aSLpa0KnD7aHkdcGn7XV+SdH7g+/Mkveuc2xfkU50rKds5N696ifUbSb0kdZWUIa+gmiSZWWNJF0h6IfAG+Ka8jr9N4PlvMbMLSz6gc+4OSfdKetE5d4xz7klJ1wa+finpeEnHSPpHiV89W9JJkn7ymMXcI+lyMytvenaspCFm1qScbQ77u6SkQKaz5X1I+n2x+3tJWimpmbwPWU8eHp/ymFk7Sb+S9HU5m00PPJ/k/c1LJW0q8ThHy9tF8J/AVz/zZllKe8468gr+s/JmUl6WdHk5z79a0pny/v4Jkp4zs2OL3d9L0hp5f/sdkl4rNqb/llQgKVneTNEFkq53zi2X9GdJXwT+7RsFtr9f3sxOSuB32sj7ACdJt0rKltRc3q6Q0ZLKO+fxZZJS5c1O9JX0h1Iyt5D3WrlWFb/ufimpU+BvGGlm55V8QjNrI+ltSXfLG9vbJL1qZs2LbdZP0jWBv+0EeR8Snw5sv1zlf6iEDyjOieENM9sraYOkH/S//xGbyHsNbC7ldzbLe+OTpKZlbFOWym5flomBTj5X0qfy3hTPDNz3O3lvspvkTdE2d87d6ZzLc86tkfS4Ap1fEK6S9LBzbk3gA8goeYWm+NTjeOfc/kCWUgVmJv4l6c5ytsmStxthRHmBAt3q/5M0KjCjsU7SQ/LeYA/73jn3uHOuUF5BOlZeASnLG4Fu8TNJn8j7kFJWzs8lNQl80Bgor1iX9FtJhwJ/z1uSaqns3RanS6ot6RHnXL5z7hVJ88t5/pedc5sCszQvSvpOR+5C+aHYY70o70NKHzNrKe8D6C2Bf68fJE1WGa+FwIeZP0oaEnit7ZU3Loe3z5c3rh0Cz/WpK/+CBPcHHme9pEck9S923ybn3N+dcwWB11Ewr7sJgb9jsbxiWvzxDrta0kzn3MzAeL0vaYG8D2CHPe2cW+2c2y1v1mS1c+6DwK6dl+V9iEEUoTgnht845xpISpPURf8rurskFcl78ynpWEnbA9/vKGObslR2+7JsOPxN4A3xBf3vzWmA/jfN2kFS68CUXk6gAI1W+YWquNaSvi92+3t5hab4729QcO6XdKGZ/aycbcZJ+ouZtSpnm2aS6pSSq02x21sOf+OcOxD49ojFfiX8xjnXyDnXwTn31/I+aAQ8K2mwvO7t9VLuHyTppUCxOSTpNZU9td1a0sYShe37MraVmQ00s6xi/57d9L/Xrcp4rNbyXgu1JW0u9ruPyetWS9Nc0tGSviq2/buBn0vSJHkzTbMC09Ujy8ocUPx1cjhTafdJlX/dlXy8wzpIuqLE67+3jvx/cGux73NLuV3e6wY+oDgnEOfcJ/L2iz4YuL1f3vRWaSuWr5S3CEySPpBXcOoH+VQfSmprZqnlbLNf3pviYaUVqpIdyvOSfmdmHeRNEb4a+PkGSWsDhefwVwPn3K8UnE3y3uAOay9vWrT4G1hQl29zzu2Q1zHdVc42K+QVstHlPNR2eV1byVwbg8kRIs9K+qu8ruxA8TsCu0jOkXS1eUcBbJE3m/ErK30F/2ZJbUpMu7cv7UkD/76Py/tg0DQw/bxEUvHfLe2xNsl7LRyS1KzYa6Ghc+7kwHYl/x23yytOJxfbPimwcE6BWYtbnXPHS/q1pKHF92+Xol0pmQ4r+dzBvO7Ke7zDNkh6tsTrv/7h9R2ITRTnxPOIpPPN7PCisJGSBgUWsjQws8bmHXv6C3n7+iTvTXqDvP1YXQILWZqa2Wgz+0kBdM59J2mapOfNW+hTx8zqmVm/Yp1HlqTfmtnRZpYs6bqKgjvnvpa3kvgJSe8553ICd82TtMfMRph3DHNNM+tmwa8efl7efuDjzDu86PA+6Uqv5g54WN6+/JPK2WaCvP3HjUq7MzBV/ZKkewL/Lh0kDZUUsWNbnXNr5e3rvr2Uu6+Rt3r7RHn7alPk7bfNVulTr1/IKzw3mVktM/utyl7pX19eIdsmSWb2e/108VqLwGPVNrMr5I31TOfcZnnT7A+Zd/hfDTM7wczODvzeVnkfHOsE/sYieR8EJptZi8DztTm8XsHMLjGz5MAHgT2SCgNfZRkW+H+onbyjFV4sZ9tgXndjA/+PnCzv9VLa4z0n6ddmdmHgtV8v8P9d23KeG1GO4pxgnHPb5O0/HBu4/Zm8BT+/ldfdfC9v/1PvQJFVYMryPEkrJL0v701qnrxpxi/LeKqb5C1umSpvJfNqeYtl3gzcP1nequCt8vaXlrYSuDTPB7JkFPubCuV1NSmS1srrhp6Qt5goGE/J+wAyO/D7ByX9Lcjf/Qnn3B55C7TKXPQVKHzPyitEZfmbvBmGNfL2E2cEskaMc+6zwH79kgZJmuac21L8S94+959MbTvn8uS9xq6Vtzvl/8mbPSjtOZfJ27/+hbzXxymS5pTY7Et5C6W2y1tc9bvArIXk7SOvI2lZ4Lle0f+meD+St7hti5kd3m0zQt7U9Vwz2yNvpujwor5Ogdv7AnmmOecyS8sd8F9JX8n78Pm2pCfL2TaY190ngWwfSnrQOTer5IM45zbIW3w2Wt4Hmg2Shon395hm5a9tAAAEw8ycpE7OuVV+Z0Hs45MVAABRhuIMAECUYVobAIAoQ+cMAECUoTgDABBlKrwyipk9JekSST84535yovzA8X9T5J0q7oCka51zCyt63GbNmrmOHTv+eHv//v2qXz/Yc1ygshjf8GJ8w4exDS/GN3xKju1XX3213TnXvJxf+VEwly17Rt7xqqWdW1fyzmPbKfDVS9I/A/8tV8eOHbVgwYIfb2dmZiotLS2IOKgKxje8GN/wYWzDi/ENn5Jja2ZlnrK2pAqntZ1zs+Vdh7YsfeVdctA55+ZKalTi6jEAAKASQnHB7zY68uTs2YGfheKqRAAABCU9PV0ZGRkVbxghzZo1q/KsRCiKc2nXjy31+Cwzu0HSDZLUsmVLZWZm/njfvn37jriN0GJ8w4vxDR/GNrziaXynTZumVatWKTk52dcczjlt3bpVKSkpVR7bUBTnbB155ZS2Kv3KKXLOpUtKl6TU1FRX/BMF+z3Ci/ENL8Y3fBjb8Iqn8W3UqJFSU1N9/bBRVFSk5cuXq06dOtq4cWOVxzYUh1LNkDTQPKdL2h24MgwAAAnDOadRo0bJOadOnTpV67GCOZTqeUlpkpqZWbakO+RdzFzOuX9JminvMKpV8g6l+n21EgEAEGPy8/M1Z84cjRw5Uo0bN67241VYnJ1zpV2btfj9TtKN1U4CAECMuuuuuzRw4MCQFGYpNPucAQAJLhpWSmdlZSklJSWiz3no0CG9+uqruuOOO1SzZs2QPS6n7wQAVFtGRoaysrJ8zZCSkqIBAwZE9DmnTZum3r17h7QwS3TOAIAQqc6hQ7Fm//79euyxxzR06NCwPD6dMwAAlfTGG2+EtUunOAMAEKTdu3drxIgRGjBggFq1ahW256E4AwAQhLy8PM2bN08jRoyQd0HG8KE4AwBQge3bt2vIkCE6++yz1aRJk7A/HwvCACAOBXtoU05Ojho1alTt5/PjMKZI2bFjh77//ntNnDhRderUichz0jkDQByK9KFNfhzGFAmbN2/WuHHj1KVLFzVs2DBiz0vnDABxKphDm+Lpwhehlp2drV27dmnSpEk6+uijI/rcdM4AAJSwefNmPfDAA+rUqVPEC7NE5wwAwBFWr16tvXv3atKkSapbt64vGeicAQAI2LNnj/75z3/q5JNP9q0wS3TOAGJYNFxsIVrF8+rpcFm2bJm2bt2qSZMmhf045orQOQOIWdFwsYVoFa+rp8OloKBAr776qs466yzfC7NE5wwgxiXSxRYQHgsXLtSaNWs0duxYv6P8iM4ZAJCwnHOaP3++Lr/8cr+jHIHOGQCQkObMmaMlS5boT3/6k99RfoLOGQCQcPbv369du3bphhtu8DtKqeicAVRZJFZLl3fuZ1Ykoyo++OADLV26VDfffLPfUcpE5wygyvxeLc2KZFTW2rVr1bRp06guzBKdM4BqCvdqac79jFB56623tH79ev31r3/1O0qFKM4AgLj32WefqWfPnrrkkkv8jhIUprUBAHFt5syZWrVqlVq2bOl3lKDROQMA4tZrr72mCy64QMccc4zfUSqF4gwkoFCtsma1NKLZ7NmzlZeXF3OFWWJaG0hIoVplzWppRKsnn3xS3bp1U79+/fyOUiV0zkCC4pzUiFdLlixRs2bN1KRJE7+jVBmdMwAgbkyZMkVHH320+vbt63eUaqE4AwDiwoYNG9S1a1cdf/zxfkepNoozACCmOed03333afv27Tr//PP9jhMS7HMGYlhVV12zyhrxwjmn7Oxs/fKXv1T37t39jhMydM5ADKvqqmtWWSMeOOc0YcIEbdmyRb169fI7TkjROQMxjlXXSERFRUVaunSprr76aiUnJ/sdJ+TonAEAMcU5pzFjxqioqCguC7NE5wwAiCEFBQXKzMzUiBEjlJSU5HecsKFzBgDEjHvvvVft2rWL68Is0TkDUYFV10D58vLy9OKLL2rMmDGqUSP++8r4/wuBGMCqa6B8jz/+uM4888yEKMwSnTMQNVh1DfxUbm6u/vGPf2jYsGF+R4moxPgIAgCIOc45vfnmm7rqqqv8jhJxFGcAQNTZu3evhg0bpt/97ndq3bq133EijuIMAIgqBw8e1FdffaWRI0cmzD7mkhLzrwYARKWdO3dq6NChOv3009WsWTO/4/iGBWFAmFTm8CgOiQKkHTt2aP369Zo4caLq1avndxxf0TkDYVKZw6M4JAqJbuvWrRo3bpySk5Pj/gQjwaBzBsKIw6OAim3atEnbt2/XAw88oPr16/sdJyrQOQMAfLNt2zbdd9996tSpE4W5GDpnAIAv1q1bpx07dmjSpEmqW7eu33GiCp0zACDiDhw4oL///e865ZRTKMyloHMGKqGsFdg5OTlq1KjRET9jBTZQupUrV2rdunV68MEHZWZ+x4lKdM5AJbACG6iewsJCvfLKKzr33HMpzOWgcwYqqbQV2JmZmUpLS/MlDxArvvnmGy1ZskS3336731GiHp0zACDsioqKNH/+fPXv39/vKDGBzhkAEFZz587V/Pnz9be//c3vKDGDzhkAEDZ79+7Vrl27NHjwYL+jxBQ6ZwBAWGRmZmrBggW67bbb/I4Sc+icAQAht2rVKjVp0oTCXEUUZwBASL377ruaOXOmTj31VL+jxCymtQEAITN79mz16NFDF110kd9RYhqdMwAgJGbNmqWVK1eqRYsWfkeJeXTOAIBqe+2113Teeefpggsu8DtKXKBzBgBUy5dffqnc3Fw1bNjQ7yhxg+IMAKiyp59+Wh07dtRVV13ld5S4QnEGAFTJd999p4YNG6ply5Z+R4k7FGcAQKVNnTpVhYWFuvzyy/2OEpcozgCAStmyZYuSk5PVpUsXv6PELYozACAozjk9+OCDWr9+vS688EK/48Q1DqUCJKWnpysjI6PC7bKyspSSkhKBREB0cc5p48aN6t27t0477TS/48Q9OmdAUkZGhrKysircLiUlRQMGDIhAIiB6OOd09913a8OGDTr99NP9jpMQ6JyBgJSUFGVmZvodA4gqzjktXrxYAwYM0AknnOB3nIRB5wwAKNP48eNVUFBAYY4wOmcAwE8UFhbqgw8+0G233aYGDRr4HSfh0DkDAH7igQceULt27SjMPqFzBgD8KD8/X88995xGjBihGjXo3/xCcUbcCPZwqNJwiBTgeeaZZ3TOOedQmH3G6CNuBHs4VGk4RAqJ7uDBg7rnnnt0/fXXs/grCgTVOZvZRZKmSKop6Qnn3H0l7m8v6d+SGgW2GemcmxnirECFOBwKqDznnN555x0NGjRIZuZ3HCiIztnMakqaKuliSV0l9TezriU2GyPpJedcd0n9JE0LdVAAQOjl5uZq6NCh+vWvf622bdv6HQcBwUxrnyZplXNujXMuT9ILkvqW2MZJOnyV7SRJm0IXEQAQDrm5uVq1apVGjRqlWrVYghRNgvnXaCNpQ7Hb2ZJ6ldhmvKRZZvY3SfUlnVfaA5nZDZJukKSWLVseMf24b98+piPDKBHGNycnR5J8+TsTYXz9wtiGx759+/T444/r6quv1rJly7Rs2TK/I8Wd6rx2gynOpe2AcCVu95f0jHPuITP7haRnzaybc67oiF9yLl1SuiSlpqa6tLS0H+/LzMxU8dsIrUQY30aNGkmSL39nIoyvXxjb0Nu5c6c2bNigZ555Rt988w3jGybVee0GM62dLaldsdtt9dNp6+skvSRJzrkvJNWT1KxKiQAAYbN9+3aNHTtWHTt2VOPGjf2OgzIEU5znS+pkZseZWR15C75mlNhmvaRzJcnMTpJXnLeFMigAoHq2bNmijRs36r777lNSUpLfcVCOCouzc65A0mBJ70laLm9V9lIzu9PMLg1sdqukP5rZN5Kel3Stc67k1DcAwCe7du3SXXfdpeTkZE7JGQOCWp4XOGZ5ZomfjSv2/TJJ/xfaaACAUFi/fr02bdqkhx9+WHXr1vU7DoLAGcIAII4dOnRIU6ZMUffu3SnMMYQD2wAgTn333XdauXKlHnzwQc78FWPonAEgDjnn9Morr+iiiy6iMMcgOmcAiDNLlizRggULNGrUKL+joIronAEgjhQVFWnBggUaOHCg31FQDXTOABAnFixYoNmzZ2vo0KF+R0E10TkDQBzYvXu3du7cqSFDhvgdBSFA54yol56eroyMjAq3y8rKUkpKSgQSAdHl008/1Zw5czRy5Ei/oyBE6JwR9TIyMpSVlVXhdikpKRowYEAEEgHRY+XKlWrSpIlGjBjhdxSEEJ0zYkJKSgqXDQRK+OCDD7Ro0SL2McchijMAxKDZs2fr1FNP1Xnnned3FIQB09oAEGMyMzO1bNkytWjRwu8oCBM6ZwCIIa+//rrS0tKUlpbmdxSEEcUZUafk6mxWYQOerKws7dmzR40bN/Y7CsKMaW1EnZKrs1mFDUjPPvusmjZtqkGDBvkdBRFA54yoxOps4H/Wr1+vunXrql27dn5HQYTQOQNAFHvssce0a9cuXXnllX5HQQRRnAEgSm3btk3t27fXz372M7+jIMIozgAQhSZPnqyVK1fq4osv9jsKfMA+ZwCIIs45bdy4UWeccYZ69erldxz4hM4ZAKKEc04TJ07U2rVrKcwJjs4ZAKKAc05ZWVnq37+/jjvuOL/jwGd0zgAQBe6++24VFBRQmCGJzhkAfFVUVKSZM2dq6NChql+/vt9xECXonAHARw8//LA6dOhAYcYR6JwBwAcFBQV6+umndeutt8rM/I6DKEPnjKiQnp7+45V2ip9XG4hXzz33nM4++2wKM0pFcUZUKH6xCy50gXh26NAh3XnnnRo0aJA6d+7sdxxEKaa1ETW42AXinXNOH3zwgQYNGkTHjHLROQNABBw4cEBDhgzR+eefrw4dOvgdB1GO4gwAYZabm6vFixdr5MiRqlOnjt9xEAMozgAQRnv27NFtt92mLl26qFWrVn7HQYxgnzMiIj09XRkZGWXen5WVpZSUlAgmAsJv165dWr9+ve68804lJSX5HQcxhM4ZEVF8NXZpWKGNeLNz506NGTNGHTp0UNOmTf2OgxhD54yIYTU2EsW2bdu0ceNGTZw4UQ0bNvQ7DmIQnTMAhNDevXs1YcIEJScnU5hRZXTOABAiGzdu1Nq1a/Xwww+zKhvVQucMACFQUFCgKVOmKDU1lcKMaqNzRrVUtAr7MFZjI56tWbNG33zzjR544AG/oyBO0DmjWipahX0Yq7ERr5xzevXVV3XJJZf4HQVxhM4Z1cYqbCSq5cuX69NPP9WwYcP8joI4Q+cMAFVQWFior776Stddd53fURCH6JwBoJK+/vprzZo1SyNGjPA7CuIUnTMAVMKuXbu0a9cuprIRVhRnAAjS559/rqlTp+qcc85RjRq8fSJ8eHUBQBCWL1+uxo0b6/bbb/c7ChIAxRkAKvDJJ5/orbfeUpcuXWRmfsdBAmBBGACU45NPPlGXLl109tln+x0FCYTOGQDK8Pnnn2vx4sVq2bKl31GQYOicAaAU//3vf3XGGWfojDPO8DsKEhDFGRUq7/zZnDMb8WjZsmXavn27mjdv7ncUJCimtVGh8s6fzTmzEW/+85//qG7dupz5C76ic0ZQOH82EsGWLVtUo0YNnXDCCX5HQYKjcwYASU888YQ2bNig/v37+x0FoDgDwM6dO3XssceqZ8+efkcBJDGtDSDBPfroozrllFPUp08fv6MAP6I44ydKrs5mRTbiVXZ2tnr16qVevXr5HQU4AtPa+ImSq7NZkY14dN999+m7776jMCMq0TmjVKzORrxyzumrr77SgAED1L59e7/jAKWicwaQUO6//37l5+dTmBHV6JwBJISioiK9+eabuvnmm3XUUUf5HQcoF50zgIQwdepUdejQgcKMmEDnDCCuFRYW6vHHH9fgwYO5FjNiBp0zJHmHT6WlpSktLa3M82gDsejFF19UWloahRkxheIMSUcePsWhU4gHeXl5Gj9+vPr166cuXbr4HQeoFKa18SMOn0K8KCoq0ieffKJBgwapRg16EMQeXrUA4kpubq6GDBmi3r1767jjjvM7DlAldM4A4saBAwe0fPlyDR8+nFXZiGl0zgDiwt69ezVs2DB17NhRbdq08TsOUC10znGs+AUscnJy1KhRozK35eIWiGW7d+/WunXrNH78eDVt2tTvOEC10TnHsZIXsCgPK7QRq3JycjRq1Ci1a9dOzZs39zsOEBJ0znHu8ArszMxMpaWl+R0HCKnt27dr/fr1mjhxopKSkvyOA4QMnTOAmJSbm6vx48erU6dOFGbEHTpnADFn8+bNWr58uSZPnqzatWv7HQcIOTpnADGlqKhIjzzyiE4//XQKM+IWnTOAmLFu3TrNnTtX999/v99RgLAKqnM2s4vMbKWZrTKzkWVsc6WZLTOzpWaWEdqYACC99tpr+u1vf+t3DCDsKuyczaympKmSzpeULWm+mc1wzi0rtk0nSaMk/Z9zbpeZtQhXYACJZ+XKlXr//fc1dOhQv6MAERFM53yapFXOuTXOuTxJL0jqW2KbP0qa6pzbJUnOuR9CGxNAoiosLNTChQv15z//2e8oQMQEU5zbSNpQ7HZ24GfFdZbU2czmmNlcM7soVAEBJK5FixYpIyND/fv3V61aLJFB4gjm1V7aFcpdKY/TSVKapLaSPjWzbs65nCMeyOwGSTdIUsuWLY+4POG+ffu4XGGI5eR4w5+Zmcn4hhnjG3q7d+/W2rVr1bdvX8Y2jHjthk91xjaY4pwtqV2x220lbSplm7nOuXxJa81spbxiPb/4Rs65dEnpkpSamuqKn7GKM1iF3uFzaaelpTG+Ycb4hta8efP08ccfa8KECYxtmDG+4VOdsQ1mWnu+pE5mdpyZ1ZHUT9KMEtu8IemXkmRmzeRNc6+pUiIACW3p0qVKSkrS+PHj/Y4C+KbC4uycK5A0WNJ7kpZLesk5t9TM7jSzSwObvSdph5ktk/SxpGHOuR3hCg0gPs2ZM0czZsxQ586dZVbaHjUgMQS1wsI5N1PSzBI/G1fseydpaOALACpt9uzZ6ty5s8444wwKMxIep+8E4LsFCxZo4cKFatWqFYUZEMUZgM/efPNNtW7dWrfccovfUYCowYGDMS70WhMiAAAc1ElEQVQ9PV0ZGaWfLTUrK0spKSkRTgQEb/Xq1dq8ebNat27tdxQgqtA5x7iMjAxlZWWVel9KSooGDBgQ4URAcF588UUdOnRIN9xwg99RgKhD5xwHUlJSOIkAYsqOHTtUUFCgrl27+h0FiEoUZwAR9cwzzyg5OVlXXXWV31GAqMW0NoCI2b17t5o3b67evXv7HQWIanTOACJi2rRpSk5OVp8+ffyOAkQ9ijOAsNuwYYN69uypnj17+h0FiAkU5yhU3uFRJXG4FKLdQw89pFNPPVXnn3++31GAmME+5yhU3uFRJXG4FKKVc05ffvml+vXrR2EGKonOOUpxeBRi3cMPP6zTTz9dbdq08TsKEHMozgBCyjmn119/XTfeeKPq1avndxwgJjGtDSCk0tPT1aFDBwozUA10zgBCorCwUNOmTdPgwYO5shRQTRTnMKrMquviWIGNWPTaa6/pnHPOoTADIcC0dhhVZtV1cazARizJz8/X2LFjddlll+nkk0/2Ow4QF+icw4xV14hnRUVFmjNnjgYNGqRatXg7AUKFzhlAlRw8eFBDhgzRz3/+cyUnJ/sdB4grfNQFUGm5ublauXKlbrvtNjVo0MDvOEDcoXMGUCn79+/XsGHD1Lp1a7Vr187vOEBconOuJM57jUS2d+9erV27VmPHjlWLFi38jgPELTrnSuK810hUe/fu1ciRI9W6dWu1bNnS7zhAXKNzrgJWYCPR7Ny5U2vWrNG9996rpKQkv+MAcY/OGUC58vLyNG7cOHXq1InCDEQInTOAMm3dulVZWVl65JFHOI4ZiCA6ZwClcs7p0UcfVe/evSnMQITxf1wQiq/QZgU2EsGGDRuUmZmpe+65x+8oQEKicw5C8RXarMBGInjjjTd0xRVX+B0DSFh0zkFihTYSwerVqzVjxgwNGTLE7yhAQqNzBiDJu7rUwoULNXjwYL+jAAmPzhmAli5dqpdeekkTJkzwOwoA0TkDCe+HH35QTk6Oxo0b53cUAAEJ2zlzjmxA+uqrr/T666/rrrvukpn5HQdAQMJ2zpwjG4luyZIlatCgAYUZiEIJ2zlLrMBG4po3b55mzZql22+/ncIMRKGE7ZyBRPXpp5+qbdu2FGYgilGcgQSyaNEizZs3T61bt6YwA1GM4gwkiJkzZyopKUm33nqr31EAVIDiDCSADRs2aN26derQoYPfUQAEgeIMxLlXXnlFO3bs0F//+le/owAIEsUZiGO7d+9Wbm4ux+kDMSahD6UC4tmzzz6rNm3a6JprrvE7CoBKonMG4tCePXvUtGlTnXPOOX5HAVAFdM5AnHnsscfUtm1b9enTx+8oAKqI4gzEke+//16pqan6+c9/7ncUANXAtDYQJ6ZMmaJly5ZRmIE4QOcMxDjnnD7//HNdeeWVOvbYY/2OAyAE6JyBGPfoo4+qoKCAwgzEETpnIEY55/Tyyy/rz3/+s+rWret3HAAhROcMxKinn35aHTp0oDADcYjOGYgxRUVFevTRR3XzzTdzZSkgTtE5AzHmrbfe0jnnnENhBuIYxRmIEQUFBRo7dqwuvPBCnXrqqX7HARBGFGcgBhQWFmrevHm65ppr2McMJACKMxDl8vLydNttt+mkk05S586d/Y4DIAJYEAZEsYMHD+rbb7/VLbfcosaNG/sdB0CE0DkDUerAgQMaNmyYmjdvrg4dOvgdB0AE0TkDUWj//v1avXq1Ro8ezZm/gARE5wxEmf3792v48OFq1aoVhRlIUHTOQBTJycnRypUrde+99yopKcnvOAB8QucMRImCggKNGzdOnTt3pjADCY7OGYgC27Zt05dffqnJkyerZs2afscB4DM6Z8Bnzjn94x//UFpaGoUZgKQ475zT09OVkZFR6n1ZWVlKSUmJcCLgSBs3btR7772nCRMm+B0FQBSJ6845IyNDWVlZpd6XkpKiAQMGRDgR8D/OOc2YMUP9+/f3OwqAKBPXnbPkFeHMzEy/YwBHWLt2rV588UWNHDnS7ygAolBcd85ANDp06JCysrI0dOhQv6MAiFIUZyCCli9frgkTJuiyyy5TnTp1/I4DIEpRnIEI2bJli3bv3q277rrL7ygAohzFGYiArKwsTZkyRaeddhqHSwGoEMUZCLMlS5aofv36uueee1SjBv/LAagY7xRAGC1cuFCvvPKKkpOTKcwAgsa7BRAmc+bMUbNmzXTHHXfIzPyOAyCGUJyBMFixYoU+++wztWvXjsIMoNIozkCIzZo1SzVq1NCIESMozACqJKjibGYXmdlKM1tlZmWe0sjMfmdmzsxSQxcRiB1bt27VihUr1LlzZ7+jAIhhFRZnM6spaaqkiyV1ldTfzLqWsl0DSTdJ+jLUIYFY8MYbb2jdunW66aab/I4CIMYF0zmfJmmVc26Ncy5P0guS+pay3V2SHpB0MIT5gJiQm5urPXv2qFevXn5HARAHginObSRtKHY7O/CzH5lZd0ntnHNvhTAbEBOef/55LV68WAMHDvQ7CoA4EcxVqUpb0eJ+vNOshqTJkq6t8IHMbpB0gyS1bNnyiKtF7du3L+RXj8rJyZEkrkql8IwvpP379+v7779Xt27dGN8w4bUbXoxv+FRnbIMpztmS2hW73VbSpmK3G0jqJikzsDK1laQZZnapc25B8QdyzqVLSpek1NRUl5aW9uN9mZmZKn47FBo1aiRJIX/cWBSO8U10Tz31lJo0aaKRI0cyvmHE2IYX4xs+1RnbYIrzfEmdzOw4SRsl9ZM04PCdzrndkpodvm1mmZJuK1mYgXiyZs0a9ejRQykpKX5HARCHKizOzrkCMxss6T1JNSU95ZxbamZ3SlrgnJsR7pDlSU9PV0ZGRqn3ZWVl8eaJkJs6darat2+vX//6135HARCngumc5ZybKWlmiZ+NK2PbtOrHCl5GRkaZRTglJUUDBgwo5beAqvn00091xRVXqEWLFn5HARDHgirO0S4lJYUFDQi7f/7znzrxxBMpzADCLi6KMxBOzjm98MILuv7661W7dm2/4wBIAJxbG6hARkaGOnbsSGEGEDF0zkAZioqK9Mgjj+jmm29WzZo1/Y4DIIHEXHEuuTqbFdkIl1mzZumXv/wlhRlAxMXctPbh1dmHsSIboVZYWKgxY8borLPOUvfu3f2OAyABxVznLLE6G+FTWFiohQsX6qqrrtLRRx/tdxwACSrmOmcgXPLz8zVs2DB16NBBJ510kt9xACSwmOycgVA7dOiQvvvuOw0ePJjjmAH4js4ZCe/gwYMaNmyYGjVqpOOPP97vOABA54zEduDAAa1atUojR45U69at/Y4DAJLonJHADh48qOHDh6tFixYUZgBRhc4ZCWnPnj1avHix7r33XjVs2NDvOABwBDpnJJyioiKNHTtWXbp0oTADiEp0zkgoO3bs0OzZszV58mTVqMFnUwDRiXcnJJRp06bp3HPPpTADiGp0zkgIW7Zs0X//+1+NHTvW7ygAUCHaB8Q955zefPNNXXPNNX5HAYCg0Dkjrn3//feaPn06HTOAmELnjLh18OBBLVq0SMOHD/c7CgBUCsUZcenbb7/VuHHjdMkll6hu3bp+xwGASqE4I+5s2rRJu3fv1r333isz8zsOAFQaxRlxZfHixZoyZYp69OihWrVYUgEgNvHuhbixZMkS1atXTxMnTuQ4ZgAxjXcwxIUlS5bopZde0gknnEBhBhDzeBdDzPviiy9Uv359TZgwgcIMIC7wToaYtmbNGn388cfq2LEji78AxA2KM2LWhx9+qAMHDmjUqFEUZgBxheKMmLRz504tWbJE3bp1ozADiDus1kbMeeutt5SUlKSbb77Z7ygAEBZ0zogpBw8e1M6dO3XmmWf6HQUAwobOGTHjpZdeUr169TRw4EC/owBAWFGcERP27Nmjhg0b6qKLLvI7CgCEHcUZUe/f//63jj76aF1xxRV+RwGAiKA4I6p999136tGjh0455RS/owBAxERlcU5PT1dGRkap92VlZSklJSXCieCHxx57TK1atVLfvn39jgIAERWVxTkjI6PMIpySkqIBAwb4kAqR9PHHH+vyyy9Xs2bN/I4CABEXlcVZ8opwZmam3zHggyeeeELt27enMANIWFFbnJF4nHN67rnndO2113ItZgAJjZOQIGq88sor6tixI4UZQMLjXRC+c87p4Ycf1k033aTatWv7HQcAfEfnDN99/PHHOvvssynMABBAcYZvioqKNGbMGKWmpio1NdXvOAAQNZjWhi8KCwu1ePFi9evXTw0bNvQ7DgBEFTpnRFx+fr5GjBih5s2bq1u3bn7HAYCoQ+eMiMrLy9OqVav0pz/9SW3atPE7DgBEJTpnRMyhQ4c0fPhwHX300erUqZPfcQAgatE5IyJyc3P17bffatiwYXTMAFABOmeEXX5+voYNG6ZmzZpRmAEgCHTOCKu9e/dq4cKFmjhxoho0aOB3HACICXTOCBvnnMaPH6+uXbtSmAGgEuicERa7du3S+++/r0mTJqlGDT4DAkBl8K6JsEhPT9cFF1xAYQaAKqBzRkj98MMPeumllzRixAi/owBAzKKtQcg45/T222/r97//vd9RACCm0TkjJLKzs5Wenq4777zT7ygAEPPonFFtubm5WrJkiUaPHu13FACICxRnVMvq1at1++2368ILL1S9evX8jgMAcYHijCrLzs7W7t27df/998vM/I4DAHGD4owqWb58uR599FGdeuqpql27tt9xACCuUJxRaUuXLlWtWrU0ceJE1arFmkIACDWKMyplxYoVysjI0AknnKCaNWv6HQcA4hLFGUGbN2+eatasqbvvvpszfwFAGPEOi6BkZ2fr3XffVXJyMou/ACDM2GGICn3yySdq0KCBxo4dS2EGgAigc0a59u7dq6+//lrdu3enMANAhNA5o0zvvPOOateurVtuucXvKACQUOicUaq8vDxt27ZN5513nt9RACDh0DnjJ1577TUVFRVp4MCBfkcBgIREccYRdu/erWOOOUYXXHCB31EAIGFRnPGj5557TjVq1NCAAQP8jgIACY3iDEnemb969Oihrl27+h0FABJeVBTn9PR0TZs2TY0aNZIkZWVlKSUlxedUiePJJ59Uo0aNdPnll/sdBQCgKCnOGRkZWrVqlVJTUyVJKSkpTK1GyIcffqjLLrtMTZo08TsKACAgKoqzJCUnJyszM9PvGAll+vTpatasGYUZAKJM1BRnRNb06dM1YMAALvkIAFGIk5AkoBkzZqh9+/YUZgCIUkEVZzO7yMxWmtkqMxtZyv1DzWyZmS0ysw/NrEPoo6K6nHN66KGHdOGFFyotLc3vOACAMlRYnM2spqSpki6W1FVSfzMrebzN15JSnXOnSnpF0gOhDorqmzNnjnr37q26dev6HQUAUI5gOufTJK1yzq1xzuVJekFS3+IbOOc+ds4dCNycK6ltaGOiOoqKivTUU0/ppJNOUq9evfyOAwCoQDA7HdtI2lDsdrak8t7hr5P0Tml3mNkNkm6QpJYtW/64OjsnJ0eFhYWs1g6DwsJCrV+/Xj179tTixYv9jhO39u3bx+s3TBjb8GJ8w6c6YxtMcS7tIr6u1A3NrpaUKuns0u53zqVLSpek1NRUd3i/Z6NGjZSTk8N+0BArKCjQ6NGjdeONN2rt2rWMbxhlZmYyvmHC2IYX4xs+1RnbYKa1syW1K3a7raRNJTcys/Mk3S7pUufcoSqlQcjk5+dr1apVuu6669ShA+vzACCWBFOc50vqZGbHmVkdSf0kzSi+gZl1l/SYvML8Q+hjojLy8vI0fPhw1a5dWyeeeKLfcQAAlVThtLZzrsDMBkt6T1JNSU8555aa2Z2SFjjnZkiaJOkYSS+bmSStd85dGsbcKMPBgwe1YsUK3XbbbWrTpo3fcQAAVRDUWSicczMlzSzxs3HFvj8vxLlQBYWFhRo+fLiGDRtGYQaAGMYpouLE/v37NXfuXE2cOFH169f3Ow4AoBo4fWecuPPOO9WtWzcKMwDEATrnGJeTk6O3335b9913nwL7+wEAMY7OOcY9+eSTuvjiiynMABBH6Jxj1Pbt2zV9+nTdeuutfkcBAIQYnXMMcs7p3Xff1R//+Ee/owAAwoDiHGM2bdqk0aNH6+qrr1aDBg38jgMACAOKcwzZv3+/li1bpnHjxlW8MQAgZlGcY8S6des0evRonXPOOTrqqKP8jgMACCOKcwzIzs5WTk6OJk2apBo1+CcDgHjHO32U+/bbbzV58mSdfPLJqlOnjt9xAAARQHGOYsuWLZMk3X///apdu7bPaQAAkUJxjlKrV6/W9OnTdcIJJ6hWLQ5HB4BEQnGOQl999ZUOHTqke++9VzVr1vQ7DgAgwijOUeaHH37Qm2++qZNOOonFXwCQoJgvjSKfffaZatWqpfHjx/sdBQDgI1qzKJGbm6v58+erV69efkcBAPiMzjkKvP/++8rLy9OQIUP8jgIAiAJ0zj7Lz8/X1q1b1adPH7+jAACiBJ2zj2bMmKF9+/bp6quv9jsKACCKUJx9smvXLtWvX1+XXnqp31EAAFGG4uyDF154QXl5eRo4cKDfUQAAUYjiHGFLly5V9+7ddeKJJ/odBQAQpVgQFkHTp0/X0qVLKcwAgHLROUfIrFmz1LdvXyUlJfkdBQAQ5eicI+CFF17QoUOHKMwAgKDQOYfZM888o6uuuopLPgIAgkbnHEbvvvuu2rZtS2EGAFQKnXMYOOf00EMP6S9/+Yvq16/vdxwAQIyhcw4x55zmz5+vX/ziFxRmAECVUJxDqKioSHfccYfat2+v//u///M7DgAgRlGcQ6SoqEjffvutfvOb36hVq1Z+xwEAxDCKcwgUFhZq1KhRqlWrlnr06OF3HABAjGNBWDUVFBRo9erV+v3vf6/k5GS/4wAA4gCdczXk5+dr+PDhMjN16dLF7zgAgDhB51xFhw4d0tKlS3XrrbeqTZs2fscBAMQROucqKCoq0ogRI9S0aVMKMwAg5OicK+nAgQOaPXu2Jk6cqKOOOsrvOACAOETnXEn33HOPfvazn1GYAQBhQ+ccpD179uj111/X3XffLTPzOw4AII7ROQfp6aefVp8+fSjMAICwo3OuwM6dO/XEE09o+PDhfkcBACQIOudyFBUV6f3339ef/vQnv6MAABIIxbkMW7Zs0YgRI3TllVcqKSnJ7zgAgARCcS7F3r17tWLFCo0fP559zACAiKM4l7B+/XqNHj1avXv35nrMAABfUJyL2bBhg3JycvTggw+qVi3WygEA/EFxDli9erUmT56sLl26qG7dun7HAQAkMNpDSStWrJAk3X///apdu7bPaQAAiS7hO+f169fr6aefVqdOnSjMAICokNCdc1ZWlmrUqKGJEyeqRo2E/5wCAIgSCVuRcnJy9Prrr6tbt24UZgBAVEnIznnu3LnKy8vThAkT/I4CAMBPJFzLmJeXpy+++EJnnnmm31EAAChVQnXOH330kXJycjRkyBC/owAAUKaE6Zzz8/O1efNm/fa3v/U7CgAA5UqIzvntt9/Wtm3bdO211/odBQCACsV9cd6+fbvq16+vPn36+B0FAICgxHVxfvnll7V371794Q9/8DsKAABBi9vivGjRInXv3l3Jycl+RwEAoFLickHY888/r8WLF1OYAQAxKe4653feeUd9+vRRw4YN/Y4CAECVxFVxfvXVV1WjRg0KMwAgpsVNcX7mmWfUv39/rsUMAIh5cbHP+aOPPlKrVq0ozACAuBDTnbNzTg8//LCuv/56JSUl+R0HAICQiNnO2TmnRYsWqWfPnhRmAEBcicni7JzTXXfdpcaNG+uss87yOw4AACEVc9PaRUVFWrNmjS6++GK1b9/e7zgAAIRcTHXORUVFGjNmjPLz89WzZ0+/4wAAEBYx0zkXFhZq9erVuvrqq3XSSSf5HQcAgLCJic65oKBAI0aMUGFhobp27ep3HAAAwirqO+f8/Hx98803uvXWW3Xsscf6HQcAgLCL6s7ZOaeRI0eqSZMmFGYAQMKI2s754MGD+uCDD3TPPfeoXr16fscBACBiorZzfuCBB9S9e3cKMwAg4QRVnM3sIjNbaWarzGxkKffXNbMXA/d/aWYdqxpo3759evLJJzV27Fi1adOmqg8DAEDMqrA4m1lNSVMlXSypq6T+ZlZyyfR1knY555IlTZZ0f1UDPfvss7r00ktlZlV9CAAAYlownfNpklY559Y45/IkvSCpb4lt+kr6d+D7VySda5WsrgUFBbrnnnv0l7/8Rc2bN6/MrwIAEFeCKc5tJG0odjs78LNSt3HOFUjaLalpZYLs27dPN954Y2V+BQCAuBTMau3SOmBXhW1kZjdIukGSWrZsqczMTElSs2bNlJSUpKysrCDioCr27dv343gj9Bjf8GFsw4vxDZ/qjG0wxTlbUrtit9tK2lTGNtlmVktSkqSdJR/IOZcuKV2SUlNTXVpamiQpLS1NmZmZOnwbocf4hhfjGz6MbXgxvuFTnbENZlp7vqROZnacmdWR1E/SjBLbzJA0KPD97yR95Jz7SecMAAAqVmHn7JwrMLPBkt6TVFPSU865pWZ2p6QFzrkZkp6U9KyZrZLXMfcLZ2gAAOKZ+dXgmtk2Sd8X+1EzSdt9CZMYGN/wYnzDh7ENL8Y3fEqObQfnXFCHI/lWnEsyswXOuVS/c8Qrxje8GN/wYWzDi/ENn+qMbdSevhMAgERFcQYAIMpEU3FO9ztAnGN8w4vxDR/GNrwY3/Cp8thGzT5nAADgiabOGQAAyIfiHMnLTyaiIMZ3qJktM7NFZvahmXXwI2csqmhsi233OzNzZsYK2EoIZnzN7MrA63epmWVEOmOsCuJ9ob2ZfWxmXwfeG37lR85YZGZPmdkPZrakjPvNzB4NjP0iM+sR1AM75yL2Je8kJqslHS+pjqRvJHUtsc1fJf0r8H0/SS9GMmMsfwU5vr+UdHTg+78wvqEb28B2DSTNljRXUqrfuWPlK8jXbidJX0tqHLjdwu/csfAV5NimS/pL4Puuktb5nTtWviSdJamHpCVl3P8rSe/IuwbF6ZK+DOZxI905R+TykwmswvF1zn3snDsQuDlX3rnSUbFgXruSdJekByQdjGS4OBDM+P5R0lTn3C5Jcs79EOGMsSqYsXWSGga+T9JPr5+AMjjnZquUa0kU01fSdOeZK6mRmR1b0eNGujhH5PKTCSyY8S3uOnmf6FCxCsfWzLpLaueceyuSweJEMK/dzpI6m9kcM5trZhdFLF1sC2Zsx0u62syyJc2U9LfIREsIlX1flhTcValCKWSXn0Spgh47M7taUqqks8OaKH6UO7ZmVkPSZEnXRipQnAnmtVtL3tR2mrwZn0/NrJtzLifM2WJdMGPbX9IzzrmHzOwX8q6V0M05VxT+eHGvSjUt0p1zZS4/qfIuP4lSBTO+MrPzJN0u6VLn3KEIZYt1FY1tA0ndJGWa2Tp5+5ZmsCgsaMG+N/zXOZfvnFsraaW8Yo3yBTO210l6SZKcc19IqifvvNCovqDel0uKdHHm8pPhVeH4BqZeH5NXmNlnF7xyx9Y5t9s518w519E511He/vxLnXML/Ikbc4J5b3hD3oJGmVkzedPcayKaMjYFM7brJZ0rSWZ2krzivC2iKePXDEkDA6u2T5e02zm3uaJfiui0tuPyk2EV5PhOknSMpJcD6+zWO+cu9S10jAhybFFFQY7ve5IuMLNlkgolDXPO7fAvdWwIcmxvlfS4mQ2RN+V6LU1RcMzseXm7WpoF9tnfIam2JDnn/iVvH/6vJK2SdEDS74N6XMYfAIDowhnCAACIMhRnAACiDMUZAIAoQ3EGACDKUJwBAIgyFGcAAKIMxRkAgChDcQYAIMr8f+xm5BNjHB5FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2f8350f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5018 - acc: 0.7656 - val_loss: 0.5170 - val_acc: 0.7708\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5016 - acc: 0.7656 - val_loss: 0.5168 - val_acc: 0.7708\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5015 - acc: 0.7639 - val_loss: 0.5167 - val_acc: 0.7708\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5013 - acc: 0.7674 - val_loss: 0.5166 - val_acc: 0.7708\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5012 - acc: 0.7656 - val_loss: 0.5164 - val_acc: 0.7708\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5010 - acc: 0.7656 - val_loss: 0.5163 - val_acc: 0.7708\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5009 - acc: 0.7656 - val_loss: 0.5162 - val_acc: 0.7708\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5007 - acc: 0.7656 - val_loss: 0.5160 - val_acc: 0.7708\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5006 - acc: 0.7656 - val_loss: 0.5159 - val_acc: 0.7708\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5004 - acc: 0.7656 - val_loss: 0.5158 - val_acc: 0.7708\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5002 - acc: 0.7656 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5001 - acc: 0.7656 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4999 - acc: 0.7656 - val_loss: 0.5154 - val_acc: 0.7656\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4998 - acc: 0.7656 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4996 - acc: 0.7656 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4995 - acc: 0.7656 - val_loss: 0.5150 - val_acc: 0.7656\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4993 - acc: 0.7656 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4992 - acc: 0.7656 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4990 - acc: 0.7656 - val_loss: 0.5146 - val_acc: 0.7656\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4989 - acc: 0.7656 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4987 - acc: 0.7656 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4986 - acc: 0.7656 - val_loss: 0.5142 - val_acc: 0.7708\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4984 - acc: 0.7656 - val_loss: 0.5141 - val_acc: 0.7708\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4983 - acc: 0.7639 - val_loss: 0.5140 - val_acc: 0.7708\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4981 - acc: 0.7656 - val_loss: 0.5139 - val_acc: 0.7708\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4980 - acc: 0.7656 - val_loss: 0.5138 - val_acc: 0.7656\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4978 - acc: 0.7639 - val_loss: 0.5136 - val_acc: 0.7656\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4977 - acc: 0.7604 - val_loss: 0.5135 - val_acc: 0.7656\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4975 - acc: 0.7604 - val_loss: 0.5134 - val_acc: 0.7656\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4974 - acc: 0.7604 - val_loss: 0.5133 - val_acc: 0.7656\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4973 - acc: 0.7604 - val_loss: 0.5132 - val_acc: 0.7656\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4971 - acc: 0.7604 - val_loss: 0.5130 - val_acc: 0.7656\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4970 - acc: 0.7604 - val_loss: 0.5129 - val_acc: 0.7656\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4968 - acc: 0.7604 - val_loss: 0.5128 - val_acc: 0.7656\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4967 - acc: 0.7604 - val_loss: 0.5127 - val_acc: 0.7656\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4966 - acc: 0.7604 - val_loss: 0.5126 - val_acc: 0.7656\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4964 - acc: 0.7604 - val_loss: 0.5125 - val_acc: 0.7656\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4963 - acc: 0.7604 - val_loss: 0.5124 - val_acc: 0.7656\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4962 - acc: 0.7604 - val_loss: 0.5122 - val_acc: 0.7656\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4960 - acc: 0.7604 - val_loss: 0.5121 - val_acc: 0.7656\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4959 - acc: 0.7604 - val_loss: 0.5120 - val_acc: 0.7656\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4957 - acc: 0.7604 - val_loss: 0.5119 - val_acc: 0.7656\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4956 - acc: 0.7604 - val_loss: 0.5118 - val_acc: 0.7708\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4955 - acc: 0.7604 - val_loss: 0.5117 - val_acc: 0.7708\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4954 - acc: 0.7604 - val_loss: 0.5116 - val_acc: 0.7708\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4952 - acc: 0.7604 - val_loss: 0.5115 - val_acc: 0.7708\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4951 - acc: 0.7604 - val_loss: 0.5113 - val_acc: 0.7708\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4949 - acc: 0.7604 - val_loss: 0.5112 - val_acc: 0.7708\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4948 - acc: 0.7604 - val_loss: 0.5111 - val_acc: 0.7708\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4947 - acc: 0.7604 - val_loss: 0.5110 - val_acc: 0.7708\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4946 - acc: 0.7604 - val_loss: 0.5109 - val_acc: 0.7708\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4944 - acc: 0.7604 - val_loss: 0.5108 - val_acc: 0.7708\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4943 - acc: 0.7622 - val_loss: 0.5107 - val_acc: 0.7708\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4942 - acc: 0.7622 - val_loss: 0.5106 - val_acc: 0.7708\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4940 - acc: 0.7604 - val_loss: 0.5105 - val_acc: 0.7708\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4939 - acc: 0.7604 - val_loss: 0.5104 - val_acc: 0.7708\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4938 - acc: 0.7604 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4937 - acc: 0.7622 - val_loss: 0.5102 - val_acc: 0.7708\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4935 - acc: 0.7622 - val_loss: 0.5101 - val_acc: 0.7708\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4934 - acc: 0.7604 - val_loss: 0.5100 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4933 - acc: 0.7622 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4932 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4930 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4929 - acc: 0.7604 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4928 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4927 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4925 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4924 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4923 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4922 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4921 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4920 - acc: 0.7639 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4918 - acc: 0.7639 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4917 - acc: 0.7656 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4916 - acc: 0.7656 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4915 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4914 - acc: 0.7656 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4912 - acc: 0.7656 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4911 - acc: 0.7656 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4910 - acc: 0.7656 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4909 - acc: 0.7656 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4908 - acc: 0.7656 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4907 - acc: 0.7656 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4906 - acc: 0.7674 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4905 - acc: 0.7656 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4903 - acc: 0.7656 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4902 - acc: 0.7656 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4901 - acc: 0.7656 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4900 - acc: 0.7674 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4899 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7604\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4898 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7604\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4897 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7604\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4895 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4894 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7604\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4893 - acc: 0.7674 - val_loss: 0.5067 - val_acc: 0.7604\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4892 - acc: 0.7674 - val_loss: 0.5066 - val_acc: 0.7604\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4891 - acc: 0.7674 - val_loss: 0.5065 - val_acc: 0.7604\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4890 - acc: 0.7674 - val_loss: 0.5065 - val_acc: 0.7604\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4889 - acc: 0.7674 - val_loss: 0.5064 - val_acc: 0.7604\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4888 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7604\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4887 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7604\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4886 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7604\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4885 - acc: 0.7691 - val_loss: 0.5060 - val_acc: 0.7604\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4884 - acc: 0.7691 - val_loss: 0.5060 - val_acc: 0.7604\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4883 - acc: 0.7691 - val_loss: 0.5059 - val_acc: 0.7604\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4882 - acc: 0.7691 - val_loss: 0.5058 - val_acc: 0.7604\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4881 - acc: 0.7691 - val_loss: 0.5057 - val_acc: 0.7604\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4880 - acc: 0.7691 - val_loss: 0.5056 - val_acc: 0.7604\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4879 - acc: 0.7674 - val_loss: 0.5055 - val_acc: 0.7604\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4878 - acc: 0.7691 - val_loss: 0.5055 - val_acc: 0.7604\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4876 - acc: 0.7674 - val_loss: 0.5054 - val_acc: 0.7604\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4876 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7604\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4875 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7604\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4874 - acc: 0.7674 - val_loss: 0.5052 - val_acc: 0.7604\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4872 - acc: 0.7674 - val_loss: 0.5051 - val_acc: 0.7604\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4872 - acc: 0.7674 - val_loss: 0.5050 - val_acc: 0.7604\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4871 - acc: 0.7691 - val_loss: 0.5049 - val_acc: 0.7604\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4870 - acc: 0.7674 - val_loss: 0.5048 - val_acc: 0.7604\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4869 - acc: 0.7674 - val_loss: 0.5048 - val_acc: 0.7604\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4868 - acc: 0.7674 - val_loss: 0.5047 - val_acc: 0.7604\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.4867 - acc: 0.7691 - val_loss: 0.5046 - val_acc: 0.7604\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4866 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7604\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4865 - acc: 0.7674 - val_loss: 0.5045 - val_acc: 0.7604\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4864 - acc: 0.7674 - val_loss: 0.5044 - val_acc: 0.7604\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4863 - acc: 0.7674 - val_loss: 0.5043 - val_acc: 0.7604\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4862 - acc: 0.7674 - val_loss: 0.5042 - val_acc: 0.7604\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4861 - acc: 0.7674 - val_loss: 0.5042 - val_acc: 0.7604\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4860 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7604\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4859 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7604\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4858 - acc: 0.7674 - val_loss: 0.5039 - val_acc: 0.7604\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4857 - acc: 0.7656 - val_loss: 0.5039 - val_acc: 0.7604\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4856 - acc: 0.7639 - val_loss: 0.5038 - val_acc: 0.7604\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4855 - acc: 0.7639 - val_loss: 0.5037 - val_acc: 0.7604\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4854 - acc: 0.7656 - val_loss: 0.5037 - val_acc: 0.7604\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4853 - acc: 0.7656 - val_loss: 0.5036 - val_acc: 0.7604\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4852 - acc: 0.7639 - val_loss: 0.5035 - val_acc: 0.7604\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4851 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7604\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4851 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7604\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4850 - acc: 0.7656 - val_loss: 0.5033 - val_acc: 0.7552\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4849 - acc: 0.7656 - val_loss: 0.5032 - val_acc: 0.7552\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4848 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7552\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4847 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7552\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4846 - acc: 0.7656 - val_loss: 0.5030 - val_acc: 0.7552\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4845 - acc: 0.7656 - val_loss: 0.5030 - val_acc: 0.7552\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4844 - acc: 0.7656 - val_loss: 0.5029 - val_acc: 0.7552\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4843 - acc: 0.7656 - val_loss: 0.5028 - val_acc: 0.7552\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4842 - acc: 0.7656 - val_loss: 0.5028 - val_acc: 0.7552\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4842 - acc: 0.7656 - val_loss: 0.5027 - val_acc: 0.7552\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4841 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7604\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4840 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7604\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4839 - acc: 0.7674 - val_loss: 0.5025 - val_acc: 0.7604\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4838 - acc: 0.7674 - val_loss: 0.5024 - val_acc: 0.7604\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4837 - acc: 0.7674 - val_loss: 0.5024 - val_acc: 0.7604\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4837 - acc: 0.7674 - val_loss: 0.5023 - val_acc: 0.7604\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4836 - acc: 0.7674 - val_loss: 0.5022 - val_acc: 0.7604\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4835 - acc: 0.7674 - val_loss: 0.5022 - val_acc: 0.7604\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4834 - acc: 0.7674 - val_loss: 0.5021 - val_acc: 0.7604\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4833 - acc: 0.7656 - val_loss: 0.5020 - val_acc: 0.7604\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4832 - acc: 0.7656 - val_loss: 0.5020 - val_acc: 0.7604\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4831 - acc: 0.7656 - val_loss: 0.5019 - val_acc: 0.7604\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4831 - acc: 0.7656 - val_loss: 0.5018 - val_acc: 0.7604\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4830 - acc: 0.7656 - val_loss: 0.5018 - val_acc: 0.7604\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4829 - acc: 0.7656 - val_loss: 0.5017 - val_acc: 0.7604\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4828 - acc: 0.7656 - val_loss: 0.5017 - val_acc: 0.7604\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4827 - acc: 0.7656 - val_loss: 0.5016 - val_acc: 0.7604\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4826 - acc: 0.7674 - val_loss: 0.5015 - val_acc: 0.7604\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4826 - acc: 0.7674 - val_loss: 0.5015 - val_acc: 0.7604\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4825 - acc: 0.7674 - val_loss: 0.5014 - val_acc: 0.7604\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4824 - acc: 0.7674 - val_loss: 0.5014 - val_acc: 0.7604\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4823 - acc: 0.7656 - val_loss: 0.5013 - val_acc: 0.7604\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4822 - acc: 0.7656 - val_loss: 0.5012 - val_acc: 0.7604\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4822 - acc: 0.7674 - val_loss: 0.5012 - val_acc: 0.7604\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4821 - acc: 0.7674 - val_loss: 0.5011 - val_acc: 0.7604\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4820 - acc: 0.7674 - val_loss: 0.5011 - val_acc: 0.7604\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4819 - acc: 0.7674 - val_loss: 0.5010 - val_acc: 0.7604\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4818 - acc: 0.7674 - val_loss: 0.5009 - val_acc: 0.7604\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4818 - acc: 0.7674 - val_loss: 0.5009 - val_acc: 0.7604\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4817 - acc: 0.7674 - val_loss: 0.5008 - val_acc: 0.7604\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4816 - acc: 0.7674 - val_loss: 0.5008 - val_acc: 0.7604\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4815 - acc: 0.7656 - val_loss: 0.5007 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4814 - acc: 0.7674 - val_loss: 0.5006 - val_acc: 0.7604\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4814 - acc: 0.7674 - val_loss: 0.5006 - val_acc: 0.7604\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4813 - acc: 0.7674 - val_loss: 0.5005 - val_acc: 0.7604\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4812 - acc: 0.7674 - val_loss: 0.5005 - val_acc: 0.7604\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4811 - acc: 0.7674 - val_loss: 0.5004 - val_acc: 0.7604\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4811 - acc: 0.7674 - val_loss: 0.5004 - val_acc: 0.7604\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4810 - acc: 0.7674 - val_loss: 0.5003 - val_acc: 0.7604\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4809 - acc: 0.7674 - val_loss: 0.5003 - val_acc: 0.7604\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4808 - acc: 0.7674 - val_loss: 0.5002 - val_acc: 0.7604\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4808 - acc: 0.7691 - val_loss: 0.5001 - val_acc: 0.7604\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4807 - acc: 0.7674 - val_loss: 0.5001 - val_acc: 0.7604\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4806 - acc: 0.7674 - val_loss: 0.5000 - val_acc: 0.7604\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4805 - acc: 0.7674 - val_loss: 0.5000 - val_acc: 0.7604\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4805 - acc: 0.7674 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4804 - acc: 0.7674 - val_loss: 0.4999 - val_acc: 0.7604\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4803 - acc: 0.7674 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4802 - acc: 0.7674 - val_loss: 0.4998 - val_acc: 0.7604\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4802 - acc: 0.7674 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4801 - acc: 0.7674 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4800 - acc: 0.7674 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4800 - acc: 0.7674 - val_loss: 0.4996 - val_acc: 0.7604\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4799 - acc: 0.7674 - val_loss: 0.4995 - val_acc: 0.7604\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4798 - acc: 0.7674 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4797 - acc: 0.7674 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4797 - acc: 0.7674 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4796 - acc: 0.7674 - val_loss: 0.4993 - val_acc: 0.7604\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4795 - acc: 0.7674 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4795 - acc: 0.7674 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5548 - acc: 0.687 - 0s 48us/step - loss: 0.4794 - acc: 0.7674 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4793 - acc: 0.7674 - val_loss: 0.4991 - val_acc: 0.7604\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4793 - acc: 0.7674 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4792 - acc: 0.7674 - val_loss: 0.4990 - val_acc: 0.7604\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4791 - acc: 0.7691 - val_loss: 0.4989 - val_acc: 0.7604\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4790 - acc: 0.7691 - val_loss: 0.4989 - val_acc: 0.7604\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4790 - acc: 0.7691 - val_loss: 0.4988 - val_acc: 0.7604\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4789 - acc: 0.7691 - val_loss: 0.4988 - val_acc: 0.7604\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4788 - acc: 0.7691 - val_loss: 0.4987 - val_acc: 0.7604\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4788 - acc: 0.7691 - val_loss: 0.4987 - val_acc: 0.7604\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4787 - acc: 0.7691 - val_loss: 0.4986 - val_acc: 0.7552\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4786 - acc: 0.7691 - val_loss: 0.4986 - val_acc: 0.7552\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4786 - acc: 0.7691 - val_loss: 0.4986 - val_acc: 0.7552\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4785 - acc: 0.7691 - val_loss: 0.4985 - val_acc: 0.7552\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4785 - acc: 0.7691 - val_loss: 0.4985 - val_acc: 0.7552\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4784 - acc: 0.7691 - val_loss: 0.4984 - val_acc: 0.7552\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4783 - acc: 0.7691 - val_loss: 0.4984 - val_acc: 0.7552\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4783 - acc: 0.7691 - val_loss: 0.4983 - val_acc: 0.7552\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4782 - acc: 0.7691 - val_loss: 0.4983 - val_acc: 0.7552\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4781 - acc: 0.7691 - val_loss: 0.4982 - val_acc: 0.7552\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4781 - acc: 0.7691 - val_loss: 0.4982 - val_acc: 0.7552\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4780 - acc: 0.7691 - val_loss: 0.4981 - val_acc: 0.7552\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4779 - acc: 0.7691 - val_loss: 0.4981 - val_acc: 0.7552\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4778 - acc: 0.7691 - val_loss: 0.4980 - val_acc: 0.7552\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4778 - acc: 0.7708 - val_loss: 0.4980 - val_acc: 0.7552\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4778 - acc: 0.7708 - val_loss: 0.4979 - val_acc: 0.7552\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4777 - acc: 0.7708 - val_loss: 0.4979 - val_acc: 0.7552\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4776 - acc: 0.7708 - val_loss: 0.4979 - val_acc: 0.7552\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4775 - acc: 0.7708 - val_loss: 0.4978 - val_acc: 0.7552\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4775 - acc: 0.7708 - val_loss: 0.4978 - val_acc: 0.7552\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4774 - acc: 0.7708 - val_loss: 0.4977 - val_acc: 0.7552\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.4774 - acc: 0.7708 - val_loss: 0.4977 - val_acc: 0.7552\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4773 - acc: 0.7726 - val_loss: 0.4976 - val_acc: 0.7552\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4772 - acc: 0.7726 - val_loss: 0.4976 - val_acc: 0.7552\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4772 - acc: 0.7726 - val_loss: 0.4975 - val_acc: 0.7552\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4771 - acc: 0.7708 - val_loss: 0.4975 - val_acc: 0.7552\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4771 - acc: 0.7726 - val_loss: 0.4975 - val_acc: 0.7552\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4770 - acc: 0.7708 - val_loss: 0.4974 - val_acc: 0.7552\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4769 - acc: 0.7726 - val_loss: 0.4974 - val_acc: 0.7552\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4769 - acc: 0.7726 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4768 - acc: 0.7726 - val_loss: 0.4973 - val_acc: 0.7552\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4768 - acc: 0.7726 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4767 - acc: 0.7726 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4766 - acc: 0.7726 - val_loss: 0.4972 - val_acc: 0.7552\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4766 - acc: 0.7726 - val_loss: 0.4971 - val_acc: 0.7500\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4765 - acc: 0.7726 - val_loss: 0.4971 - val_acc: 0.7552\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4765 - acc: 0.7726 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4764 - acc: 0.7726 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4763 - acc: 0.7726 - val_loss: 0.4970 - val_acc: 0.7552\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4763 - acc: 0.7726 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4762 - acc: 0.7726 - val_loss: 0.4969 - val_acc: 0.7552\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4762 - acc: 0.7726 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4761 - acc: 0.7726 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4761 - acc: 0.7726 - val_loss: 0.4968 - val_acc: 0.7552\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4760 - acc: 0.7726 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4759 - acc: 0.7726 - val_loss: 0.4967 - val_acc: 0.7552\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4759 - acc: 0.7726 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4758 - acc: 0.7726 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4758 - acc: 0.7726 - val_loss: 0.4966 - val_acc: 0.7552\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4757 - acc: 0.7726 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4756 - acc: 0.7726 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4756 - acc: 0.7726 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4756 - acc: 0.7726 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4755 - acc: 0.7726 - val_loss: 0.4964 - val_acc: 0.7552\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4755 - acc: 0.7726 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4754 - acc: 0.7726 - val_loss: 0.4963 - val_acc: 0.7552\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4753 - acc: 0.7708 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4753 - acc: 0.7726 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4752 - acc: 0.7708 - val_loss: 0.4962 - val_acc: 0.7552\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4752 - acc: 0.7726 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4751 - acc: 0.7726 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4751 - acc: 0.7708 - val_loss: 0.4961 - val_acc: 0.7552\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4750 - acc: 0.7726 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4749 - acc: 0.7726 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4749 - acc: 0.7708 - val_loss: 0.4960 - val_acc: 0.7552\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4748 - acc: 0.7726 - val_loss: 0.4959 - val_acc: 0.7552\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4748 - acc: 0.7708 - val_loss: 0.4959 - val_acc: 0.7552\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4747 - acc: 0.7726 - val_loss: 0.4958 - val_acc: 0.7552\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4747 - acc: 0.7708 - val_loss: 0.4958 - val_acc: 0.7552\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4746 - acc: 0.7708 - val_loss: 0.4958 - val_acc: 0.7552\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4746 - acc: 0.7708 - val_loss: 0.4957 - val_acc: 0.7552\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4745 - acc: 0.7708 - val_loss: 0.4957 - val_acc: 0.7552\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4745 - acc: 0.7726 - val_loss: 0.4957 - val_acc: 0.7552\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4744 - acc: 0.7708 - val_loss: 0.4956 - val_acc: 0.7552\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4744 - acc: 0.7726 - val_loss: 0.4956 - val_acc: 0.7552\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4743 - acc: 0.7726 - val_loss: 0.4956 - val_acc: 0.7552\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4743 - acc: 0.7708 - val_loss: 0.4955 - val_acc: 0.7552\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4742 - acc: 0.7708 - val_loss: 0.4955 - val_acc: 0.7552\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4742 - acc: 0.7708 - val_loss: 0.4955 - val_acc: 0.7552\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4741 - acc: 0.7708 - val_loss: 0.4954 - val_acc: 0.7552\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4741 - acc: 0.7708 - val_loss: 0.4954 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4740 - acc: 0.7708 - val_loss: 0.4953 - val_acc: 0.7552\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4740 - acc: 0.7708 - val_loss: 0.4953 - val_acc: 0.7552\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4739 - acc: 0.7708 - val_loss: 0.4953 - val_acc: 0.7552\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4739 - acc: 0.7708 - val_loss: 0.4952 - val_acc: 0.7552\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4738 - acc: 0.7708 - val_loss: 0.4952 - val_acc: 0.7552\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4738 - acc: 0.7708 - val_loss: 0.4952 - val_acc: 0.7552\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4737 - acc: 0.7708 - val_loss: 0.4951 - val_acc: 0.7552\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4737 - acc: 0.7708 - val_loss: 0.4951 - val_acc: 0.7552\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4736 - acc: 0.7708 - val_loss: 0.4951 - val_acc: 0.7552\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4736 - acc: 0.7708 - val_loss: 0.4950 - val_acc: 0.7552\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4735 - acc: 0.7708 - val_loss: 0.4950 - val_acc: 0.7552\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4735 - acc: 0.7708 - val_loss: 0.4950 - val_acc: 0.7552\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4734 - acc: 0.7708 - val_loss: 0.4950 - val_acc: 0.7552\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4734 - acc: 0.7708 - val_loss: 0.4949 - val_acc: 0.7552\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4734 - acc: 0.7708 - val_loss: 0.4949 - val_acc: 0.7552\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4733 - acc: 0.7708 - val_loss: 0.4949 - val_acc: 0.7552\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4732 - acc: 0.7708 - val_loss: 0.4948 - val_acc: 0.7552\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4732 - acc: 0.7708 - val_loss: 0.4948 - val_acc: 0.7552\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4732 - acc: 0.7708 - val_loss: 0.4948 - val_acc: 0.7552\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4731 - acc: 0.7708 - val_loss: 0.4947 - val_acc: 0.7552\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4731 - acc: 0.7708 - val_loss: 0.4947 - val_acc: 0.7552\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4730 - acc: 0.7708 - val_loss: 0.4947 - val_acc: 0.7552\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4730 - acc: 0.7708 - val_loss: 0.4946 - val_acc: 0.7552\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4729 - acc: 0.7708 - val_loss: 0.4946 - val_acc: 0.7552\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4729 - acc: 0.7708 - val_loss: 0.4946 - val_acc: 0.7552\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4729 - acc: 0.7708 - val_loss: 0.4945 - val_acc: 0.7552\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4728 - acc: 0.7708 - val_loss: 0.4945 - val_acc: 0.7552\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4727 - acc: 0.7708 - val_loss: 0.4945 - val_acc: 0.7552\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4727 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7552\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4726 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7552\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4726 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7552\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4725 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7552\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4725 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7552\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4725 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7552\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4724 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7552\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4724 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7552\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4724 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7552\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4723 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7552\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4723 - acc: 0.7708 - val_loss: 0.4942 - val_acc: 0.7552\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4722 - acc: 0.7708 - val_loss: 0.4941 - val_acc: 0.7552\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4722 - acc: 0.7708 - val_loss: 0.4941 - val_acc: 0.7552\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4721 - acc: 0.7708 - val_loss: 0.4941 - val_acc: 0.7552\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4721 - acc: 0.7708 - val_loss: 0.4940 - val_acc: 0.7552\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4720 - acc: 0.7708 - val_loss: 0.4940 - val_acc: 0.7552\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4720 - acc: 0.7708 - val_loss: 0.4940 - val_acc: 0.7552\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4720 - acc: 0.7708 - val_loss: 0.4940 - val_acc: 0.7552\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4719 - acc: 0.7708 - val_loss: 0.4939 - val_acc: 0.7552\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4719 - acc: 0.7708 - val_loss: 0.4939 - val_acc: 0.7552\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4718 - acc: 0.7726 - val_loss: 0.4939 - val_acc: 0.7552\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4718 - acc: 0.7708 - val_loss: 0.4938 - val_acc: 0.7552\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4717 - acc: 0.7726 - val_loss: 0.4938 - val_acc: 0.7552\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4717 - acc: 0.7726 - val_loss: 0.4938 - val_acc: 0.7552\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4717 - acc: 0.7726 - val_loss: 0.4938 - val_acc: 0.7552\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4716 - acc: 0.7726 - val_loss: 0.4937 - val_acc: 0.7552\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4716 - acc: 0.7726 - val_loss: 0.4937 - val_acc: 0.7552\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4715 - acc: 0.7726 - val_loss: 0.4937 - val_acc: 0.7552\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4715 - acc: 0.7726 - val_loss: 0.4937 - val_acc: 0.7552\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4715 - acc: 0.7726 - val_loss: 0.4936 - val_acc: 0.7552\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4714 - acc: 0.7726 - val_loss: 0.4936 - val_acc: 0.7552\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4714 - acc: 0.7726 - val_loss: 0.4936 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4713 - acc: 0.7726 - val_loss: 0.4935 - val_acc: 0.7552\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4713 - acc: 0.7726 - val_loss: 0.4935 - val_acc: 0.7552\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4712 - acc: 0.7726 - val_loss: 0.4935 - val_acc: 0.7552\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4712 - acc: 0.7726 - val_loss: 0.4935 - val_acc: 0.7552\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4712 - acc: 0.7726 - val_loss: 0.4934 - val_acc: 0.7552\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4711 - acc: 0.7726 - val_loss: 0.4934 - val_acc: 0.7552\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4711 - acc: 0.7726 - val_loss: 0.4934 - val_acc: 0.7552\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4710 - acc: 0.7726 - val_loss: 0.4934 - val_acc: 0.7552\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4710 - acc: 0.7726 - val_loss: 0.4933 - val_acc: 0.7552\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4710 - acc: 0.7726 - val_loss: 0.4933 - val_acc: 0.7552\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4709 - acc: 0.7726 - val_loss: 0.4933 - val_acc: 0.7552\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4709 - acc: 0.7726 - val_loss: 0.4933 - val_acc: 0.7552\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4709 - acc: 0.7726 - val_loss: 0.4932 - val_acc: 0.7552\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4708 - acc: 0.7726 - val_loss: 0.4932 - val_acc: 0.7552\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4708 - acc: 0.7726 - val_loss: 0.4932 - val_acc: 0.7552\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4707 - acc: 0.7726 - val_loss: 0.4932 - val_acc: 0.7552\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4707 - acc: 0.7726 - val_loss: 0.4931 - val_acc: 0.7552\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4706 - acc: 0.7726 - val_loss: 0.4931 - val_acc: 0.7552\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4706 - acc: 0.7726 - val_loss: 0.4931 - val_acc: 0.7552\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4706 - acc: 0.7726 - val_loss: 0.4931 - val_acc: 0.7552\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4705 - acc: 0.7726 - val_loss: 0.4930 - val_acc: 0.7552\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4705 - acc: 0.7726 - val_loss: 0.4930 - val_acc: 0.7552\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4705 - acc: 0.7726 - val_loss: 0.4930 - val_acc: 0.7552\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4704 - acc: 0.7726 - val_loss: 0.4930 - val_acc: 0.7552\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4704 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.7552\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4703 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.7552\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4703 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.7552\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4703 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.7552\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4702 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7552\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4702 - acc: 0.7726 - val_loss: 0.4928 - val_acc: 0.7552\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4702 - acc: 0.7726 - val_loss: 0.4928 - val_acc: 0.7552\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4701 - acc: 0.7743 - val_loss: 0.4928 - val_acc: 0.7552\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4701 - acc: 0.7743 - val_loss: 0.4927 - val_acc: 0.7552\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4700 - acc: 0.7743 - val_loss: 0.4927 - val_acc: 0.7552\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4700 - acc: 0.7760 - val_loss: 0.4927 - val_acc: 0.7552\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4700 - acc: 0.7760 - val_loss: 0.4927 - val_acc: 0.7552\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4700 - acc: 0.7743 - val_loss: 0.4927 - val_acc: 0.7552\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4699 - acc: 0.7743 - val_loss: 0.4926 - val_acc: 0.7552\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4699 - acc: 0.7743 - val_loss: 0.4926 - val_acc: 0.7552\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4698 - acc: 0.7743 - val_loss: 0.4926 - val_acc: 0.7552\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4698 - acc: 0.7760 - val_loss: 0.4926 - val_acc: 0.7552\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4698 - acc: 0.7743 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4697 - acc: 0.7743 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4697 - acc: 0.7778 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4697 - acc: 0.7760 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4696 - acc: 0.7778 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4696 - acc: 0.7778 - val_loss: 0.4924 - val_acc: 0.7552\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4696 - acc: 0.7778 - val_loss: 0.4924 - val_acc: 0.7552\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4696 - acc: 0.7795 - val_loss: 0.4924 - val_acc: 0.7552\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4695 - acc: 0.7778 - val_loss: 0.4924 - val_acc: 0.7552\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4694 - acc: 0.7778 - val_loss: 0.4923 - val_acc: 0.7552\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4694 - acc: 0.7778 - val_loss: 0.4923 - val_acc: 0.7552\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4694 - acc: 0.7760 - val_loss: 0.4923 - val_acc: 0.7552\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4694 - acc: 0.7760 - val_loss: 0.4923 - val_acc: 0.7552\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4693 - acc: 0.7778 - val_loss: 0.4923 - val_acc: 0.7552\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4693 - acc: 0.7760 - val_loss: 0.4922 - val_acc: 0.7552\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4693 - acc: 0.7760 - val_loss: 0.4922 - val_acc: 0.7552\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4692 - acc: 0.7778 - val_loss: 0.4922 - val_acc: 0.7552\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4692 - acc: 0.7778 - val_loss: 0.4922 - val_acc: 0.7552\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4692 - acc: 0.7778 - val_loss: 0.4922 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4691 - acc: 0.7778 - val_loss: 0.4921 - val_acc: 0.7552\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4691 - acc: 0.7778 - val_loss: 0.4921 - val_acc: 0.7552\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4691 - acc: 0.7778 - val_loss: 0.4921 - val_acc: 0.7552\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4690 - acc: 0.7778 - val_loss: 0.4921 - val_acc: 0.7552\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4690 - acc: 0.7778 - val_loss: 0.4921 - val_acc: 0.7552\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4689 - acc: 0.7778 - val_loss: 0.4920 - val_acc: 0.7552\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4689 - acc: 0.7778 - val_loss: 0.4920 - val_acc: 0.7552\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4689 - acc: 0.7778 - val_loss: 0.4920 - val_acc: 0.7552\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4689 - acc: 0.7778 - val_loss: 0.4920 - val_acc: 0.7552\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4688 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4688 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4688 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4688 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4687 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4687 - acc: 0.7778 - val_loss: 0.4919 - val_acc: 0.7552\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4687 - acc: 0.7778 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4686 - acc: 0.7778 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4686 - acc: 0.7778 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4685 - acc: 0.7778 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4685 - acc: 0.7778 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4685 - acc: 0.7778 - val_loss: 0.4917 - val_acc: 0.7552\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4685 - acc: 0.7778 - val_loss: 0.4917 - val_acc: 0.7552\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4684 - acc: 0.7795 - val_loss: 0.4917 - val_acc: 0.7552\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4684 - acc: 0.7795 - val_loss: 0.4917 - val_acc: 0.7552\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4684 - acc: 0.7795 - val_loss: 0.4917 - val_acc: 0.7552\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4683 - acc: 0.7778 - val_loss: 0.4916 - val_acc: 0.7552\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4683 - acc: 0.7778 - val_loss: 0.4916 - val_acc: 0.7552\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4683 - acc: 0.7795 - val_loss: 0.4916 - val_acc: 0.7552\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4682 - acc: 0.7795 - val_loss: 0.4916 - val_acc: 0.7552\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4682 - acc: 0.7795 - val_loss: 0.4916 - val_acc: 0.7552\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4682 - acc: 0.7795 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4682 - acc: 0.7778 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4680 - acc: 0.7795 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4680 - acc: 0.7795 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4680 - acc: 0.7795 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4680 - acc: 0.7795 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4679 - acc: 0.7795 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4679 - acc: 0.7795 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4678 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4678 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4678 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4678 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4677 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4677 - acc: 0.7795 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4677 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4677 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4676 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4676 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4676 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4675 - acc: 0.7795 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4675 - acc: 0.7795 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4675 - acc: 0.7795 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4675 - acc: 0.7795 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4674 - acc: 0.7812 - val_loss: 0.4911 - val_acc: 0.7604\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4674 - acc: 0.7812 - val_loss: 0.4911 - val_acc: 0.7604\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4674 - acc: 0.7812 - val_loss: 0.4911 - val_acc: 0.7604\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4674 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4673 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4673 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4673 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4672 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4672 - acc: 0.7812 - val_loss: 0.4910 - val_acc: 0.7604\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4672 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4672 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4671 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4671 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4671 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4670 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4670 - acc: 0.7812 - val_loss: 0.4909 - val_acc: 0.7604\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4670 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4670 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4670 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4669 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4669 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4669 - acc: 0.7812 - val_loss: 0.4908 - val_acc: 0.7604\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4669 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4668 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4668 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4668 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4667 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4667 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7604\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4667 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7656\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4667 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4665 - acc: 0.7830 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4665 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4665 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4665 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4664 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4664 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4664 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4664 - acc: 0.7830 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4663 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4663 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4663 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4663 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4663 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4662 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4662 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4662 - acc: 0.7830 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4661 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4661 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4661 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4661 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4660 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4660 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4660 - acc: 0.7830 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4660 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4660 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4659 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4659 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4659 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4659 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4658 - acc: 0.7830 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4658 - acc: 0.7847 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4658 - acc: 0.7812 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4658 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4656 - acc: 0.7830 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4656 - acc: 0.7847 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4656 - acc: 0.7830 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4656 - acc: 0.7830 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4656 - acc: 0.7847 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4655 - acc: 0.7847 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4655 - acc: 0.7830 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4655 - acc: 0.7830 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4655 - acc: 0.7830 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4654 - acc: 0.7830 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4654 - acc: 0.7830 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4652 - acc: 0.7830 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4652 - acc: 0.7812 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4650 - acc: 0.7830 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3992 - acc: 0.843 - 0s 43us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4650 - acc: 0.7830 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4649 - acc: 0.7830 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4649 - acc: 0.7830 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4648 - acc: 0.7830 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4648 - acc: 0.7830 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4648 - acc: 0.7830 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4647 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4647 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4647 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 599/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 45us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4646 - acc: 0.7830 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4645 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4645 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4645 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4645 - acc: 0.7830 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4645 - acc: 0.7830 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4645 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4644 - acc: 0.7830 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4644 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4644 - acc: 0.7830 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4644 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4643 - acc: 0.7847 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4644 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4643 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4643 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4643 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4643 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4642 - acc: 0.7830 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4642 - acc: 0.7812 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4642 - acc: 0.7830 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4642 - acc: 0.7847 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4642 - acc: 0.7812 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4642 - acc: 0.7812 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4641 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4641 - acc: 0.7830 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4641 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4641 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4641 - acc: 0.7830 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4641 - acc: 0.7830 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4640 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4640 - acc: 0.7830 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4640 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4640 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4640 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4639 - acc: 0.7812 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4639 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7656\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4639 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4639 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4639 - acc: 0.7830 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4639 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4637 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7604\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4637 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4637 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4637 - acc: 0.7830 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4637 - acc: 0.7830 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4637 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4637 - acc: 0.7830 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4636 - acc: 0.7830 - val_loss: 0.4890 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4636 - acc: 0.7830 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4635 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4634 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3622 - acc: 0.875 - 0s 42us/step - loss: 0.4634 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7604\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.4888 - val_acc: 0.7604\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4629 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4628 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4628 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4628 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4628 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 718/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 43us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4886 - val_acc: 0.7604\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4627 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7795 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4626 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7604\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4624 - acc: 0.7795 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4623 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7604\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4622 - acc: 0.7830 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4622 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4622 - acc: 0.7795 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4621 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7604\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4620 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4619 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4619 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4619 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4619 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4882 - val_acc: 0.7604\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4618 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4617 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4881 - val_acc: 0.7604\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4615 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4614 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4880 - val_acc: 0.7604\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4612 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4611 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4610 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7812 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4879 - val_acc: 0.7604\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4606 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4878 - val_acc: 0.7604\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7552\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4602 - acc: 0.7795 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4602 - acc: 0.7795 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4602 - acc: 0.7795 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4600 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4600 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4600 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4600 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4600 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7604\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2f956be0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid\n",
    "model_2 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(6, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "    \n",
    "])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 629us/step - loss: 0.6716 - acc: 0.6285 - val_loss: 0.6803 - val_acc: 0.6302\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6703 - acc: 0.6285 - val_loss: 0.6792 - val_acc: 0.6302\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6690 - acc: 0.6319 - val_loss: 0.6781 - val_acc: 0.6302\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6677 - acc: 0.6267 - val_loss: 0.6770 - val_acc: 0.6198\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6664 - acc: 0.6267 - val_loss: 0.6759 - val_acc: 0.6146\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6652 - acc: 0.6302 - val_loss: 0.6748 - val_acc: 0.6302\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6639 - acc: 0.6372 - val_loss: 0.6738 - val_acc: 0.6354\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.6627 - acc: 0.6372 - val_loss: 0.6728 - val_acc: 0.6354\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6615 - acc: 0.6389 - val_loss: 0.6718 - val_acc: 0.6406\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6604 - acc: 0.6424 - val_loss: 0.6708 - val_acc: 0.6354\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6592 - acc: 0.6424 - val_loss: 0.6698 - val_acc: 0.6354\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6580 - acc: 0.6476 - val_loss: 0.6688 - val_acc: 0.6354\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.6569 - acc: 0.6458 - val_loss: 0.6679 - val_acc: 0.6354\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6558 - acc: 0.6476 - val_loss: 0.6669 - val_acc: 0.6406\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6546 - acc: 0.6510 - val_loss: 0.6660 - val_acc: 0.6406\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6535 - acc: 0.6528 - val_loss: 0.6651 - val_acc: 0.6406\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6524 - acc: 0.6493 - val_loss: 0.6642 - val_acc: 0.6354\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6513 - acc: 0.6510 - val_loss: 0.6633 - val_acc: 0.6354\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6503 - acc: 0.6510 - val_loss: 0.6624 - val_acc: 0.6406\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6492 - acc: 0.6510 - val_loss: 0.6615 - val_acc: 0.6406\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6481 - acc: 0.6545 - val_loss: 0.6606 - val_acc: 0.6406\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6471 - acc: 0.6545 - val_loss: 0.6598 - val_acc: 0.6406\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6460 - acc: 0.6528 - val_loss: 0.6589 - val_acc: 0.6406\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6450 - acc: 0.6510 - val_loss: 0.6580 - val_acc: 0.6406\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6439 - acc: 0.6528 - val_loss: 0.6572 - val_acc: 0.6406\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6429 - acc: 0.6545 - val_loss: 0.6563 - val_acc: 0.6406\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6419 - acc: 0.6545 - val_loss: 0.6554 - val_acc: 0.6406\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6409 - acc: 0.6545 - val_loss: 0.6546 - val_acc: 0.6406\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6399 - acc: 0.6545 - val_loss: 0.6537 - val_acc: 0.6406\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6389 - acc: 0.6545 - val_loss: 0.6529 - val_acc: 0.6406\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6379 - acc: 0.6545 - val_loss: 0.6520 - val_acc: 0.6406\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6369 - acc: 0.6545 - val_loss: 0.6512 - val_acc: 0.6406\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6359 - acc: 0.6545 - val_loss: 0.6504 - val_acc: 0.6406\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6350 - acc: 0.6545 - val_loss: 0.6495 - val_acc: 0.6406\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6340 - acc: 0.6545 - val_loss: 0.6487 - val_acc: 0.6406\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6330 - acc: 0.6545 - val_loss: 0.6479 - val_acc: 0.6406\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6320 - acc: 0.6545 - val_loss: 0.6470 - val_acc: 0.6406\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6310 - acc: 0.6545 - val_loss: 0.6462 - val_acc: 0.6406\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6301 - acc: 0.6545 - val_loss: 0.6454 - val_acc: 0.6406\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6291 - acc: 0.6545 - val_loss: 0.6446 - val_acc: 0.6406\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6282 - acc: 0.6545 - val_loss: 0.6438 - val_acc: 0.6406\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6272 - acc: 0.6545 - val_loss: 0.6430 - val_acc: 0.6406\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6263 - acc: 0.6545 - val_loss: 0.6422 - val_acc: 0.6406\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6253 - acc: 0.6545 - val_loss: 0.6414 - val_acc: 0.6406\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6244 - acc: 0.6545 - val_loss: 0.6406 - val_acc: 0.6406\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6234 - acc: 0.6545 - val_loss: 0.6398 - val_acc: 0.6406\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6225 - acc: 0.6562 - val_loss: 0.6390 - val_acc: 0.6406\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6215 - acc: 0.6562 - val_loss: 0.6382 - val_acc: 0.6406\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6206 - acc: 0.6545 - val_loss: 0.6374 - val_acc: 0.6406\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6196 - acc: 0.6562 - val_loss: 0.6366 - val_acc: 0.6406\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6187 - acc: 0.6562 - val_loss: 0.6358 - val_acc: 0.6406\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6177 - acc: 0.6562 - val_loss: 0.6350 - val_acc: 0.6406\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6168 - acc: 0.6562 - val_loss: 0.6342 - val_acc: 0.6406\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6158 - acc: 0.6562 - val_loss: 0.6334 - val_acc: 0.6406\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6149 - acc: 0.6562 - val_loss: 0.6326 - val_acc: 0.6406\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.6139 - acc: 0.6562 - val_loss: 0.6318 - val_acc: 0.6406\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.6130 - acc: 0.6562 - val_loss: 0.6310 - val_acc: 0.6406\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6120 - acc: 0.6562 - val_loss: 0.6302 - val_acc: 0.6406\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6111 - acc: 0.6562 - val_loss: 0.6294 - val_acc: 0.6406\n",
      "Epoch 60/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6101 - acc: 0.6562 - val_loss: 0.6286 - val_acc: 0.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6092 - acc: 0.6562 - val_loss: 0.6278 - val_acc: 0.6406\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6082 - acc: 0.6562 - val_loss: 0.6270 - val_acc: 0.6406\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6073 - acc: 0.6545 - val_loss: 0.6263 - val_acc: 0.6406\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6064 - acc: 0.6545 - val_loss: 0.6255 - val_acc: 0.6406\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6054 - acc: 0.6545 - val_loss: 0.6247 - val_acc: 0.6406\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6045 - acc: 0.6545 - val_loss: 0.6239 - val_acc: 0.6406\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6035 - acc: 0.6545 - val_loss: 0.6231 - val_acc: 0.6406\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6025 - acc: 0.6545 - val_loss: 0.6223 - val_acc: 0.6406\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6016 - acc: 0.6545 - val_loss: 0.6215 - val_acc: 0.6406\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6006 - acc: 0.6545 - val_loss: 0.6207 - val_acc: 0.6406\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5996 - acc: 0.6545 - val_loss: 0.6199 - val_acc: 0.6406\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5986 - acc: 0.6545 - val_loss: 0.6191 - val_acc: 0.6406\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5976 - acc: 0.6562 - val_loss: 0.6183 - val_acc: 0.6406\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5966 - acc: 0.6562 - val_loss: 0.6175 - val_acc: 0.6406\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5956 - acc: 0.6562 - val_loss: 0.6167 - val_acc: 0.6406\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5945 - acc: 0.6562 - val_loss: 0.6159 - val_acc: 0.6406\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5935 - acc: 0.6562 - val_loss: 0.6151 - val_acc: 0.6406\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5925 - acc: 0.6562 - val_loss: 0.6142 - val_acc: 0.6406\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5915 - acc: 0.6562 - val_loss: 0.6134 - val_acc: 0.6406\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5904 - acc: 0.6562 - val_loss: 0.6126 - val_acc: 0.6406\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5894 - acc: 0.6562 - val_loss: 0.6118 - val_acc: 0.6406\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5884 - acc: 0.6562 - val_loss: 0.6110 - val_acc: 0.6406\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5874 - acc: 0.6562 - val_loss: 0.6102 - val_acc: 0.6406\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5864 - acc: 0.6562 - val_loss: 0.6094 - val_acc: 0.6406\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5853 - acc: 0.6562 - val_loss: 0.6086 - val_acc: 0.6406\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5843 - acc: 0.6580 - val_loss: 0.6077 - val_acc: 0.6406\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5833 - acc: 0.6580 - val_loss: 0.6069 - val_acc: 0.6406\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5822 - acc: 0.6597 - val_loss: 0.6061 - val_acc: 0.6406\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5812 - acc: 0.6615 - val_loss: 0.6053 - val_acc: 0.6406\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5801 - acc: 0.6615 - val_loss: 0.6045 - val_acc: 0.6406\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5791 - acc: 0.6615 - val_loss: 0.6037 - val_acc: 0.6406\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5780 - acc: 0.6615 - val_loss: 0.6028 - val_acc: 0.6406\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5770 - acc: 0.6615 - val_loss: 0.6020 - val_acc: 0.6406\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5759 - acc: 0.6615 - val_loss: 0.6012 - val_acc: 0.6406\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5749 - acc: 0.6615 - val_loss: 0.6004 - val_acc: 0.6406\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5739 - acc: 0.6615 - val_loss: 0.5996 - val_acc: 0.6406\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5728 - acc: 0.6615 - val_loss: 0.5988 - val_acc: 0.6406\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5718 - acc: 0.6597 - val_loss: 0.5980 - val_acc: 0.6406\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5708 - acc: 0.6597 - val_loss: 0.5972 - val_acc: 0.6406\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5698 - acc: 0.6580 - val_loss: 0.5965 - val_acc: 0.6406\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5688 - acc: 0.6580 - val_loss: 0.5957 - val_acc: 0.6406\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5678 - acc: 0.6580 - val_loss: 0.5950 - val_acc: 0.6406\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5668 - acc: 0.6597 - val_loss: 0.5942 - val_acc: 0.6406\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5658 - acc: 0.6597 - val_loss: 0.5934 - val_acc: 0.6406\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5648 - acc: 0.6597 - val_loss: 0.5927 - val_acc: 0.6406\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5638 - acc: 0.6597 - val_loss: 0.5920 - val_acc: 0.6458\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5628 - acc: 0.6597 - val_loss: 0.5912 - val_acc: 0.6458\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5618 - acc: 0.6597 - val_loss: 0.5905 - val_acc: 0.6458\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5608 - acc: 0.6597 - val_loss: 0.5898 - val_acc: 0.6458\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5598 - acc: 0.6597 - val_loss: 0.5891 - val_acc: 0.6510\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5588 - acc: 0.6597 - val_loss: 0.5883 - val_acc: 0.6510\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5578 - acc: 0.6597 - val_loss: 0.5876 - val_acc: 0.6510\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5568 - acc: 0.6615 - val_loss: 0.5869 - val_acc: 0.6510\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5558 - acc: 0.6667 - val_loss: 0.5862 - val_acc: 0.6562\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5548 - acc: 0.6684 - val_loss: 0.5855 - val_acc: 0.6510\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5539 - acc: 0.6667 - val_loss: 0.5848 - val_acc: 0.6510\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5529 - acc: 0.6667 - val_loss: 0.5841 - val_acc: 0.6458\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5519 - acc: 0.6684 - val_loss: 0.5834 - val_acc: 0.6458\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5509 - acc: 0.6701 - val_loss: 0.5827 - val_acc: 0.6510\n",
      "Epoch 120/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5500 - acc: 0.6701 - val_loss: 0.5820 - val_acc: 0.6510\n",
      "Epoch 121/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 50us/step - loss: 0.5490 - acc: 0.6684 - val_loss: 0.5813 - val_acc: 0.6510\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5481 - acc: 0.6667 - val_loss: 0.5806 - val_acc: 0.6510\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5471 - acc: 0.6667 - val_loss: 0.5799 - val_acc: 0.6562\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5462 - acc: 0.6667 - val_loss: 0.5792 - val_acc: 0.6615\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5452 - acc: 0.6701 - val_loss: 0.5786 - val_acc: 0.6615\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5443 - acc: 0.6719 - val_loss: 0.5779 - val_acc: 0.6615\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5434 - acc: 0.6701 - val_loss: 0.5772 - val_acc: 0.6615\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5424 - acc: 0.6736 - val_loss: 0.5766 - val_acc: 0.6615\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5415 - acc: 0.6753 - val_loss: 0.5759 - val_acc: 0.6615\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5406 - acc: 0.6788 - val_loss: 0.5752 - val_acc: 0.6562\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5396 - acc: 0.6806 - val_loss: 0.5746 - val_acc: 0.6562\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5387 - acc: 0.6823 - val_loss: 0.5739 - val_acc: 0.6615\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5378 - acc: 0.6823 - val_loss: 0.5732 - val_acc: 0.6615\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5369 - acc: 0.6823 - val_loss: 0.5726 - val_acc: 0.6719\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5360 - acc: 0.6823 - val_loss: 0.5720 - val_acc: 0.6719\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5351 - acc: 0.6840 - val_loss: 0.5713 - val_acc: 0.6823\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5342 - acc: 0.6840 - val_loss: 0.5707 - val_acc: 0.6927\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5333 - acc: 0.6858 - val_loss: 0.5701 - val_acc: 0.6979\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5325 - acc: 0.6858 - val_loss: 0.5695 - val_acc: 0.7031\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.5316 - acc: 0.6858 - val_loss: 0.5689 - val_acc: 0.7083\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5308 - acc: 0.6823 - val_loss: 0.5683 - val_acc: 0.7135\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5299 - acc: 0.6858 - val_loss: 0.5677 - val_acc: 0.7135\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5291 - acc: 0.6875 - val_loss: 0.5671 - val_acc: 0.7135\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5283 - acc: 0.6892 - val_loss: 0.5665 - val_acc: 0.7135\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5275 - acc: 0.6910 - val_loss: 0.5660 - val_acc: 0.7083\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5267 - acc: 0.6910 - val_loss: 0.5654 - val_acc: 0.7083\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5259 - acc: 0.6910 - val_loss: 0.5649 - val_acc: 0.7135\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5251 - acc: 0.6910 - val_loss: 0.5643 - val_acc: 0.7135\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5244 - acc: 0.6910 - val_loss: 0.5638 - val_acc: 0.7135\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5236 - acc: 0.6927 - val_loss: 0.5633 - val_acc: 0.7135\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5229 - acc: 0.6927 - val_loss: 0.5628 - val_acc: 0.7135\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5222 - acc: 0.6927 - val_loss: 0.5623 - val_acc: 0.7031\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5214 - acc: 0.6944 - val_loss: 0.5618 - val_acc: 0.7031\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5207 - acc: 0.6944 - val_loss: 0.5613 - val_acc: 0.7031\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5200 - acc: 0.6927 - val_loss: 0.5608 - val_acc: 0.7031\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5192 - acc: 0.6944 - val_loss: 0.5603 - val_acc: 0.7031\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5185 - acc: 0.6962 - val_loss: 0.5598 - val_acc: 0.7031\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5178 - acc: 0.6979 - val_loss: 0.5593 - val_acc: 0.7031\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5171 - acc: 0.6997 - val_loss: 0.5588 - val_acc: 0.7083\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5164 - acc: 0.6997 - val_loss: 0.5584 - val_acc: 0.7083\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5157 - acc: 0.7031 - val_loss: 0.5579 - val_acc: 0.7083\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5151 - acc: 0.7049 - val_loss: 0.5575 - val_acc: 0.7083\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5144 - acc: 0.7031 - val_loss: 0.5571 - val_acc: 0.6979\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5137 - acc: 0.7031 - val_loss: 0.5566 - val_acc: 0.6979\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5131 - acc: 0.7101 - val_loss: 0.5562 - val_acc: 0.7135\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5124 - acc: 0.7222 - val_loss: 0.5558 - val_acc: 0.7135\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5117 - acc: 0.7222 - val_loss: 0.5554 - val_acc: 0.7135\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5111 - acc: 0.7222 - val_loss: 0.5549 - val_acc: 0.7240\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5103 - acc: 0.7240 - val_loss: 0.5545 - val_acc: 0.7240\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5097 - acc: 0.7257 - val_loss: 0.5541 - val_acc: 0.7188\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5090 - acc: 0.7240 - val_loss: 0.5536 - val_acc: 0.7188\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5083 - acc: 0.7257 - val_loss: 0.5532 - val_acc: 0.7135\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5076 - acc: 0.7292 - val_loss: 0.5528 - val_acc: 0.7135\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5069 - acc: 0.7309 - val_loss: 0.5524 - val_acc: 0.7135\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5063 - acc: 0.7344 - val_loss: 0.5520 - val_acc: 0.7135\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5056 - acc: 0.7361 - val_loss: 0.5516 - val_acc: 0.7135\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5050 - acc: 0.7396 - val_loss: 0.5512 - val_acc: 0.7188\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5043 - acc: 0.7396 - val_loss: 0.5509 - val_acc: 0.7240\n",
      "Epoch 179/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5037 - acc: 0.7396 - val_loss: 0.5505 - val_acc: 0.7240\n",
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5031 - acc: 0.7413 - val_loss: 0.5501 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5025 - acc: 0.7535 - val_loss: 0.5498 - val_acc: 0.7240\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5018 - acc: 0.7587 - val_loss: 0.5494 - val_acc: 0.7240\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5012 - acc: 0.7587 - val_loss: 0.5490 - val_acc: 0.7292\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5006 - acc: 0.7639 - val_loss: 0.5487 - val_acc: 0.7292\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.5000 - acc: 0.7639 - val_loss: 0.5483 - val_acc: 0.7292\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4994 - acc: 0.7674 - val_loss: 0.5480 - val_acc: 0.7292\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4989 - acc: 0.7674 - val_loss: 0.5476 - val_acc: 0.7292\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4983 - acc: 0.7674 - val_loss: 0.5473 - val_acc: 0.7292\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4977 - acc: 0.7674 - val_loss: 0.5469 - val_acc: 0.7292\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4972 - acc: 0.7674 - val_loss: 0.5466 - val_acc: 0.7292\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4966 - acc: 0.7674 - val_loss: 0.5463 - val_acc: 0.7292\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4961 - acc: 0.7674 - val_loss: 0.5460 - val_acc: 0.7292\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4956 - acc: 0.7674 - val_loss: 0.5457 - val_acc: 0.7292\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4950 - acc: 0.7674 - val_loss: 0.5454 - val_acc: 0.7344\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4945 - acc: 0.7708 - val_loss: 0.5451 - val_acc: 0.7344\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4940 - acc: 0.7708 - val_loss: 0.5448 - val_acc: 0.7344\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4934 - acc: 0.7726 - val_loss: 0.5445 - val_acc: 0.7344\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4930 - acc: 0.7726 - val_loss: 0.5442 - val_acc: 0.7344\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4924 - acc: 0.7726 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4919 - acc: 0.7743 - val_loss: 0.5436 - val_acc: 0.7396\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4915 - acc: 0.7743 - val_loss: 0.5433 - val_acc: 0.7396\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4910 - acc: 0.7726 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4905 - acc: 0.7743 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4901 - acc: 0.7743 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4896 - acc: 0.7743 - val_loss: 0.5423 - val_acc: 0.7448\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4891 - acc: 0.7760 - val_loss: 0.5420 - val_acc: 0.7448\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4887 - acc: 0.7760 - val_loss: 0.5417 - val_acc: 0.7448\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4882 - acc: 0.7795 - val_loss: 0.5415 - val_acc: 0.7448\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4878 - acc: 0.7795 - val_loss: 0.5412 - val_acc: 0.7448\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4873 - acc: 0.7795 - val_loss: 0.5410 - val_acc: 0.7448\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4870 - acc: 0.7778 - val_loss: 0.5407 - val_acc: 0.7344\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4865 - acc: 0.7778 - val_loss: 0.5405 - val_acc: 0.7344\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4861 - acc: 0.7778 - val_loss: 0.5403 - val_acc: 0.7396\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4857 - acc: 0.7778 - val_loss: 0.5400 - val_acc: 0.7396\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4852 - acc: 0.7795 - val_loss: 0.5398 - val_acc: 0.7396\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4848 - acc: 0.7778 - val_loss: 0.5396 - val_acc: 0.7396\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4844 - acc: 0.7778 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4840 - acc: 0.7760 - val_loss: 0.5391 - val_acc: 0.7396\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4836 - acc: 0.7778 - val_loss: 0.5389 - val_acc: 0.7396\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4832 - acc: 0.7778 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4829 - acc: 0.7778 - val_loss: 0.5385 - val_acc: 0.7396\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4825 - acc: 0.7778 - val_loss: 0.5383 - val_acc: 0.7396\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4821 - acc: 0.7760 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4818 - acc: 0.7778 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4814 - acc: 0.7778 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4810 - acc: 0.7760 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4806 - acc: 0.7760 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4803 - acc: 0.7760 - val_loss: 0.5371 - val_acc: 0.7448\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4799 - acc: 0.7760 - val_loss: 0.5369 - val_acc: 0.7448\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4796 - acc: 0.7760 - val_loss: 0.5368 - val_acc: 0.7448\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4793 - acc: 0.7743 - val_loss: 0.5366 - val_acc: 0.7448\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4789 - acc: 0.7743 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4785 - acc: 0.7743 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4782 - acc: 0.7743 - val_loss: 0.5361 - val_acc: 0.7448\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4779 - acc: 0.7743 - val_loss: 0.5359 - val_acc: 0.7448\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4776 - acc: 0.7743 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4772 - acc: 0.7778 - val_loss: 0.5356 - val_acc: 0.7396\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4769 - acc: 0.7778 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4767 - acc: 0.7778 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4763 - acc: 0.7778 - val_loss: 0.5352 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4760 - acc: 0.7778 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4757 - acc: 0.7778 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4754 - acc: 0.7778 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4751 - acc: 0.7778 - val_loss: 0.5346 - val_acc: 0.7396\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4749 - acc: 0.7778 - val_loss: 0.5345 - val_acc: 0.7448\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4746 - acc: 0.7778 - val_loss: 0.5344 - val_acc: 0.7448\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4743 - acc: 0.7795 - val_loss: 0.5343 - val_acc: 0.7448\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4740 - acc: 0.7812 - val_loss: 0.5342 - val_acc: 0.7448\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4737 - acc: 0.7795 - val_loss: 0.5340 - val_acc: 0.7448\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4735 - acc: 0.7812 - val_loss: 0.5339 - val_acc: 0.7448\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4732 - acc: 0.7778 - val_loss: 0.5338 - val_acc: 0.7500\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4729 - acc: 0.7760 - val_loss: 0.5337 - val_acc: 0.7500\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4726 - acc: 0.7760 - val_loss: 0.5336 - val_acc: 0.7500\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4724 - acc: 0.7743 - val_loss: 0.5335 - val_acc: 0.7500\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4721 - acc: 0.7743 - val_loss: 0.5334 - val_acc: 0.7500\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4718 - acc: 0.7760 - val_loss: 0.5333 - val_acc: 0.7500\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4716 - acc: 0.7760 - val_loss: 0.5332 - val_acc: 0.7500\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4713 - acc: 0.7760 - val_loss: 0.5331 - val_acc: 0.7500\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4711 - acc: 0.7760 - val_loss: 0.5330 - val_acc: 0.7500\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4708 - acc: 0.7760 - val_loss: 0.5329 - val_acc: 0.7500\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4705 - acc: 0.7760 - val_loss: 0.5328 - val_acc: 0.7500\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4703 - acc: 0.7760 - val_loss: 0.5327 - val_acc: 0.7500\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4700 - acc: 0.7760 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4698 - acc: 0.7760 - val_loss: 0.5325 - val_acc: 0.7448\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4696 - acc: 0.7760 - val_loss: 0.5324 - val_acc: 0.7448\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4693 - acc: 0.7760 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4691 - acc: 0.7760 - val_loss: 0.5322 - val_acc: 0.7448\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4689 - acc: 0.7778 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4686 - acc: 0.7778 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4684 - acc: 0.7795 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4679 - acc: 0.7795 - val_loss: 0.5318 - val_acc: 0.7448\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4677 - acc: 0.7812 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4675 - acc: 0.7812 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4673 - acc: 0.7830 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4671 - acc: 0.7830 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4669 - acc: 0.7847 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4667 - acc: 0.7847 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4665 - acc: 0.7847 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4663 - acc: 0.7847 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4661 - acc: 0.7847 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4659 - acc: 0.7847 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4657 - acc: 0.7847 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4655 - acc: 0.7847 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4649 - acc: 0.7830 - val_loss: 0.5308 - val_acc: 0.7500\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4648 - acc: 0.7830 - val_loss: 0.5308 - val_acc: 0.7500\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4646 - acc: 0.7830 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4644 - acc: 0.7812 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4642 - acc: 0.7812 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4641 - acc: 0.7812 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4639 - acc: 0.7830 - val_loss: 0.5305 - val_acc: 0.7500\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4637 - acc: 0.7812 - val_loss: 0.5305 - val_acc: 0.7500\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4636 - acc: 0.7812 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4634 - acc: 0.7830 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4633 - acc: 0.7830 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 298/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4631 - acc: 0.7830 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4629 - acc: 0.7830 - val_loss: 0.5302 - val_acc: 0.7500\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4627 - acc: 0.7830 - val_loss: 0.5302 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4626 - acc: 0.7830 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4624 - acc: 0.7830 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4623 - acc: 0.7830 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4621 - acc: 0.7830 - val_loss: 0.5300 - val_acc: 0.7448\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4620 - acc: 0.7830 - val_loss: 0.5300 - val_acc: 0.7448\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4618 - acc: 0.7830 - val_loss: 0.5299 - val_acc: 0.7448\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4617 - acc: 0.7812 - val_loss: 0.5299 - val_acc: 0.7448\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4615 - acc: 0.7812 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.7830 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4612 - acc: 0.7812 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4611 - acc: 0.7812 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4608 - acc: 0.7795 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4605 - acc: 0.7812 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4604 - acc: 0.7812 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4602 - acc: 0.7812 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4601 - acc: 0.7812 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4600 - acc: 0.7812 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4599 - acc: 0.7812 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4597 - acc: 0.7812 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4596 - acc: 0.7812 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4595 - acc: 0.7812 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4594 - acc: 0.7812 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4592 - acc: 0.7812 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4591 - acc: 0.7812 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4590 - acc: 0.7795 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4587 - acc: 0.7795 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4583 - acc: 0.7795 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4579 - acc: 0.7812 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4579 - acc: 0.7795 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4577 - acc: 0.7778 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4576 - acc: 0.7795 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4575 - acc: 0.7795 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4574 - acc: 0.7795 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4573 - acc: 0.7795 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4572 - acc: 0.7795 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4571 - acc: 0.7795 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4570 - acc: 0.7795 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4569 - acc: 0.7795 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4568 - acc: 0.7795 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4567 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4566 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4565 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4564 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4563 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4562 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4561 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4561 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4559 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4559 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4558 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4557 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4556 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4555 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4554 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4553 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4552 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4551 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4551 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4549 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4548 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4548 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4547 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4546 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4545 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4545 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4543 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4543 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4542 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4541 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4540 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4540 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4539 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4538 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4537 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4536 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4535 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4535 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4534 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4531 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4528 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4526 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4525 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4525 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4524 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4523 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7448\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4522 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7448\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4522 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7448\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4521 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7448\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4520 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7448\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4520 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4519 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4518 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4517 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4517 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4514 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4513 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4513 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4512 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4511 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7396\n",
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4510 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7396\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4510 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4509 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4509 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4508 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4506 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4505 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4505 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.7448\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4504 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4504 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4503 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4502 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4501 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4500 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4500 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4499 - acc: 0.7795 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.5284 - val_acc: 0.7448\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4496 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4495 - acc: 0.7778 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4495 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4494 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4494 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4493 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4492 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4492 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4491 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4490 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7448\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4489 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4489 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4488 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4488 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4487 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4486 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4486 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4485 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4484 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4484 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4483 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4483 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4482 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4482 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4481 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4480 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4480 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4479 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4478 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4478 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4477 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4476 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4476 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4475 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4474 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4474 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4473 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4473 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4472 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4471 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4471 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4470 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4470 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4469 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4468 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4468 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4467 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4466 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4466 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4466 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4465 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4464 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4464 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4463 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4462 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4462 - acc: 0.7795 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4461 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4460 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4459 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4459 - acc: 0.7830 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4458 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4458 - acc: 0.7812 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4457 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4456 - acc: 0.7812 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4456 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4455 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4454 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4454 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4453 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4452 - acc: 0.7830 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4452 - acc: 0.7847 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4451 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4450 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4450 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4449 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4449 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4448 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4447 - acc: 0.7865 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4446 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4446 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4446 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7552\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4445 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7552\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4444 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7552\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4444 - acc: 0.7865 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4443 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4441 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4441 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4440 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4440 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4439 - acc: 0.7865 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4438 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4435 - acc: 0.7882 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4435 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 538/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4434 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4434 - acc: 0.7865 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4433 - acc: 0.7882 - val_loss: 0.5288 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4432 - acc: 0.7882 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4432 - acc: 0.7882 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4431 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7552\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4431 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7552\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4430 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7552\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4430 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4429 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4428 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4428 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4427 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4427 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4426 - acc: 0.7882 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4426 - acc: 0.7882 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4425 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4424 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4424 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4421 - acc: 0.7865 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4421 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4420 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4418 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4418 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7448\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7448\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4416 - acc: 0.7865 - val_loss: 0.5291 - val_acc: 0.7448\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4416 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4415 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4415 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4414 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4414 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4413 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4413 - acc: 0.7865 - val_loss: 0.5292 - val_acc: 0.7448\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4412 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4412 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4411 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4411 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4410 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4410 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4408 - acc: 0.7882 - val_loss: 0.5293 - val_acc: 0.7448\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4408 - acc: 0.7865 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4407 - acc: 0.7882 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4407 - acc: 0.7882 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4406 - acc: 0.7865 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4406 - acc: 0.7882 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4405 - acc: 0.7899 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4405 - acc: 0.7899 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.5294 - val_acc: 0.7448\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4403 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4403 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 597/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4402 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4402 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4401 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4401 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.5295 - val_acc: 0.7396\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4400 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4399 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4398 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4397 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4397 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4396 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4396 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4395 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4395 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4394 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4393 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4393 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4393 - acc: 0.7882 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4392 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4392 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4391 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4391 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4390 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4389 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7396\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4389 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4389 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4388 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7396\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4387 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4386 - acc: 0.7934 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4386 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4385 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4384 - acc: 0.7934 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4384 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4384 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4383 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4382 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4382 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4381 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4381 - acc: 0.7934 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4380 - acc: 0.7934 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4380 - acc: 0.7899 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4379 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4378 - acc: 0.7917 - val_loss: 0.5296 - val_acc: 0.7448\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4377 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4377 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4376 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4376 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4376 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4375 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4374 - acc: 0.7917 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4374 - acc: 0.7899 - val_loss: 0.5297 - val_acc: 0.7448\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4373 - acc: 0.7899 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4373 - acc: 0.7917 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4372 - acc: 0.7899 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4372 - acc: 0.7899 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4371 - acc: 0.7899 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4371 - acc: 0.7917 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4370 - acc: 0.7917 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4370 - acc: 0.7899 - val_loss: 0.5299 - val_acc: 0.7448\n",
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4369 - acc: 0.7917 - val_loss: 0.5299 - val_acc: 0.7448\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4368 - acc: 0.7899 - val_loss: 0.5299 - val_acc: 0.7448\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4368 - acc: 0.7934 - val_loss: 0.5299 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4367 - acc: 0.7917 - val_loss: 0.5299 - val_acc: 0.7500\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4367 - acc: 0.7917 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4366 - acc: 0.7917 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4366 - acc: 0.7917 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4365 - acc: 0.7934 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4364 - acc: 0.7934 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4364 - acc: 0.7917 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4363 - acc: 0.7917 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4363 - acc: 0.7934 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 670/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4362 - acc: 0.7934 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 671/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4362 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 672/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4362 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 673/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4361 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 674/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4360 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 675/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4360 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 676/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4360 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 677/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4359 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 678/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4359 - acc: 0.7934 - val_loss: 0.5301 - val_acc: 0.7448\n",
      "Epoch 679/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4358 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 680/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4358 - acc: 0.7917 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 681/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4358 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 682/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4357 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 683/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4356 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 684/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4356 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 685/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4355 - acc: 0.7917 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 686/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4355 - acc: 0.7934 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 687/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4355 - acc: 0.7917 - val_loss: 0.5302 - val_acc: 0.7448\n",
      "Epoch 688/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4354 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 689/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4354 - acc: 0.7934 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 690/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 691/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 692/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4353 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 693/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4352 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 694/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4351 - acc: 0.7917 - val_loss: 0.5303 - val_acc: 0.7448\n",
      "Epoch 695/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4351 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 696/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4351 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 697/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4350 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 698/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4350 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 699/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4349 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 700/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4349 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7448\n",
      "Epoch 701/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4349 - acc: 0.7917 - val_loss: 0.5304 - val_acc: 0.7396\n",
      "Epoch 702/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 703/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 704/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4348 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 705/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4347 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 706/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4347 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 707/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4346 - acc: 0.7917 - val_loss: 0.5305 - val_acc: 0.7396\n",
      "Epoch 708/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4346 - acc: 0.7917 - val_loss: 0.5306 - val_acc: 0.7396\n",
      "Epoch 709/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4345 - acc: 0.7917 - val_loss: 0.5306 - val_acc: 0.7396\n",
      "Epoch 710/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4345 - acc: 0.7917 - val_loss: 0.5306 - val_acc: 0.7396\n",
      "Epoch 711/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4345 - acc: 0.7917 - val_loss: 0.5306 - val_acc: 0.7448\n",
      "Epoch 712/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4344 - acc: 0.7917 - val_loss: 0.5306 - val_acc: 0.7448\n",
      "Epoch 713/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4344 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 714/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4343 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 715/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4343 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 716/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 717/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 718/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.5307 - val_acc: 0.7448\n",
      "Epoch 719/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.5308 - val_acc: 0.7448\n",
      "Epoch 720/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.5308 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.5308 - val_acc: 0.7448\n",
      "Epoch 722/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.5308 - val_acc: 0.7448\n",
      "Epoch 723/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 724/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 725/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 726/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4339 - acc: 0.7934 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 727/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.5309 - val_acc: 0.7448\n",
      "Epoch 728/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4338 - acc: 0.7934 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 729/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4338 - acc: 0.7934 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 730/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4337 - acc: 0.7934 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 731/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 732/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 733/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 734/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 735/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4335 - acc: 0.7934 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 736/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 737/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 738/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 739/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 740/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 741/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 742/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 743/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.5312 - val_acc: 0.7448\n",
      "Epoch 744/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 745/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 746/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 747/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 748/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 749/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.5313 - val_acc: 0.7448\n",
      "Epoch 750/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 751/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 752/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 753/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 754/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 755/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 756/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 757/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 758/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 759/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4327 - acc: 0.7951 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 760/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.5315 - val_acc: 0.7448\n",
      "Epoch 761/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 762/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 763/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 764/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 765/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 766/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 767/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 768/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4324 - acc: 0.7951 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 769/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4324 - acc: 0.7951 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 770/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4324 - acc: 0.7951 - val_loss: 0.5318 - val_acc: 0.7448\n",
      "Epoch 771/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4323 - acc: 0.7969 - val_loss: 0.5318 - val_acc: 0.7448\n",
      "Epoch 772/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4323 - acc: 0.7951 - val_loss: 0.5318 - val_acc: 0.7448\n",
      "Epoch 773/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4323 - acc: 0.7951 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 774/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4323 - acc: 0.7969 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 775/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 776/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4322 - acc: 0.7969 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 777/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4322 - acc: 0.7969 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 778/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4321 - acc: 0.7951 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 779/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4321 - acc: 0.7951 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 780/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4321 - acc: 0.7969 - val_loss: 0.5321 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 781/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4320 - acc: 0.7951 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 782/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4320 - acc: 0.7969 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 783/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4320 - acc: 0.7969 - val_loss: 0.5322 - val_acc: 0.7448\n",
      "Epoch 784/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4320 - acc: 0.7951 - val_loss: 0.5322 - val_acc: 0.7448\n",
      "Epoch 785/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4319 - acc: 0.7969 - val_loss: 0.5322 - val_acc: 0.7448\n",
      "Epoch 786/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4319 - acc: 0.7969 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 787/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4319 - acc: 0.7969 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 788/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4318 - acc: 0.7969 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 789/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4318 - acc: 0.7969 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 790/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4318 - acc: 0.7969 - val_loss: 0.5324 - val_acc: 0.7448\n",
      "Epoch 791/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4318 - acc: 0.7969 - val_loss: 0.5324 - val_acc: 0.7448\n",
      "Epoch 792/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4317 - acc: 0.7969 - val_loss: 0.5324 - val_acc: 0.7448\n",
      "Epoch 793/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4317 - acc: 0.7969 - val_loss: 0.5325 - val_acc: 0.7448\n",
      "Epoch 794/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4317 - acc: 0.7969 - val_loss: 0.5325 - val_acc: 0.7448\n",
      "Epoch 795/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4316 - acc: 0.7969 - val_loss: 0.5325 - val_acc: 0.7448\n",
      "Epoch 796/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4316 - acc: 0.7969 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 797/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4316 - acc: 0.7969 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 798/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4316 - acc: 0.7969 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 799/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4315 - acc: 0.7969 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 800/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4315 - acc: 0.7969 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 801/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4315 - acc: 0.7969 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 802/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 803/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 804/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 805/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 806/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5328 - val_acc: 0.7448\n",
      "Epoch 807/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4314 - acc: 0.7969 - val_loss: 0.5328 - val_acc: 0.7448\n",
      "Epoch 808/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4313 - acc: 0.7969 - val_loss: 0.5328 - val_acc: 0.7448\n",
      "Epoch 809/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4313 - acc: 0.7969 - val_loss: 0.5329 - val_acc: 0.7448\n",
      "Epoch 810/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4313 - acc: 0.7969 - val_loss: 0.5329 - val_acc: 0.7448\n",
      "Epoch 811/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4313 - acc: 0.7969 - val_loss: 0.5329 - val_acc: 0.7448\n",
      "Epoch 812/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4312 - acc: 0.7969 - val_loss: 0.5330 - val_acc: 0.7448\n",
      "Epoch 813/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4312 - acc: 0.7969 - val_loss: 0.5330 - val_acc: 0.7396\n",
      "Epoch 814/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5330 - val_acc: 0.7396\n",
      "Epoch 815/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5331 - val_acc: 0.7396\n",
      "Epoch 816/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5331 - val_acc: 0.7396\n",
      "Epoch 817/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5331 - val_acc: 0.7396\n",
      "Epoch 818/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4311 - acc: 0.7969 - val_loss: 0.5332 - val_acc: 0.7396\n",
      "Epoch 819/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4310 - acc: 0.7969 - val_loss: 0.5332 - val_acc: 0.7396\n",
      "Epoch 820/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4310 - acc: 0.7969 - val_loss: 0.5332 - val_acc: 0.7396\n",
      "Epoch 821/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4310 - acc: 0.7986 - val_loss: 0.5333 - val_acc: 0.7396\n",
      "Epoch 822/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4310 - acc: 0.7969 - val_loss: 0.5333 - val_acc: 0.7396\n",
      "Epoch 823/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4309 - acc: 0.7969 - val_loss: 0.5333 - val_acc: 0.7396\n",
      "Epoch 824/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4309 - acc: 0.7986 - val_loss: 0.5334 - val_acc: 0.7396\n",
      "Epoch 825/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4309 - acc: 0.7986 - val_loss: 0.5334 - val_acc: 0.7396\n",
      "Epoch 826/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4308 - acc: 0.7969 - val_loss: 0.5334 - val_acc: 0.7396\n",
      "Epoch 827/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4308 - acc: 0.7969 - val_loss: 0.5335 - val_acc: 0.7396\n",
      "Epoch 828/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4308 - acc: 0.7969 - val_loss: 0.5335 - val_acc: 0.7396\n",
      "Epoch 829/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4308 - acc: 0.7986 - val_loss: 0.5336 - val_acc: 0.7396\n",
      "Epoch 830/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4307 - acc: 0.7986 - val_loss: 0.5336 - val_acc: 0.7396\n",
      "Epoch 831/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4307 - acc: 0.7986 - val_loss: 0.5336 - val_acc: 0.7396\n",
      "Epoch 832/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4306 - acc: 0.7986 - val_loss: 0.5336 - val_acc: 0.7396\n",
      "Epoch 833/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4306 - acc: 0.7969 - val_loss: 0.5337 - val_acc: 0.7396\n",
      "Epoch 834/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4306 - acc: 0.7986 - val_loss: 0.5337 - val_acc: 0.7396\n",
      "Epoch 835/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4306 - acc: 0.7986 - val_loss: 0.5337 - val_acc: 0.7396\n",
      "Epoch 836/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4305 - acc: 0.7986 - val_loss: 0.5338 - val_acc: 0.7396\n",
      "Epoch 837/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4305 - acc: 0.7986 - val_loss: 0.5338 - val_acc: 0.7396\n",
      "Epoch 838/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4305 - acc: 0.7986 - val_loss: 0.5338 - val_acc: 0.7396\n",
      "Epoch 839/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4305 - acc: 0.7969 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 840/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4304 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4304 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 842/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4304 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 843/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4304 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 844/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4303 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 845/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4303 - acc: 0.7986 - val_loss: 0.5339 - val_acc: 0.7396\n",
      "Epoch 846/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4303 - acc: 0.7986 - val_loss: 0.5340 - val_acc: 0.7396\n",
      "Epoch 847/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4303 - acc: 0.7986 - val_loss: 0.5340 - val_acc: 0.7396\n",
      "Epoch 848/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.7986 - val_loss: 0.5340 - val_acc: 0.7396\n",
      "Epoch 849/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4302 - acc: 0.7986 - val_loss: 0.5340 - val_acc: 0.7396\n",
      "Epoch 850/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4301 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 851/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4301 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 852/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4301 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 853/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5102 - acc: 0.781 - 0s 45us/step - loss: 0.4301 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 854/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4300 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 855/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4300 - acc: 0.7986 - val_loss: 0.5341 - val_acc: 0.7396\n",
      "Epoch 856/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4300 - acc: 0.7986 - val_loss: 0.5342 - val_acc: 0.7396\n",
      "Epoch 857/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4300 - acc: 0.7986 - val_loss: 0.5342 - val_acc: 0.7396\n",
      "Epoch 858/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4300 - acc: 0.7986 - val_loss: 0.5342 - val_acc: 0.7396\n",
      "Epoch 859/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4299 - acc: 0.7986 - val_loss: 0.5342 - val_acc: 0.7396\n",
      "Epoch 860/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4299 - acc: 0.7986 - val_loss: 0.5343 - val_acc: 0.7396\n",
      "Epoch 861/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4298 - acc: 0.7986 - val_loss: 0.5343 - val_acc: 0.7396\n",
      "Epoch 862/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4299 - acc: 0.7986 - val_loss: 0.5343 - val_acc: 0.7396\n",
      "Epoch 863/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4298 - acc: 0.7986 - val_loss: 0.5343 - val_acc: 0.7396\n",
      "Epoch 864/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4298 - acc: 0.7986 - val_loss: 0.5343 - val_acc: 0.7396\n",
      "Epoch 865/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4298 - acc: 0.7986 - val_loss: 0.5344 - val_acc: 0.7396\n",
      "Epoch 866/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4297 - acc: 0.7986 - val_loss: 0.5344 - val_acc: 0.7396\n",
      "Epoch 867/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4297 - acc: 0.7986 - val_loss: 0.5344 - val_acc: 0.7396\n",
      "Epoch 868/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4297 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 869/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4297 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 870/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4296 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 871/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4296 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 872/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4296 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 873/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4296 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 874/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4295 - acc: 0.7986 - val_loss: 0.5345 - val_acc: 0.7396\n",
      "Epoch 875/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4295 - acc: 0.7986 - val_loss: 0.5346 - val_acc: 0.7396\n",
      "Epoch 876/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4295 - acc: 0.7986 - val_loss: 0.5346 - val_acc: 0.7396\n",
      "Epoch 877/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5346 - val_acc: 0.7396\n",
      "Epoch 878/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5346 - val_acc: 0.7396\n",
      "Epoch 879/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5347 - val_acc: 0.7396\n",
      "Epoch 880/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5347 - val_acc: 0.7396\n",
      "Epoch 881/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5347 - val_acc: 0.7396\n",
      "Epoch 882/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5347 - val_acc: 0.7396\n",
      "Epoch 883/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5347 - val_acc: 0.7396\n",
      "Epoch 884/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 885/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 886/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 887/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 888/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4292 - acc: 0.8003 - val_loss: 0.5348 - val_acc: 0.7396\n",
      "Epoch 889/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4292 - acc: 0.8003 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 890/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4292 - acc: 0.8003 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 891/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4291 - acc: 0.7986 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 892/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4291 - acc: 0.7986 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 893/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4291 - acc: 0.8003 - val_loss: 0.5349 - val_acc: 0.7396\n",
      "Epoch 894/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 895/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4290 - acc: 0.8003 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 896/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 897/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4290 - acc: 0.8021 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 898/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4290 - acc: 0.8003 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 899/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4290 - acc: 0.8003 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 900/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 39us/step - loss: 0.4289 - acc: 0.8021 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 901/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4289 - acc: 0.8003 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 902/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4289 - acc: 0.8003 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 903/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 904/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5351 - val_acc: 0.7396\n",
      "Epoch 905/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 906/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 907/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 908/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4288 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 909/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4287 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 910/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4287 - acc: 0.8021 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 911/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4287 - acc: 0.8003 - val_loss: 0.5352 - val_acc: 0.7396\n",
      "Epoch 912/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 913/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 914/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 915/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 916/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5353 - val_acc: 0.7396\n",
      "Epoch 917/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4286 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 918/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4285 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 919/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4285 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 920/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4285 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 921/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4285 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 922/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4285 - acc: 0.8003 - val_loss: 0.5354 - val_acc: 0.7396\n",
      "Epoch 923/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4284 - acc: 0.8003 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 924/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4284 - acc: 0.8021 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 925/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4284 - acc: 0.8003 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 926/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.8003 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 927/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4284 - acc: 0.8021 - val_loss: 0.5355 - val_acc: 0.7396\n",
      "Epoch 928/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4283 - acc: 0.8003 - val_loss: 0.5356 - val_acc: 0.7396\n",
      "Epoch 929/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4283 - acc: 0.8021 - val_loss: 0.5356 - val_acc: 0.7396\n",
      "Epoch 930/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4283 - acc: 0.8003 - val_loss: 0.5356 - val_acc: 0.7396\n",
      "Epoch 931/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4283 - acc: 0.8003 - val_loss: 0.5356 - val_acc: 0.7396\n",
      "Epoch 932/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4283 - acc: 0.8003 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 933/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4282 - acc: 0.8021 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 934/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4282 - acc: 0.8021 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 935/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4282 - acc: 0.8003 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 936/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4282 - acc: 0.8021 - val_loss: 0.5357 - val_acc: 0.7396\n",
      "Epoch 937/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4282 - acc: 0.8021 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 938/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4281 - acc: 0.8003 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 939/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4281 - acc: 0.8003 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 940/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4281 - acc: 0.8021 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 941/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4281 - acc: 0.8003 - val_loss: 0.5358 - val_acc: 0.7396\n",
      "Epoch 942/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4281 - acc: 0.8003 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 943/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4281 - acc: 0.8021 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 944/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4280 - acc: 0.8003 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 945/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4280 - acc: 0.8003 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 946/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4280 - acc: 0.8021 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 947/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4280 - acc: 0.8021 - val_loss: 0.5359 - val_acc: 0.7396\n",
      "Epoch 948/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4280 - acc: 0.8003 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 949/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4279 - acc: 0.8021 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 950/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8021 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 951/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8021 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 952/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8003 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 953/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8003 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 954/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8021 - val_loss: 0.5360 - val_acc: 0.7396\n",
      "Epoch 955/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8003 - val_loss: 0.5361 - val_acc: 0.7396\n",
      "Epoch 956/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4279 - acc: 0.8021 - val_loss: 0.5361 - val_acc: 0.7396\n",
      "Epoch 957/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4278 - acc: 0.8021 - val_loss: 0.5361 - val_acc: 0.7396\n",
      "Epoch 958/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4278 - acc: 0.8003 - val_loss: 0.5361 - val_acc: 0.7396\n",
      "Epoch 959/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4278 - acc: 0.8021 - val_loss: 0.5361 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 960/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4278 - acc: 0.8021 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 961/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4278 - acc: 0.8003 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 962/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4277 - acc: 0.8021 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 963/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4277 - acc: 0.8003 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 964/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4277 - acc: 0.8021 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 965/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4277 - acc: 0.8021 - val_loss: 0.5363 - val_acc: 0.7396\n",
      "Epoch 966/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4277 - acc: 0.8021 - val_loss: 0.5363 - val_acc: 0.7396\n",
      "Epoch 967/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5363 - val_acc: 0.7396\n",
      "Epoch 968/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 969/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 970/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 971/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 972/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4275 - acc: 0.8021 - val_loss: 0.5364 - val_acc: 0.7396\n",
      "Epoch 973/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4275 - acc: 0.8038 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 974/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 975/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4275 - acc: 0.8021 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 976/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4275 - acc: 0.8021 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 977/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4275 - acc: 0.8003 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 978/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4275 - acc: 0.8056 - val_loss: 0.5365 - val_acc: 0.7396\n",
      "Epoch 979/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4275 - acc: 0.8021 - val_loss: 0.5366 - val_acc: 0.7396\n",
      "Epoch 980/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4274 - acc: 0.8038 - val_loss: 0.5366 - val_acc: 0.7396\n",
      "Epoch 981/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4275 - acc: 0.8038 - val_loss: 0.5366 - val_acc: 0.7396\n",
      "Epoch 982/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4274 - acc: 0.8038 - val_loss: 0.5366 - val_acc: 0.7396\n",
      "Epoch 983/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4274 - acc: 0.8021 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 984/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4274 - acc: 0.8038 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 985/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4274 - acc: 0.8038 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 986/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4274 - acc: 0.8038 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 987/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 988/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 989/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5367 - val_acc: 0.7396\n",
      "Epoch 990/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4273 - acc: 0.8038 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 991/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4273 - acc: 0.8038 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 992/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4273 - acc: 0.8038 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 993/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 994/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4272 - acc: 0.8038 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 995/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 996/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4272 - acc: 0.8038 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 997/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 998/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 999/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4272 - acc: 0.8021 - val_loss: 0.5369 - val_acc: 0.7396\n",
      "Epoch 1000/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4272 - acc: 0.8021 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 1001/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4271 - acc: 0.8038 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 1002/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 1003/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4271 - acc: 0.8038 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 1004/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5370 - val_acc: 0.7396\n",
      "Epoch 1005/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4271 - acc: 0.8073 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1006/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1007/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4270 - acc: 0.8073 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1008/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1009/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4270 - acc: 0.8073 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1010/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4270 - acc: 0.8038 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1011/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4270 - acc: 0.8073 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1012/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4270 - acc: 0.8073 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1013/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4269 - acc: 0.8090 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 1014/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4269 - acc: 0.8073 - val_loss: 0.5372 - val_acc: 0.7396\n",
      "Epoch 1015/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4269 - acc: 0.8073 - val_loss: 0.5372 - val_acc: 0.7396\n",
      "Epoch 1016/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4269 - acc: 0.8073 - val_loss: 0.5372 - val_acc: 0.7396\n",
      "Epoch 1017/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4269 - acc: 0.8090 - val_loss: 0.5372 - val_acc: 0.7396\n",
      "Epoch 1018/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4268 - acc: 0.8090 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1019/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4268 - acc: 0.8090 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1020/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4268 - acc: 0.8090 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1021/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4268 - acc: 0.8090 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1022/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4268 - acc: 0.8090 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1023/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4267 - acc: 0.8073 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1024/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4268 - acc: 0.8073 - val_loss: 0.5373 - val_acc: 0.7396\n",
      "Epoch 1025/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4267 - acc: 0.8073 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1026/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4267 - acc: 0.8073 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1027/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4267 - acc: 0.8073 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1028/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4267 - acc: 0.8090 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1029/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4267 - acc: 0.8090 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1030/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4266 - acc: 0.8073 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 1031/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4267 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1032/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1033/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1034/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1035/1500\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1036/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5375 - val_acc: 0.7396\n",
      "Epoch 1037/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7396\n",
      "Epoch 1038/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4265 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7396\n",
      "Epoch 1039/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7396\n",
      "Epoch 1040/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7396\n",
      "Epoch 1041/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7396\n",
      "Epoch 1042/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4265 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1043/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1044/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4265 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1045/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1046/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1047/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 1048/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5378 - val_acc: 0.7396\n",
      "Epoch 1049/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5378 - val_acc: 0.7396\n",
      "Epoch 1050/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.5378 - val_acc: 0.7396\n",
      "Epoch 1051/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4263 - acc: 0.8090 - val_loss: 0.5378 - val_acc: 0.7396\n",
      "Epoch 1052/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4263 - acc: 0.8090 - val_loss: 0.5378 - val_acc: 0.7396\n",
      "Epoch 1053/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4263 - acc: 0.8090 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 1054/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 1055/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 1056/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 1057/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5380 - val_acc: 0.7396\n",
      "Epoch 1058/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5380 - val_acc: 0.7396\n",
      "Epoch 1059/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5380 - val_acc: 0.7396\n",
      "Epoch 1060/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4263 - acc: 0.8090 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 1061/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 1062/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4262 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7396\n",
      "Epoch 1063/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7396\n",
      "Epoch 1064/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7396\n",
      "Epoch 1065/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7396\n",
      "Epoch 1066/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7396\n",
      "Epoch 1067/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.5383 - val_acc: 0.7396\n",
      "Epoch 1068/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4261 - acc: 0.8073 - val_loss: 0.5383 - val_acc: 0.7396\n",
      "Epoch 1069/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5383 - val_acc: 0.7396\n",
      "Epoch 1070/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5384 - val_acc: 0.7396\n",
      "Epoch 1071/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5384 - val_acc: 0.7396\n",
      "Epoch 1072/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5384 - val_acc: 0.7396\n",
      "Epoch 1073/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5384 - val_acc: 0.7396\n",
      "Epoch 1074/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5385 - val_acc: 0.7396\n",
      "Epoch 1075/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4260 - acc: 0.8090 - val_loss: 0.5385 - val_acc: 0.7396\n",
      "Epoch 1076/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5385 - val_acc: 0.7396\n",
      "Epoch 1077/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5386 - val_acc: 0.7396\n",
      "Epoch 1078/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5386 - val_acc: 0.7396\n",
      "Epoch 1079/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4258 - acc: 0.8090 - val_loss: 0.5386 - val_acc: 0.7396\n",
      "Epoch 1080/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5386 - val_acc: 0.7396\n",
      "Epoch 1081/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8090 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 1082/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 1083/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5388 - val_acc: 0.7396\n",
      "Epoch 1084/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5388 - val_acc: 0.7396\n",
      "Epoch 1085/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4258 - acc: 0.8090 - val_loss: 0.5388 - val_acc: 0.7396\n",
      "Epoch 1086/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5388 - val_acc: 0.7396\n",
      "Epoch 1087/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5389 - val_acc: 0.7396\n",
      "Epoch 1088/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5389 - val_acc: 0.7396\n",
      "Epoch 1089/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5389 - val_acc: 0.7396\n",
      "Epoch 1090/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4258 - acc: 0.8073 - val_loss: 0.5390 - val_acc: 0.7396\n",
      "Epoch 1091/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5390 - val_acc: 0.7396\n",
      "Epoch 1092/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5391 - val_acc: 0.7396\n",
      "Epoch 1093/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5391 - val_acc: 0.7396\n",
      "Epoch 1094/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4257 - acc: 0.8056 - val_loss: 0.5392 - val_acc: 0.7396\n",
      "Epoch 1095/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5392 - val_acc: 0.7396\n",
      "Epoch 1096/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5392 - val_acc: 0.7396\n",
      "Epoch 1097/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4257 - acc: 0.8073 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 1098/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4256 - acc: 0.8073 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 1099/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4256 - acc: 0.8073 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 1100/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5393 - val_acc: 0.7396\n",
      "Epoch 1101/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4255 - acc: 0.8073 - val_loss: 0.5394 - val_acc: 0.7396\n",
      "Epoch 1102/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5394 - val_acc: 0.7396\n",
      "Epoch 1103/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4256 - acc: 0.8073 - val_loss: 0.5395 - val_acc: 0.7396\n",
      "Epoch 1104/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4255 - acc: 0.8073 - val_loss: 0.5395 - val_acc: 0.7396\n",
      "Epoch 1105/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5395 - val_acc: 0.7396\n",
      "Epoch 1106/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5396 - val_acc: 0.7396\n",
      "Epoch 1107/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5396 - val_acc: 0.7396\n",
      "Epoch 1108/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4255 - acc: 0.8073 - val_loss: 0.5397 - val_acc: 0.7396\n",
      "Epoch 1109/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5397 - val_acc: 0.7396\n",
      "Epoch 1110/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5397 - val_acc: 0.7396\n",
      "Epoch 1111/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8073 - val_loss: 0.5397 - val_acc: 0.7396\n",
      "Epoch 1112/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8038 - val_loss: 0.5398 - val_acc: 0.7396\n",
      "Epoch 1113/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8073 - val_loss: 0.5398 - val_acc: 0.7396\n",
      "Epoch 1114/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5398 - val_acc: 0.7396\n",
      "Epoch 1115/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5399 - val_acc: 0.7396\n",
      "Epoch 1116/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5399 - val_acc: 0.7396\n",
      "Epoch 1117/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5399 - val_acc: 0.7396\n",
      "Epoch 1118/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5400 - val_acc: 0.7396\n",
      "Epoch 1119/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5400 - val_acc: 0.7396\n",
      "Epoch 1120/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5400 - val_acc: 0.7396\n",
      "Epoch 1121/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 1122/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 1123/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 1124/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5402 - val_acc: 0.7396\n",
      "Epoch 1125/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5402 - val_acc: 0.7396\n",
      "Epoch 1126/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5402 - val_acc: 0.7396\n",
      "Epoch 1127/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5403 - val_acc: 0.7396\n",
      "Epoch 1128/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5403 - val_acc: 0.7396\n",
      "Epoch 1129/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5403 - val_acc: 0.7396\n",
      "Epoch 1130/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5403 - val_acc: 0.7396\n",
      "Epoch 1131/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5404 - val_acc: 0.7396\n",
      "Epoch 1132/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5404 - val_acc: 0.7396\n",
      "Epoch 1133/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5405 - val_acc: 0.7396\n",
      "Epoch 1134/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5405 - val_acc: 0.7396\n",
      "Epoch 1135/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5405 - val_acc: 0.7396\n",
      "Epoch 1136/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 1137/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 1138/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 1139/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 1140/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 1141/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5407 - val_acc: 0.7396\n",
      "Epoch 1142/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5407 - val_acc: 0.7396\n",
      "Epoch 1143/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5407 - val_acc: 0.7396\n",
      "Epoch 1144/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 1145/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 1146/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 1147/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4251 - acc: 0.8056 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 1148/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 1149/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 1150/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 1151/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 1152/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8038 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 1153/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5410 - val_acc: 0.7396\n",
      "Epoch 1154/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5410 - val_acc: 0.7396\n",
      "Epoch 1155/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5410 - val_acc: 0.7396\n",
      "Epoch 1156/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5411 - val_acc: 0.7396\n",
      "Epoch 1157/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4250 - acc: 0.8056 - val_loss: 0.5411 - val_acc: 0.7396\n",
      "Epoch 1158/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5411 - val_acc: 0.7396\n",
      "Epoch 1159/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5412 - val_acc: 0.7396\n",
      "Epoch 1160/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5412 - val_acc: 0.7396\n",
      "Epoch 1161/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5412 - val_acc: 0.7396\n",
      "Epoch 1162/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5412 - val_acc: 0.7396\n",
      "Epoch 1163/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1164/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1165/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1166/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4249 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1167/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1168/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5413 - val_acc: 0.7396\n",
      "Epoch 1169/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 1170/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 1171/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 1172/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 1173/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 1174/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 1175/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 1176/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 1177/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5416 - val_acc: 0.7396\n",
      "Epoch 1178/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5416 - val_acc: 0.7396\n",
      "Epoch 1179/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5416 - val_acc: 0.7396\n",
      "Epoch 1180/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1181/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1182/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1183/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1184/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4247 - acc: 0.8056 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1185/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5417 - val_acc: 0.7396\n",
      "Epoch 1186/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 1187/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 1188/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 1189/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 1190/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 1191/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4245 - acc: 0.8056 - val_loss: 0.5419 - val_acc: 0.7396\n",
      "Epoch 1192/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8056 - val_loss: 0.5419 - val_acc: 0.7396\n",
      "Epoch 1193/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4245 - acc: 0.8056 - val_loss: 0.5419 - val_acc: 0.7396\n",
      "Epoch 1194/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5419 - val_acc: 0.7396\n",
      "Epoch 1195/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5420 - val_acc: 0.7396\n",
      "Epoch 1196/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5420 - val_acc: 0.7396\n",
      "Epoch 1197/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5420 - val_acc: 0.7396\n",
      "Epoch 1198/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5420 - val_acc: 0.7396\n",
      "Epoch 1199/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 1200/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 1201/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8056 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 1202/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4244 - acc: 0.8056 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 1203/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 1204/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8056 - val_loss: 0.5422 - val_acc: 0.7396\n",
      "Epoch 1205/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5422 - val_acc: 0.7396\n",
      "Epoch 1206/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5422 - val_acc: 0.7396\n",
      "Epoch 1207/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5422 - val_acc: 0.7396\n",
      "Epoch 1208/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1209/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1210/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1211/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1212/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1213/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8056 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 1214/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 1215/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 1216/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 1217/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 1218/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 1219/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 1220/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 1221/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4242 - acc: 0.8056 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 1222/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 1223/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 1224/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 1225/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4242 - acc: 0.8056 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 1226/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4242 - acc: 0.8056 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 1227/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8056 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1228/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1229/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1230/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4241 - acc: 0.8056 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1231/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4241 - acc: 0.8056 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1232/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4241 - acc: 0.8056 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 1233/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8056 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1234/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1235/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1236/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1237/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1238/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1239/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 1240/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1241/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1242/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1243/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1244/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1245/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1246/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 1247/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1248/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1249/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4240 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1250/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1251/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1252/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1253/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 1254/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1255/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 39us/step - loss: 0.4239 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1256/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1257/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1258/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1259/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1260/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5431 - val_acc: 0.7396\n",
      "Epoch 1261/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5432 - val_acc: 0.7396\n",
      "Epoch 1262/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5432 - val_acc: 0.7396\n",
      "Epoch 1263/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5432 - val_acc: 0.7396\n",
      "Epoch 1264/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5432 - val_acc: 0.7396\n",
      "Epoch 1265/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5433 - val_acc: 0.7396\n",
      "Epoch 1266/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5433 - val_acc: 0.7448\n",
      "Epoch 1267/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4238 - acc: 0.8056 - val_loss: 0.5433 - val_acc: 0.7448\n",
      "Epoch 1268/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5433 - val_acc: 0.7448\n",
      "Epoch 1269/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5433 - val_acc: 0.7448\n",
      "Epoch 1270/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5434 - val_acc: 0.7448\n",
      "Epoch 1271/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5434 - val_acc: 0.7448\n",
      "Epoch 1272/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5434 - val_acc: 0.7448\n",
      "Epoch 1273/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5434 - val_acc: 0.7448\n",
      "Epoch 1274/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5435 - val_acc: 0.7448\n",
      "Epoch 1275/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4237 - acc: 0.8056 - val_loss: 0.5435 - val_acc: 0.7448\n",
      "Epoch 1276/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5435 - val_acc: 0.7448\n",
      "Epoch 1277/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5435 - val_acc: 0.7448\n",
      "Epoch 1278/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5435 - val_acc: 0.7448\n",
      "Epoch 1279/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1280/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1281/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1282/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1283/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1284/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1285/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1286/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4236 - acc: 0.8056 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 1287/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5437 - val_acc: 0.7448\n",
      "Epoch 1288/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5437 - val_acc: 0.7448\n",
      "Epoch 1289/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5437 - val_acc: 0.7448\n",
      "Epoch 1290/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5437 - val_acc: 0.7448\n",
      "Epoch 1291/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5437 - val_acc: 0.7448\n",
      "Epoch 1292/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1293/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1294/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1295/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1296/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1297/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5438 - val_acc: 0.7396\n",
      "Epoch 1298/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 1299/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 1300/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 1301/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4234 - acc: 0.8056 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 1302/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5439 - val_acc: 0.7396\n",
      "Epoch 1303/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 1304/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 1305/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 1306/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 1307/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 1308/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4233 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1309/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1310/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1311/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1312/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1313/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4232 - acc: 0.8073 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1314/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 39us/step - loss: 0.4232 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1315/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4231 - acc: 0.8056 - val_loss: 0.5441 - val_acc: 0.7396\n",
      "Epoch 1316/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4232 - acc: 0.8073 - val_loss: 0.5442 - val_acc: 0.7396\n",
      "Epoch 1317/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4231 - acc: 0.8073 - val_loss: 0.5442 - val_acc: 0.7396\n",
      "Epoch 1318/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4231 - acc: 0.8073 - val_loss: 0.5442 - val_acc: 0.7396\n",
      "Epoch 1319/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4231 - acc: 0.8073 - val_loss: 0.5442 - val_acc: 0.7396\n",
      "Epoch 1320/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4231 - acc: 0.8073 - val_loss: 0.5443 - val_acc: 0.7396\n",
      "Epoch 1321/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4231 - acc: 0.8056 - val_loss: 0.5443 - val_acc: 0.7396\n",
      "Epoch 1322/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4231 - acc: 0.8073 - val_loss: 0.5443 - val_acc: 0.7396\n",
      "Epoch 1323/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4230 - acc: 0.8073 - val_loss: 0.5443 - val_acc: 0.7396\n",
      "Epoch 1324/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8073 - val_loss: 0.5444 - val_acc: 0.7396\n",
      "Epoch 1325/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4230 - acc: 0.8073 - val_loss: 0.5444 - val_acc: 0.7396\n",
      "Epoch 1326/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8073 - val_loss: 0.5444 - val_acc: 0.7396\n",
      "Epoch 1327/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5444 - val_acc: 0.7396\n",
      "Epoch 1328/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8056 - val_loss: 0.5444 - val_acc: 0.7396\n",
      "Epoch 1329/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8056 - val_loss: 0.5445 - val_acc: 0.7396\n",
      "Epoch 1330/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4229 - acc: 0.8056 - val_loss: 0.5445 - val_acc: 0.7396\n",
      "Epoch 1331/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4229 - acc: 0.8073 - val_loss: 0.5445 - val_acc: 0.7396\n",
      "Epoch 1332/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4229 - acc: 0.8056 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1333/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4229 - acc: 0.8056 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1334/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4229 - acc: 0.8056 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1335/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4229 - acc: 0.8073 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1336/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1337/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5446 - val_acc: 0.7396\n",
      "Epoch 1338/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5447 - val_acc: 0.7396\n",
      "Epoch 1339/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5447 - val_acc: 0.7396\n",
      "Epoch 1340/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5447 - val_acc: 0.7396\n",
      "Epoch 1341/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5447 - val_acc: 0.7396\n",
      "Epoch 1342/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5447 - val_acc: 0.7396\n",
      "Epoch 1343/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1344/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1345/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1346/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1347/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1348/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1349/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1350/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1351/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1352/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1353/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1354/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 1355/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1356/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4225 - acc: 0.8056 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1357/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4225 - acc: 0.8056 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1358/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1359/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1360/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4224 - acc: 0.8056 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1361/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1362/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1363/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5449 - val_acc: 0.7396\n",
      "Epoch 1364/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1365/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1366/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1367/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1368/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1369/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1370/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1371/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1372/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1373/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 39us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1374/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1375/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1376/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1377/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4222 - acc: 0.8021 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1378/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1379/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1380/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 1381/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1382/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1383/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1384/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1385/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1386/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1387/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1388/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1389/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1390/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7396\n",
      "Epoch 1391/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1392/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1393/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1394/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1395/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1396/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1397/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1398/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1399/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1400/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1401/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1402/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1403/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1404/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5451 - val_acc: 0.7448\n",
      "Epoch 1405/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1406/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1407/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4218 - acc: 0.8021 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1408/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1409/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1410/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1411/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1412/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1413/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1414/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1415/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1416/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1417/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1418/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1419/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1420/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1421/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1422/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1423/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1424/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5452 - val_acc: 0.7448\n",
      "Epoch 1425/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1426/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1427/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1428/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1429/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1430/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1431/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1432/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1433/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1434/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1435/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1436/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5453 - val_acc: 0.7448\n",
      "Epoch 1437/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1438/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1439/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1440/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1441/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1442/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1443/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1444/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1445/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1446/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1447/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1448/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1449/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5454 - val_acc: 0.7448\n",
      "Epoch 1450/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1451/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1452/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1453/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1454/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1455/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1456/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1457/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1458/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1459/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1460/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1461/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1462/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5455 - val_acc: 0.7448\n",
      "Epoch 1463/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1464/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1465/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1466/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1467/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1468/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1469/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1470/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1471/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1472/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1473/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1474/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1475/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1476/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5456 - val_acc: 0.7448\n",
      "Epoch 1477/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1478/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1479/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1480/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1481/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1482/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1483/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1484/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1485/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4210 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1486/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1487/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1488/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1489/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7448\n",
      "Epoch 1490/1500\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1491/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 38us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1492/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5458 - val_acc: 0.7396\n",
      "Epoch 1493/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1494/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1495/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1496/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1497/1500\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1498/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1499/1500\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n",
      "Epoch 1500/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5457 - val_acc: 0.7396\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])   # we could also use different optimizer like adap instead of SGD\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve\n",
    "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.740\n",
      "roc-auc is 0.796\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VOX5//HPza4IYRVkEdRAEdEGC8X6RU3dLVZrrf4AF2y1dtGqoKwCgiioqKittsa1aKO4FxUVt4iiKIhRdmUTwiZb2CHb8/tjBjrELJNkZp5Z3q/rymVO5mTmk4fj3HOf85xzzDknAAAQP2r5DgAAAA5GcQYAIM5QnAEAiDMUZwAA4gzFGQCAOENxBgAgzlCckXLM7BAze93MtpnZi77zpCoze9rM7gh+f4qZLQnz964ys0+im84vM+toZs7M6pTz+BgzezbWuRA7FOckZ2YrzWyPme00s/XBN8TDSq1zspl9YGY7ggXrdTPrWmqdxmb2gJmtCj7X0uByi3Je18zsBjObb2a7zCzPzF40s+Oj+feG6XeSWklq7py7pKZPZmaZwTfSh0v9/BMzuyr4/VXBdQaXWifPzDJrmiGMjKHbwQYze2r/dmBmOWZ2Tam/5ZVSv//T4M9zSv3czGy5mS2sST7n3MfOuZ/U5DnCkQqFHcmB4pwafu2cO0xShqTukobvf8DMfiFpuqT/Smoj6ShJX0uaaWZHB9epJ+l9ScdJOldSY0knS9os6eflvOaDkm6UdIOkZpI6S3pNUp+qhi+ve6iBDpK+dc4VRTDLLklXmlnHCn59i6ShZta4qq8bIfu3gxMl9ZQ0spz1Nko62cyah/xsgKRvy1j3VEmHSzrazHpGMmwyi8I2jSRDcU4hzrn1kt5RoEjvd4+kyc65B51zO5xzW5xzIyXNkjQmuM6Vko6UdJFzbqFzrsQ594Nzbpxzblrp1zGzTpKuk9TPOfeBc26fc263c+4/zrm7gusc6NaCywd1NMEu7Toz+07Sd2b2LzO7t9Tr/NfMBgW/b2NmL5vZRjNbYWY3lDUGZjZW0mhJ/y/YRV5tZrXMbKSZfW9mP5jZZDNLC66/f/fi1Wa2StIH5QxvvqSnJd1WzuOStEjSZ5IGVrBOaNa0YJaNwWwjzaxW8LGrgp35vWa2Nfg3nxfO8zrn1kh6S1K3clYpUOCDVN/ga9WWdKmk/5Sx7gAFPthNC35f0d/T3czmBvfQTJHUIOSxTDPLC1keZmbLgusuNLOLfvx09vfgnp7FZnZGyANpZvaEma0zszVmdoeZ1TazYyX9S9Ivgv/2+cH16wfHcVVwr8K/zOyQ4GMtzOwNM8s3sy1m9vH+f4My/j5ngb1Fy81sk5lNLPXvNdPMJpnZFkljKtruQvzBzNYG/5abKxjbk8zs02DOry1kb0zw/7U7go/vtMCeseZm9h8z225msyv5UAkPKM4pxMzaSTpP0tLg8qEKdMBlHXd9QdJZwe/PlPS2c25nmC91hqQ859wXNUus30jqJamrpGwFCqpJkpk1lXS2pOeDb4CvK9Dxtw2+/k1mdk7pJ3TO3SZpvKQpzrnDnHNPSLoq+PVLSUdLOkzSP0r96mmSjpX0o+cMcaeki82sot2zoyQNNLNmFayz398lpQUznabAh6TfhzzeS9ISSS0U+JD1xP7xqYiZtZf0K0lfVbDa5ODrSYG/eYGktaWe51AFDhH8J/jV1wJ7Wcp6zXoKFPxnFNiT8qKkiyt4/WWSTlHg7x8r6VkzOyLk8V6Slivwt98m6ZWQMf23pCJJ6QrsKTpb0jXOuUWS/izps+C/fZPg+ncrsGcnI/g7bRX4ACdJN0vKk9RSgUMhIyRVdM3jiyT1UGDvxIWS/lBG5sMV2FauUuXb3S8ldQr+DcPM7MzSL2hmbSW9KekOBcb2Fkkvm1nLkNX6Sroi+Lcdo8CHxKeC6y9SxR8q4QHFOTW8ZmY7JK2W9IP+9z9iMwW2gXVl/M46Bd74JKl5OeuUp6rrl2dCsJPfI+ljBd4UTwk+9jsF3mTXKrCLtqVz7nbnXIFzbrmkxxTs/MJwmaT7nXPLgx9AhitQaEJ3PY5xzu0KZilTcM/EvyTdXsE6uQocRhhaUaBgt/r/JA0P7tFYKek+Bd5g9/veOfeYc65YgYJ0hAIFpDyvBbvFTyR9pMCHlPJyfiqpWfCDxpUKFOvSfitpX/DveUNSHZV/2OIkSXUlPeCcK3TOvSRpdgWv/6Jzbm1wL80USd/p4EMoP4Q81xQFPqT0MbNWCnwAvSn47/WDpEkqZ1sIfpj5o6SBwW1thwLjsn/9QgXGtUPwtT52Fd+Q4O7g86yS9ICkfiGPrXXO/d05VxTcjsLZ7sYG/455ChTT0Ofb73JJ05xz04Lj9a6kOQp8ANvvKefcMufcNgX2mixzzr0XPLTzogIfYhBHKM6p4TfOuUaSMiV10f+K7lZJJQq8+ZR2hKRNwe83l7NOeaq6fnlW7/8m+Ib4vP735tRf/9vN2kFSm+AuvfxgARqhigtVqDaSvg9Z/l6BQhP6+6sVnrslnWNmP61gndGS/mJmrStYp4WkemXkahuyvH7/N8653cFvD5rsV8pvnHNNnHMdnHN/reiDRtAzkq5XoHt7tYzHB0h6IVhs9kl6ReXv2m4jaU2pwvZ9OevKzK40s9yQf89u+t92q3Keq40C20JdSetCfvdRBbrVsrSUdKikL0PWfzv4c0maqMCepunB3dXDysscFLqd7M9U1mNS1be70s+3XwdJl5Ta/nvr4P8HN4R8v6eM5Yq2G3hAcU4hzrmPFDguem9weZcCu7fKmrF8qQKTwCTpPQUKTsMwX+p9Se3MrEcF6+xS4E1xv7IKVekO5TlJvzOzDgrsInw5+PPVklYEC8/+r0bOuV8pPGsVeIPb70gFdouGvoGFdfs259xmBTqmcRWss1iBQjaigqfapEDXVjrXmnByRMgzkv6qQFe2O/SB4CGS0yVdboGzANYrsDfjV1b2DP51ktqW2u1+ZFkvGvz3fUyBDwbNg7uf50sK/d2ynmutAtvCPkktQraFxs6544Lrlf533KRAcTouZP204MQ5Bfda3OycO1rSryUNCj2+XYb2ZWTar/Rrh7PdVfR8+62W9Eyp7b/h/vkdSEwU59TzgKSzzGz/pLBhkgYEJ7I0MrOmFjj39BcKHOuTAm/SqxU4jtUlOJGluZmNMLMfFUDn3HeSHpH0nAUm+tQzswZm1jek88iV9FszO9TM0iVdXVlw59xXCswkflzSO865/OBDX0jabmZDLXAOc20z62bhzx5+ToHjwEdZ4PSi/cekqzybO+h+BY7lH1vBOmMVOH7cpKwHg7uqX5B0Z/DfpYOkQZJidm6rc26FAse6by3j4SsUmL39EwWO1WYocNw2T2Xvev1MgcJzg5nVMbPfqvyZ/g0VKGQbJcnMfq8fT147PPhcdc3sEgXGeppzbp0Cu9nvs8Dpf7XM7BgzOy34exsU+OBYL/g3lijwQWCSmR0efL22++crmNn5ZpYe/CCwXVJx8Ks8g4P/D7VX4GyFKRWsG852Nyr4/8hxCmwvZT3fs5J+bWbnBLf9BsH/79pV8NqIcxTnFOOc26jA8cNRweVPFJjw81sFupvvFTj+1DtYZBXcZXmmpMWS3lXgTeoLBXYzfl7OS92gwOSWhxWYybxMgckyrwcfn6TArOANChwvLWsmcFmeC2bJDvmbihXoajIkrVCgG3pcgclE4XhSgQ8gM4K/v1fS38L83R9xzm1XYIJWuZO+goXvGQUKUXn+psAehuUKHCfODmaNGefcJ8Hj+qUNkPSIc2596JcCx9x/tGvbOVegwDZ2lQKHU/6fAnsPynrNhQocX/9Mge3jeEkzS632uQITpTYpMLnqd8G9FlLgGHk9SQuDr/WS/reL9wMFJretN7P9h22GKrDrepaZbVdgT9H+SX2dgss7g3kecc7llJU76L+SvlTgw+ebkp6oYN1wtruPgtnel3Svc2566Sdxzq1WYPLZCAU+0KyWNFi8vyc0q3huAwAgHGbmJHVyzi31nQWJj09WAADEGYozAABxht3aAADEGTpnAADiDMUZAIA4U+mdUczsSUnnS/rBOfejC+UHz/97UIFLxe2WdJVzbm5lz9uiRQvXsWPHA8u7du1Sw4bhXuMCVcX4RhfjGz2MbXQxvtFTemy//PLLTc65lhX8ygHh3LbsaQXOVy3r2rpS4Dq2nYJfvST9M/jfCnXs2FFz5sw5sJyTk6PMzMww4qA6GN/oYnyjh7GNLsY3ekqPrZmVe8na0irdre2cm6HAfWjLc6ECtxx0zrlZkpqUunsMAACogkjc8LutDr44e17wZ5G4KxEAAHEpKytL2dnZ5T7eokWLau+ViERxLuv+sWWen2Vm10q6VpJatWqlnJycA4/t3LnzoGVEFuMbXYxv9DC20cX4Vt8jjzyipUuXKj09/aCfO+e0YcMGZWRkVHtsI1Gc83TwnVPaqew7p8g5lyUpS5J69OjhQj9RcNwjuhjf6GJ8o4exjS7Gt/qaNGmiHj16HFSAS0pKtGjRItWrV09r1qyp9thG4lSqqZKutICTJG0L3hkGAICU4ZzT8OHD5ZxTp06davRc4ZxK9ZykTEktzCxP0m0K3Mxczrl/SZqmwGlUSxU4ler3NUoEAECCKSws1MyZMzVs2DA1bdq0xs9XaXF2zpV1b9bQx52k62qcBACABDVu3DhdeeWVESnMUmSOOQMAEkjoLOP8/Hw1adLEc6LElJubqxNOOEHZ2dm67bbbVLt27Yg9N5fvBIAUk52drdzcXN8xEl5GRoZat26t3r17R7QwS3TOAJCS9p/mw2zt6tm1a5ceffRRDRo0KCrPT+cMAEAVvfbaa+rfv3/Unp/iDABAmLZt26ahQ4eqf//+at26ddReh+IMAEAYCgoK9MUXX2jo0KEK3JAxeijOAABUYtOmTRo4cKBOO+00NWvWLOqvx4QwAEgCld2EIVRubq4yMjKinCh5bN68Wd9//70mTJigevXqxeQ16ZwBIAlU5fSojIyMqE5mSibr1q3T6NGj1aVLFzVu3Dhmr0vnDABJoiZ3QcKP5eXlaevWrZo4caIOPfTQmL42nTMAAKWsW7dO99xzjzp16hTzwizROQMAcJBly5Zpx44dmjhxourXr+8lA50zAABB27dv1z//+U8dd9xx3gqzROcMAAepyqzneMIM7JpbuHChNmzYoIkTJ0b9PObK0DkDQIhEvSkEM7BrpqioSC+//LJOPfVU74VZonMGgB9h1nNqmTt3rpYvX65Ro0b5jnIAnTMAIGU55zR79mxdfPHFvqMchM4ZAJCSZs6cqfnz5+tPf/qT7yg/QucMAEg5u3bt0tatW3Xttdf6jlImOmcASam6s66Z9Zz83nvvPS1YsEA33nij7yjlonMGkJSqO+uaWc/JbcWKFWrevHlcF2aJzhlAEmPWNUK98cYbWrVqlf7617/6jlIpijMAIOl98skn6tmzp84//3zfUcLCbm0AQFKbNm2ali5dqlatWvmOEjY6ZwBA0nrllVd09tln67DDDvMdpUronAEASWnGjBkqKChIuMIsUZwBAEnoiSeeULdu3dS3b1/fUaqF4gwASCrz589XixYt1KxZM99Rqo3iDABIGg8++KAOPfRQXXjhhb6j1AjFGQCQFFavXq2uXbvq6KOP9h2lxijOAICE5pzTXXfdpU2bNumss87yHSciOJUKgHcVXQc7Pz9fTZo0qfJzco3s1OCcU15enn75y1+qe/fuvuNEDJ0zAO+qex3sinCN7OTnnNPYsWO1fv169erVy3eciKJzBhAXyrsOdk5OjjIzM2OeB/GtpKRECxYs0OWXX6709HTfcSKOzhkAkFCccxo5cqRKSkqSsjBLdM4AgARSVFSknJwcDR06VGlpab7jRA2dMwAgYYwfP17t27dP6sIs0TkDCa2iWc6JhJnVqExBQYGmTJmikSNHqlat5O8rk/8vBJJYNGY5+8DMalTmscce0ymnnJIShVmicwYSXnmznIFksGfPHv3jH//Q4MGDfUeJqdT4CAIASDjOOb3++uu67LLLfEeJOYozACDu7NixQ4MHD9bvfvc7tWnTxnecmKM4AwDiyt69e/Xll19q2LBhKXOMubTU/KsBAHFpy5YtGjRokE466SS1aNHCdxxvmBAGAIgLmzdv1qpVqzRhwgQ1aNDAdxyv6JwBAN5t2LBBo0ePVnp6etJfYCQcdM4AAK/Wrl2rTZs26Z577lHDhg19x4kLdM4AAG82btyou+66S506daIwh6BzBgB4sXLlSm3evFkTJ05U/fr1fceJK3TOAICY2717t/7+97/r+OOPpzCXgc4ZiHMV3dyCG0YgES1ZskQrV67UvffeKzPzHScu0TkDca6im1twwwgkmuLiYr300ks644wzKMwVoHMGEgA3t0Ay+PrrrzV//nzdeuutvqPEPTpnAEDUlZSUaPbs2erXr5/vKAmBzhkAEFWzZs3S7Nmz9be//c13lIRB5wwAiJodO3Zo69atuv76631HSSh0zkAcYEY2klFOTo7mzJmjW265xXeUhEPnDMQBZmQj2SxdulTNmjWjMFcTnTMQJ5iRjWTx9ttv69tvv9UNN9zgO0rCojgDACJmxowZOvHEE3Xuuef6jpLQ2K0NAIiI6dOna8mSJTr88MN9R0l4dM4AgBp75ZVXdOaZZ+rss8/2HSUp0DkDAGrk888/1549e9S4cWPfUZIGxRkAUG1PPfWUOnbsqMsuu8x3lKRCcQYAVMt3332nxo0bq1WrVr6jJB2KMwCgyh5++GEVFxfr4osv9h0lKVGcAQBVsn79eqWnp6tLly6+oyQtijMAICzOOd17771atWqVzjnnHN9xkhqnUgExUNG1syWun43455zTmjVr1Lt3b/385z/3HSfp0TkDMVDRtbMlrp+N+Oac0x133KHVq1frpJNO8h0nJdA5AzHCtbORiJxzmjdvnvr3769jjjnGd5yUQecMACjXmDFjVFRURGGOMTpnAMCPFBcX67333tMtt9yiRo0a+Y6TcuicAQA/cs8996h9+/YUZk/onAEABxQWFurZZ5/V0KFDVasW/ZsvjDwQJVlZWcrMzFRmZmaFM7WBePL000/r1FNPpTB7xugDURJ6+hSnSiHe7d27V3feeaeuueYaJn/FgbB2a5vZuZIelFRb0uPOubtKPX6kpH9LahJcZ5hzblqEswIJh9OnkAicc3rrrbc0YMAAmZnvOFAYnbOZ1Zb0sKTzJHWV1M/MupZabaSkF5xz3SX1lfRIpIMCACJvz549GjRokH7961+rXbt2vuMgKJzd2j+XtNQ5t9w5VyDpeUkXllrHSdp/l+00SWsjFxEAEA179uzR0qVLNXz4cNWpw/zgeBLOv0ZbSatDlvMk9Sq1zhhJ083sb5IaSjqzrCcys2slXStJrVq1Omh3386dO9n9F0WMb3SVNb75+fmSxLjXENtudOzcuVOPPfaYLr/8ci1cuFALFy70HSnp1GTbDac4l3UAwpVa7ifpaefcfWb2C0nPmFk351zJQb/kXJakLEnq0aOHy8zMPPBYTk6OQpcRWYxvdOXk5Ojbb7896OYWK1euVEZGBuNeQ2y7kbdlyxatXr1aTz/9tL7++mvGN0pqsu2Gs1s7T1L7kOV2+vFu66slvSBJzrnPJDWQ1KJaiYAEVfrmFszQRjzatGmTRo0apY4dO6pp06a+46Ac4XTOsyV1MrOjJK1RYMJX6XecVZLOkPS0mR2rQHHeGMmgQCJgdjbi2fr167VhwwbdddddXPkrzlXaOTvniiRdL+kdSYsUmJW9wMxuN7MLgqvdLOmPZva1pOckXeWcK73rGwDgydatWzVu3Dilp6dTmBNAWNPzgucsTyv1s9Eh3y+U9H+RjQYAiIRVq1Zp7dq1uv/++1W/fn3fcRAGrhAGAEls3759evDBB9W9e3cKcwLhxDagBrKyspSdna38/PwDs7OBePHdd99pyZIluvfee7nyV4KhcwZqgOtnI1455/TSSy/p3HPPpTAnIDpnoIYyMjI0ZswYzhVF3Jg/f77mzJmj4cOH+46CaqJzBoAkUlJSojlz5ujKK6/0HQU1QOcMAElizpw5mjFjhgYNGuQ7CmqIzhkAksC2bdu0ZcsWDRw40HcURACdM1CJ/TOyy5Kbm8sMbXj38ccfa+bMmRo2bJjvKIgQOmegEqWvmR2KGdrwbcmSJWrWrJmGDh3qOwoiiM4ZCENl18zmetrw4b333tM333zDMeYkRHEGgAQ0Y8YMnXDCCTrzzDN9R0EUsFsbABJMTk6OFi5cqMMPP9x3FEQJnTMAJJBXX31VmZmZXPQmyVGckTQqmlVdE8zIRrzIzc3V9u3b1bRpU99REGXs1kbSqGhWdU0wIxvx4JlnnlHz5s01YMAA31EQA3TOSCqVzaoGEtGqVatUv359tW/f3ncUxAidMwDEsUcffVRbt27VpZde6jsKYojiDABxauPGjTryyCP105/+1HcUxBjFGQDi0KRJk7RkyRKdd955vqPAA445A0Accc5pzZo1Ovnkk9WrVy/fceAJnTMAxAnnnCZMmKAVK1ZQmFMcnTMAxAHnnHJzc9WvXz8dddRRvuPAMzpnAIgDd9xxh4qKiijMkETnDABelZSUaNq0aRo0aJAaNmzoOw7iBJ0zAHh0//33q0OHDhRmHITOGQA8KCoq0lNPPaWbb75ZZuY7DuIMxRlxp7o3sOAGFUgkzz77rE477TQKM8rEbm3EnerewIIbVCAR7Nu3T7fffrsGDBigzp07+46DOEXnjLjEDSyQjJxzeu+99zRgwAA6ZlSIzhkAYmD37t0aOHCgzjrrLHXo0MF3HMQ5ijMARNmePXs0b948DRs2TPXq1fMdBwmA4gwAUbR9+3bdcsst6tKli1q3bu07DhIEx5xRI9WdWV0RZl0jWWzdulWrVq3S7bffrrS0NN9xkEDonFEj1Z1ZXRFmXSMZbNmyRSNHjlSHDh3UvHlz33GQYOicUWPMrAYOtnHjRq1Zs0YTJkxQ48aNfcdBAqJzBoAI2rFjh8aOHav09HQKM6qNzhkAImTNmjVasWKF7r//fmZlo0bonAEgAoqKivTggw+qR48eFGbUGJ0zfqQqM7CZWQ1Iy5cv19dff6177rnHdxQkCTpn/EhVZmAzsxqpzjmnl19+Weeff77vKEgidM4oEzOwgcotWrRIH3/8sQYPHuw7CpIMnTMAVENxcbG+/PJLXX311b6jIAnROQNAFX311VeaPn26hg4d6jsKkhSdMwBUwdatW7V161Z2ZSOqKM4AEKZPP/1UDz/8sE4//XTVqsXbJ6KHrQsAwrBo0SI1bdpUt956q+8oSAEUZwCoxEcffaQ33nhDXbp0kZn5joMUwIQwAKjARx99pC5duui0007zHQUphM4ZAMrx6aefat68eWrVqpXvKEgxdM4AUIb//ve/Ovnkk3XyySf7joIUROcMAKUsXLhQmzZtUsuWLX1HQYqiOANAiP/85z+qX78+V/6CVxRnAAhav369atWqpWOOOcZ3FKQ4ijMASHr88ce1evVq9evXz3cUgOIMAFu2bNERRxyhnj17+o4CSGK2NoAU99BDD+n4449Xnz59fEcBDqA4A0hZeXl56tWrl3r16uU7CnAQdmsDSEl33XWXvvvuOwoz4hKdM4CU4pzTl19+qf79++vII4/0HQcoE50zgJRy9913q7CwkMKMuEbnDCAllJSU6PXXX9eNN96oQw45xHccoEJ0zgBSwsMPP6wOHTpQmJEQ6JwBJLXi4mI99thjuv7667kXMxIGxTlFZWVlKTs7u8zHcnNzlZGREeNEQHRMmTJFmZmZFGYkFHZrp6js7Gzl5uaW+VhGRob69+8f40RAZBUUFGjMmDHq27evunTp4jsOUCV0ziksIyNDOTk5vmMAEVdSUqKPPvpIAwYMUK1a9CBIPGy1AJLKnj17NHDgQPXu3VtHHXWU7zhAtdA5A0gau3fv1qJFizRkyBBmZSOh0TkDSAo7duzQ4MGD1bFjR7Vt29Z3HKBG6JyTSEUzsPPz89WkSZMDy8zIRjLZtm2bVq5cqTFjxqh58+a+4wA1RuecRCqagV0aM7KRLPLz8zV8+HC1b99eLVu29B0HiAg65yRT3gzsnJwcZWZmxjwPEE2bNm3SqlWrNGHCBKWlpfmOA0QMnTOAhLRnzx6NGTNGnTp1ojAj6dA5A0g469at06JFizRp0iTVrVvXdxwg4uicASSUkpISPfDAAzrppJMozEhadM4JLnSGNjOwkexWrlypWbNm6e677/YdBYiqsDpnMzvXzJaY2VIzG1bOOpea2UIzW2BmZZ/Pg4gLnaHNDGwku1deeUW//e1vfccAoq7SztnMakt6WNJZkvIkzTazqc65hSHrdJI0XNL/Oee2mtnh0QqMH+Ma2Uh2S5Ys0bvvvqtBgwb5jgLERDid888lLXXOLXfOFUh6XtKFpdb5o6SHnXNbJck590NkYwJIVcXFxZo7d67+/Oc/+44CxEw4xbmtpNUhy3nBn4XqLKmzmc00s1lmdm6kAgJIXd98842ys7PVr18/1anDFBmkjnC29rLuUO7KeJ5OkjIltZP0sZl1c87lH/REZtdKulaSWrVqddCu2J07d7Jrthry8wNDXNnYMb7RxfhG3rZt27RixQpdeOGFjG0Use1GT03GNpzinCepfchyO0lry1hnlnOuUNIKM1uiQLGeHbqScy5LUpYk9ejRw4VesYorWFXP/utlVzZ2jG90Mb6R9cUXX+jDDz/U2LFjGdsoY3yjpyZjG85u7dmSOpnZUWZWT1JfSVNLrfOapF9Kkpm1UGA39/JqJQKQ0hYsWKC0tDSNGTPGdxTAm0qLs3OuSNL1kt6RtEjSC865BWZ2u5ldEFztHUmbzWyhpA8lDXbObY5WaADJaebMmZo6dao6d+4ss7KOqAGpIawZFs65aZKmlfrZ6JDvnaRBwS8AqLIZM2aoc+fOOvnkkynMSHlcvhOAd3PmzNHcuXPVunVrCjMgijMAz15//XW1adNGN910k+8oQNzgxEFPQq+JXRNcTxuJbNmyZVq3bp3atGmPUX0jAAAc5klEQVTjOwoQV+icPQm9JnZNcD1tJKopU6Zo3759uvbaa31HAeIOnbNHXBMbqWrz5s0qKipS165dfUcB4hLFGUBMPf3000pPT9dll13mOwoQt9itDSBmtm3bppYtW6p3796+owBxjc4ZQEw88sgjSk9PV58+fXxHAeIexRlA1K1evVo9e/ZUz549fUcBEgLFOUZKnzrFKVBIFffdd59OOOEEnXXWWb6jAAmDY84xUvrUKU6BQrJzzunzzz9X3759KcxAFdE5xxCnTiGV3H///TrppJPUtm1b31GAhENxBhBRzjm9+uqruu6669SgQQPfcYCExG5tABGVlZWlDh06UJiBGqBzBhARxcXFeuSRR3T99ddzZymghuicAUTEK6+8otNPP53CDEQAxRlAjRQWFmrUqFG66KKLdNxxx/mOAyQFijOAaispKdHMmTM1YMAA1anDUTIgUijOAKpl7969GjhwoH72s58pPT3ddxwgqfBRF0CV7dmzR0uWLNEtt9yiRo0a+Y4DJB06ZwBVsmvXLg0ePFht2rRR+/btfccBkhLFOYqysrKUmZmpzMzMgy7dCSSqHTt2aNmyZRo1apQOP/xw33GApEVxjqLQ62lzLW0kuh07dmjYsGFq06aNWrVq5TsOkNQ45hxlXE8byWDLli1avny5xo8fr7S0NN9xgKRH5wygQgUFBRo9erQ6depEYQZihM4ZQLk2bNig3NxcPfDAA5zHDMQQnTOAMjnn9NBDD6l3794UZiDG+D8ugrKyspSdnX1gOTc3VxkZGR4TAdWzevVq5eTk6M477/QdBUhJdM4RFDo7W2KGNhLXa6+9pksuucR3DCBl0TlHGLOzkciWLVumqVOnauDAgb6jACmNzhmApMDdpebOnavrr7/edxQg5dE5A9CCBQv0wgsvaOzYsb6jABCdM5DyfvjhB+Xn52v06NG+owAIojjXENfPRiL78ssv9dBDD+nkk09W7dq1fccBEERxriGun41ENX/+fDVq1Ejjxo2TmfmOAyAEx5wjgBnaSDRffPGFpk+frltvvZXCDMQhOmcgxXz88cdq164dhRmIYxRnIIV88803+uKLL9SmTRsKMxDHKM5Aipg2bZrS0tJ08803+44CoBIccw5D6Wtmh+L62UgEq1ev1sqVK/WrX/3KdxQAYaBzDkPpa2aHYoY24t1LL72kzZs3669//avvKADCROccJmZkIxFt27ZNe/bsYe8OkGAozkCSeuaZZ9S2bVtdccUVvqMAqCJ2awNJaPv27WrevLlOP/1031EAVAOdM5BkHn30UbVr1059+vTxHQVANVGcgSTy/fffq0ePHvrZz37mOwqAGqA4l6H0qVOcLoVE8OCDD6pz584677zzfEcBUEMU5zLsP3Vqf0HmdCnEM+ecPv30U1166aU64ogjfMcBEAEU53Jw6hQSxUMPPaSMjAwKM5BEKM5AgnLO6cUXX9Sf//xn1a9f33ccABHEqVRAgnrqqafUoUMHCjOQhOicgQRTUlKihx56SDfeeCN3lgKSFJ0zkGDeeOMNnX766RRmIIlRnIEEUVRUpFGjRumcc87RCSec4DsOgCiiOAMJoLi4WF988YWuuOIKjjEDKYDiDMS5goIC3XLLLTr22GPVuXNn33EAxAATwoA4tnfvXn377be66aab1LRpU99xAMQInTMQp3bv3q3BgwerZcuW6tChg+84AGKIzhmIQ7t27dKyZcs0YsQIrvwFpCA6ZyDO7Nq1S0OGDFHr1q0pzECKonMG4kh+fr6WLFmi8ePHKy0tzXccAJ7QOQNxoqioSKNHj1bnzp0pzECKo3MG4sDGjRv1+eefa9KkSapdu7bvOAA8o3MGPHPO6R//+IcyMzMpzAAk0TkDXq1Zs0bvvPOOxo4d6zsKgDhC5wx44pzT1KlT1a9fP99RAMQZOmfAgxUrVmjKlCkaNmyY7ygA4hCdMxBj+/btU25urgYNGuQ7CoA4RXEGYmjRokUaO3asLrroItWrV893HABxiuIMxMj69eu1bds2jRs3zncUAHGOY86SsrKylJ2dfWA5NzdXGRkZHhMh2eTm5mrKlCm68847VasWn4kBVIx3CUnZ2dnKzc09sJyRkaH+/ft7TIRkMn/+fDVs2JDCDCBsdM5BGRkZysnJ8R0DSWbu3LmaOnWqbrvtNpmZ7zgAEgQf44EomTlzplq0aEFhBlBlFGcgChYvXqxPPvlE7du3pzADqDKKMxBh06dPV61atTR06FAKM4BqCas4m9m5ZrbEzJaaWbmXNDKz35mZM7MekYsIJI4NGzZo8eLF6ty5s+8oABJYpRPCzKy2pIclnSUpT9JsM5vqnFtYar1Gkm6Q9Hk0glZH6VOkysOpU4iE1157TUcccYRuuOEG31EAJLhwOuefS1rqnFvunCuQ9LykC8tYb5ykeyTtjWC+Gil9ilR5OHUKNbVnzx5t375dvXr18h0FQBII51SqtpJWhyznSTroHcjMuktq75x7w8xuiWC+GuMUKUTbc889p9WrV2vIkCG+owBIEuEU57JmtLgDD5rVkjRJ0lWVPpHZtZKulaRWrVodVDR37twZ8SKan58vSRRnRWd8Ie3atUvff/+9unXrxvhGCdtudDG+0VOTsQ2nOOdJah+y3E7S2pDlRpK6ScoJzkxtLWmqmV3gnJsT+kTOuSxJWZLUo0cPl5mZeeCxnJwchS5HQpMmTSQp4s+biKIxvqnuySefVLNmzTRs2DDGN4oY2+hifKOnJmMbTnGeLamTmR0laY2kvpIOHKB1zm2T1GL/spnlSLqldGEGksny5ct14oknMpEQQFRUOiHMOVck6XpJ70haJOkF59wCM7vdzC6IdkAg3jz88MNasGABhRlA1IR1bW3n3DRJ00r9bHQ562bWPBYQnz7++GNdcsklOvzww31HAZDEuEIYEKZ//vOfKiwspDADiDruSgVUwjmn559/Xtdcc43q1q3rOw6AFEDnDFQiOztbHTt2pDADiBk6Z6AcJSUleuCBB3TjjTeqdu3avuMASCF0zkA5pk+frl/+8pcUZgAxR3EGSikuLtbIkSN16qmnqnv37r7jAEhBFGcgRHFxsebOnavLLrtMhx56qO84AFIUxRkIKiws1ODBg9WhQwcde+yxvuMASGFMCAMk7du3T999952uv/56zmMG4B2dM1Le3r17NXjwYDVp0kRHH3207zgAQOeM1LZ7924tXbpUw4YNU5s2bXzHAQBJdM5IYXv37tWQIUN0+OGHU5gBxBU6Z6Sk7du3a968eRo/frwaN27sOw4AHITOGSmnpKREo0aNUpcuXSjMAOISnTNSyubNmzVjxgxNmjRJtWrx2RRAfOLdCSnlkUce0RlnnEFhBhDX6JyREtavX6///ve/GjVqlO8oAFAp2gckPeecXn/9dV1xxRW+owBAWOickdS+//57TZ48mY4ZQEKhc0bS2rt3r7755hsNGTLEdxQAqBKKM5LSt99+q9GjR+v8889X/fr1fccBgCqhOCPprF27Vtu2bdP48eNlZr7jAECVJdUx56ysLGVnZx9Yzs3NVUZGhsdEiLV58+bp2Wef1fjx41W7dm3fcQCgWpKqc87OzlZubu6B5YyMDPXv399jIsTS/Pnz1aBBA02YMIHCDCChJVXnLAUKck5Oju8YiLH58+frhRde0JgxY7jACICEx7sYEt5nn32mhg0bauzYsRRmAEmBdzIktOXLl+vDDz9Ux44dmfwFIGlQnJGw3n//fe3evVvDhw+nMANIKhRnJKQtW7Zo/vz56tatG4UZQNJJuglhSH5vvPGG0tLSdOONN/qOAgBRQeeMhLJ3715t2bJFp5xyiu8oABA1dM5IGC+88IIaNGigK6+80ncUAIgqijMSwvbt29W4cWOde+65vqMAQNRRnBH3/v3vf+vQQw/VJZdc4jsKAMQExRlx7bvvvtOJJ56o448/3ncUAIgZJoQhbj366KNauHAhhRlAyqFzRlz68MMPdfHFF6tFixa+owBAzNE5I+48/vjjKiwspDADSFl0zogbzjk9++yzuuqqq1SnDpsmgNRF54y48dJLL6ljx44UZgApj3dBeOec0/33368bbrhBdevW9R0HALyjc4Z3H374oU477TQKMwAEUZzhTUlJiUaOHKkePXqoR48evuMAQNxgtza8KC4u1rx589S3b181btzYdxwAiCt0zoi5wsJCDR06VC1btlS3bt18xwGAuEPnjJgqKCjQ0qVL9ac//Ult27b1HQcA4hKdM2Jm3759GjJkiA499FB16tTJdxwAiFsJX5yzsrKUmZmpzMxM5ebm+o6DcuzZs0eLFy/W4MGD1bFjR99xACCuJXxxzs7OPlCUMzIy1L9/f8+JUFphYaEGDx6sFi1asCsbAMKQFMecMzIylJOT4zsGyrBjxw7NnTtXEyZMUKNGjXzHAYCEkPCdM+KXc05jxoxR165dKcwAUAVJ0Tkj/mzdulXvvvuuJk6cqFq1+AwIAFXBuyaiIisrS2effTaFGQCqgc4ZEfXDDz/ohRde0NChQ31HAYCERVuDiHHO6c0339Tvf/9731EAIKHROSMi8vLylJWVpdtvv913FABIeHTOqLE9e/Zo/vz5GjFihO8oAJAUKM6okWXLlunWW2/VOeecowYNGviOAwBJgeKMasvLy9O2bdt09913y8x8xwGApBGXx5yzsrKUnZ0d1rq5ubnKyMiIciKUtmjRIj311FMaP3686tSJy80IABJWXHbOodfLrgzX0469BQsWqE6dOpowYQKFGQCiIG7fWblednxavHixsrOzNW7cOC4wAgBRwrsrwvbFF1+odu3auuOOOyjMABBFvMMiLHl5eXr77beVnp7O5C8AiLK43a2N+PHRRx+pUaNGGjVqFIUZAGKAzhkV2rFjh7766it1796dwgwAMULnjHK99dZbqlu3rm666SbfUQAgpdA5o0wFBQXauHGjzjzzTN9RACDl0DnjR1555RWVlJToyiuv9B0FAFISxRkH2bZtmw477DCdffbZvqMAQMqiOOOAZ599VrVq1eKKawDgGcUZkgJX/jrxxBPVtWtX31EAIOUxIQx64okntGDBAgozAMQJOucU9/777+uiiy5Ss2bNfEcBAATROaewyZMna9++fRRmAIgzdM4pavLkyerfvz+3fASAOETnnIKmTp2qI488ksIMAHEqrOJsZuea2RIzW2pmw8p4fJCZLTSzb8zsfTPrEPmoqCnnnO677z6dc845yszM9B0HAFCOSouzmdWW9LCk8yR1ldTPzEpP6/1KUg/n3AmSXpJ0T6SDouZmzpyp3r17q379+r6jAAAqEE7n/HNJS51zy51zBZKel3Rh6ArOuQ+dc7uDi7MktYtsTNRESUmJnnzySR177LHq1auX7zgAgEqEc9CxraTVIct5kip6h79a0ltlPWBm10q6VpJatWqlnJycA4/t3LnzwHJ+fr4kHfQ4qqe4uFirVq1Sz549NW/ePN9xklbo9ovIYmyji/GNnpqMbTjFuayb+LoyVzS7XFIPSaeV9bhzLktSliT16NHDhR73zMnJOXActEmTJpLEcdEaKioq0ogRI3TddddpxYoVjGcUhW6/iCzGNroY3+ipydiGs1s7T1L7kOV2ktaWXsnMzpR0q6QLnHP7qpUGEVNYWKilS5fq6quvVocOzM8DgEQSTnGeLamTmR1lZvUk9ZU0NXQFM+su6VEFCvMPkY+JqigoKNCQIUNUt25d/eQnP/EdBwBQRZXu1nbOFZnZ9ZLekVRb0pPOuQVmdrukOc65qZImSjpM0otmJkmrnHMXRDE3yrF3714tXrxYt9xyi9q2bes7DgCgGsK6CoVzbpqkaaV+Njrk+zMjnAvVUFxcrCFDhmjw4MEUZgBIYFwiKkns2rVLs2bN0oQJE9SwYUPfcQAANcDlO5PE7bffrm7dulGYASAJ0DknuPz8fL355pu66667FDzeDwBIcHTOCe6JJ57QeeedR2EGgCRC55ygNm3apMmTJ+vmm2/2HQUAEGF0zgnIOae3335bf/zjH31HAQBEAcU5waxdu1YjRozQ5ZdfrkaNGvmOAwCIAopzAtm1a5cWLlyo0aNHV74yACBhUZwTxMqVKzVixAidfvrpOuSQQ3zHAQBEEcU5AeTl5Sk/P18TJ05UrVr8kwFAsuOdPs59++23mjRpko477jjVq1fPdxwAQAxQnOPYwoULJUl333236tat6zkNACBWKM5xatmyZZo8ebKOOeYY1anD6egAkEooznHoyy+/1L59+zR+/HjVrl3bdxwAQIxRnOPMDz/8oNdff13HHnssk78AIEWxvzSOfPLJJ6pTp47GjBnjOwoAwCNaszixZ88ezZ49W7169fIdBQDgGZ1zHHj33XdVUFCggQMH+o4CAIgDdM6eFRYWasOGDerTp4/vKACAOEHn7NHUqVO1c+dOXX755b6jAADiCMXZk61bt6phw4a64IILfEcBAMQZirMHzz//vAoKCnTllVf6jgIAiEMU5xhbsGCBunfvrp/85Ce+owAA4hQTwmJo8uTJWrBgAYUZAFAhOucYmT59ui688EKlpaX5jgIAiHN0zjHw/PPPa9++fRRmAEBY6Jyj7Omnn9Zll13GLR8BAGGjc46it99+W+3ataMwAwCqhM45Cpxzuu+++/SXv/xFDRs29B0HAJBg6JwjzDmn2bNn6xe/+AWFGQBQLRTnCCopKdFtt92mI488Uv/3f//nOw4AIEFRnCOkpKRE3377rX7zm9+odevWvuMAABIYxTkCiouLNXz4cNWpU0cnnnii7zgAgATHhLAaKioq0rJly/T73/9e6enpvuMAAJIAnXMNFBYWasiQITIzdenSxXccAECSoHOupn379mnBggW6+eab1bZtW99xAABJhM65GkpKSjR06FA1b96cwgwAiDg65yravXu3ZsyYoQkTJuiQQw7xHQcAkITonKvozjvv1E9/+lMKMwAgauicw7R9+3a9+uqruuOOO2RmvuMAAJIYnXOYnnrqKfXp04fCDACIOjrnSmzZskWPP/64hgwZ4jsKACBF0DlXoKSkRO+++67+9Kc/+Y4CAEghFOdyrF+/XkOHDtWll16qtLQ033EAACmE4lyGHTt2aPHixRozZgzHmAEAMUdxLmXVqlUaMWKEevfuzf2YAQBeUJxDrF69Wvn5+br33ntVpw5z5QAAflCcg5YtW6ZJkyapS5cuql+/vu84AIAUFhftYVZWlh555BE1adJEkpSbm6uMjIyYvf7ixYslSXfffbfq1q0bs9cFAKAscdE5Z2dna+nSpQeWMzIy1L9//5i89qpVq/TUU0+pU6dOFGYAQFyIi85ZktLT05WTkxPT18zNzVWtWrU0YcIE1aoVF59TAACIj87Zh/z8fL366qvq1q0bhRkAEFfipnOOpVmzZqmgoEBjx471HQUAgB9JuZaxoKBAn332mU455RTfUQAAKFNKdc4ffPCB8vPzNXDgQN9RAAAoV8p0zoWFhVq3bp1++9vf+o4CAECFUqJzfvPNN7Vx40ZdddVVvqMAAFCppC/OmzZtUsOGDdWnTx/fUQAACEtSF+cXX3xRO3bs0B/+8AffUQAACFvSFudvvvlG3bt3V3p6uu8oAABUSVJOCHvuuec0b948CjMAICElXef81ltvqU+fPmrcuLHvKAAAVEtSFeeXX35ZtWrVojADABJa0hTnp59+Wv369eNezACAhJcUx5w/+OADtW7dmsIMAEgKCd05O+d0//3365prrlFaWprvOAAARETCds7OOX3zzTfq2bMnhRkAkFQSsjg75zRu3Dg1bdpUp556qu84AABEVMLt1i4pKdHy5ct13nnn6cgjj/QdBwCAiEuozrmkpEQjR45UYWGhevbs6TsOAABRkTCdc3FxsZYtW6bLL79cxx57rO84AABETUJ0zkVFRRo6dKiKi4vVtWtX33EAAIiquO+cCwsL9fXXX+vmm2/WEUcc4TsOAABRF9eds3NOw4YNU7NmzSjMAICUEbed8969e/Xee+/pzjvvVIMGDXzHAQAgZuK2c77nnnvUvXt3CjMAIOWEVZzN7FwzW2JmS81sWBmP1zezKcHHPzezjtUNtHPnTj3xxBMaNWqU2rZtW92nAQAgYVVanM2stqSHJZ0nqaukfmZWesr01ZK2OufSJU2SdHd1Az3zzDO64IILZGbVfQoAABJaOJ3zzyUtdc4td84VSHpe0oWl1rlQ0r+D378k6QyrYnUtKirSnXfeqb/85S9q2bJlVX4VAICkEk5xbitpdchyXvBnZa7jnCuStE1S86oE2blzp6677rqq/AoAAEkpnNnaZXXArhrryMyulXStJLVq1Uo5OTmSpBYtWigtLU25ublhxEF17Ny588B4I/IY3+hhbKOL8Y2emoxtOMU5T1L7kOV2ktaWs06emdWRlCZpS+kncs5lScqSpB49erjMzExJUmZmpnJycrR/GZHH+EYX4xs9jG10Mb7RU5OxDWe39mxJnczsKDOrJ6mvpKml1pkqaUDw+99J+sA596POGQAAVK7Sztk5V2Rm10t6R1JtSU865xaY2e2S5jjnpkp6QtIzZrZUgY65bzRDAwCQzMxXg2tmGyV9H/KjFpI2eQmTGhjf6GJ8o4exjS7GN3pKj20H51xYpyN5K86lmdkc51wP3zmSFeMbXYxv9DC20cX4Rk9NxjZuL98JAECqojgDABBn4qk4Z/kOkOQY3+hifKOHsY0uxjd6qj22cXPMGQAABMRT5wwAAOShOMfy9pOpKIzxHWRmC83sGzN738w6+MiZiCob25D1fmdmzsyYAVsF4YyvmV0a3H4XmFl2rDMmqjDeF440sw/N7Kvge8OvfORMRGb2pJn9YGbzy3nczOyh4Nh/Y2YnhvXEzrmYfSlwEZNlko6WVE/S15K6llrnr5L+Ffy+r6QpscyYyF9hju8vJR0a/P4vjG/kxja4XiNJMyTNktTDd+5E+Qpz2+0k6StJTYPLh/vOnQhfYY5tlqS/BL/vKmml79yJ8iXpVEknSppfzuO/kvSWAvegOEnS5+E8b6w755jcfjKFVTq+zrkPnXO7g4uzFLhWOioXzrYrSeMk3SNpbyzDJYFwxvePkh52zm2VJOfcDzHOmKjCGVsnqXHw+zT9+P4JKIdzbobKuJdEiAslTXYBsyQ1MbMjKnveWBfnmNx+MoWFM76hrlbgEx0qV+nYmll3Se2dc2/EMliSCGfb7Syps5nNNLNZZnZuzNIltnDGdoyky80sT9I0SX+LTbSUUNX3ZUnh3ZUqkiJ2+0mUKeyxM7PLJfWQdFpUEyWPCsfWzGpJmiTpqlgFSjLhbLt1FNi1nanAHp+Pzaybcy4/ytkSXThj20/S0865+8zsFwrcK6Gbc64k+vGSXrVqWqw756rcflIV3X4SZQpnfGVmZ0q6VdIFzrl9McqW6Cob20aSuknKMbOVChxbmsqksLCF+97wX+dcoXNuhaQlChRrVCycsb1a0guS5Jz7TFIDBa4LjZoL6325tFgXZ24/GV2Vjm9w1+ujChRmjtmFr8Kxdc5tc861cM51dM51VOB4/gXOuTl+4iaccN4bXlNgQqPMrIUCu7mXxzRlYgpnbFdJOkOSzOxYBYrzxpimTF5TJV0ZnLV9kqRtzrl1lf1STHdrO24/GVVhju9ESYdJejE4z26Vc+4Cb6ETRJhji2oKc3zfkXS2mS2UVCxpsHNus7/UiSHMsb1Z0mNmNlCBXa5X0RSFx8yeU+BQS4vgMfvbJNWVJOfcvxQ4hv8rSUsl7Zb0+7Cel/EHACC+cIUwAADiDMUZAIA4Q3EGACDOUJwBAIgzFGcAAOIMxRkAgDhDcQYAIM5QnAEAiDP/H1x2Do6PrByuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a3029d668>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a303dfe80>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"acc\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_acc\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a304cbf60>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X10VfWd7/H3FwiEZwigoEwbrNiKEWKaKsda8BFHnYq6HOsDI9raOLpux9ZFBdpZfbRq0LGU1TsqxXr1whV7VcTraGOltDgVoQEUKRRjW2gjBCNUigLV4O/+sfeJ52GfnJPz/PB5rbXXOft3ftn7l52c797797TNOYeIiFSWPoUugIiI5J+Cv4hIBVLwFxGpQAr+IiIVSMFfRKQCKfiLiFQgBX8RkQqk4C8iUoEU/EVEKlC/QhcgkdGjR7va2tpCF0NEpKRs2LDhbefcmGT5ijb419bW0traWuhiiIiUFDPbmUo+VfuIiFQgBX8RkQqk4C8iUoGKts5fRPLjgw8+oL29ncOHDxe6KNIL1dXVjB8/nqqqqrR+XsFfpMK1t7czdOhQamtrMbNCF0dS4Jxj7969tLe3M2HChLS2oWofkQp3+PBhRo0apcBfQsyMUaNGZXS3VpbBf+1auPNO71VEklPgLz2Z/s3KrtqnpQUuugg+/BCqquBXv4JQqNClEhEpLmV35f/YY3DkCDgH778PjzxS6BKJSE/27t1LfX099fX1jB07lmOPPbZ7/f33309pG9dffz3bt29PeZ9Llizhq1/9arpFLgtld+X/hz9Er2/dWphyiEhqRo0axSuvvALAd77zHYYMGcKcOXOi8jjncM7Rp0/w9epDDz2U83KWm7K78u/sjF5//fXClEOkrOWhYe2NN96grq6Of/3Xf6WhoYHdu3fT1NREY2MjJ510Et/73ve6855xxhm88sordHV1MWLECObNm8eUKVMIhUK89dZbKe9z6dKlnHzyydTV1fGNb3wDgK6uLv7lX/6lO33RokUA/PCHP2TSpElMmTKFWbNmZfeXz4OMrvzNrAZ4DKgFdgBXOOf+miDvMGAbsMI59z8y2W9PPvlJ2Lbto/WODli8GJqacrVHkTLy1a+CfxWe0P79sHmz17DWpw9MngzDhyfOX18PCxemVZytW7fy0EMPcf/99wNw1113UVNTQ1dXF2eddRaXX345kyZNiinefqZPn85dd93Frbfeyk9/+lPmzZuXdF/t7e38+7//O62trQwfPpxzzz2XZ555hjFjxvD222/z2muvAfDOO+8AsGDBAnbu3En//v2700pJplf+84BVzrmJwCp/PZHvA7/OcH9J3TbnQ+BDwHWnpfl/JyJB9u/3Aj94r/v352xXn/jEJ/jMZz7Tvf7oo4/S0NBAQ0MD27ZtY2tAve7AgQO54IILAPj0pz/Njh07UtrXunXrOPvssxk9ejRVVVVcffXVrFmzhuOPP57t27dzyy230NLSwnD/RHfSSScxa9Ysli1blvZAq0LKtM5/JnCm//5h4FfA3NhMZvZp4Gjg50BjhvvsUWhNM2O5lg6O8VMce/aoG5tISlK5Ulq7Fs45x+tR0b8/LFuWsy51gwcP7n7f1tbGj370I9avX8+IESOYNWtWYD/3/v37d7/v27cvXV1dKe3LOReYPmrUKDZv3sxzzz3HokWLeOKJJ1i8eDEtLS38+te/ZuXKldx+++1s2bKFvn379vI3LJxMr/yPds7tBvBfj4rNYGZ9gP8Avp5sY2bWZGatZtbaGVt5n6qnnmIq66KS9u3zqn5EJAtCIVi1Cr7/fe81T32p//a3vzF06FCGDRvG7t27aWlpyer2p06dyurVq9m7dy9dXV0sX76c6dOn09nZiXOOf/7nf+a73/0uGzdu5MiRI7S3t3P22Wdz991309nZycGDB7NanlxLeuVvZi8AYwM++maK+7gZeNY595dkgxKcc4uBxQCNjY3Bp+Fkqqu5jbt5ipl45zZvn3fcoXp/kawJhfI+gKahoYFJkyZRV1fHcccdx2c/+9mMtvfggw/y+OOPd6+3trbyve99jzPPPBPnHJ///Oe56KKL2LhxI1/60pdwzmFmNDc309XVxdVXX82BAwf48MMPmTt3LkOHDs30V8wrS3Srk9IPm20HznTO7TazccCvnHOfjMmzDPgcXkX8EKA/8J/OuR5bYBobG11aD3O59FJ46ikm8Ad2MIFw8Ad46SUN+BKJtW3bNk488cRCF0PSEPS3M7MNzrmk1euZVvs8Dcz2388GVsZmcM5d45z7mHOuFpgDPJIs8GdkrHeTMp87wyXo/mjBgpztVUSkpGQa/O8CzjOzNuA8fx0zazSzJZkWLi3XXgtAE0uo4e2oj9asKUSBRESKT0bB3zm31zl3jnNuov+6z09vdc7dEJD/f+Wyjz/g1evU1wMwlj1RH6nhV0TEU3YjfAGorQXgFn7kJ6jPv4hIpPIM/n69f1DVz549QT8gIlJZyjP4+/X+ANN4MeojVf2IiJRr8A+FwJ/v4zbuJna6hzvuKEyxRCTemWeeGTdga+HChdx88809/tyQIUMA2LVrF5dffnnCbSfrMr5w4cKoAVoXXnhhVubq+c53vsM999yT8XZypTyDP8AttwAQ4mXG0hH10c6desqXSLG46qqrWL58eVTa8uXLueqqq1L6+WOOOSZqsFZvxQb/Z599lhEjRqS9vVJRvsG/qam77j92ugdQn3+RTGRzRufLL7+cZ555hr///e8A7Nixg127dnHGGWfw7rvvcs4559DQ0MDJJ5/MypVxQ4nYsWMHdXV1ABw6dIgrr7ySyZMn84UvfIFDhw5157vpppu6p4P+9re/DcCiRYvYtWsXZ511FmeddRYAtbW1vP2211Z47733UldXR11dHQv93iI7duzgxBNP5Mtf/jInnXQSM2bMiNpPMkHbfO+997jooouYMmUKdXV1PPbYYwDMmzePSZMmMXny5LhnHGSq7B7mEuWEE6Cjw5/u4RI/0Rvxqz7/IvEKMaPzqFGjOPXUU/n5z3/OzJkzWb58OV/4whcwM6qrq1mxYgXDhg3j7bffZurUqVx88cUJn1973333MWjQIDZv3szmzZtpaGjo/uwHP/gBNTU1HDlyhHPOOYfNmzfzb//2b9x7772sXr2a0aNHR21rw4YNPPTQQ6xbtw7nHKeddhrTp09n5MiRtLW18eijj/KTn/yEK664gieeeCKlOf0TbfOPf/wjxxxzDP/1X//lH+P97Nu3jxUrVvD73/8eM8v6tNHle+UP3fX+IV6mlh1RH6nhVyQ9uZjRObLqJ7LKxznHN77xDSZPnsy5557Lm2++yZ4euuytWbOmOwhPnjyZyZMnd3/2s5/9jIaGBk455RR+97vfBU4HHem///u/ufTSSxk8eDBDhgzhsssu48UXvQ4kEyZMoN4fT9SbaaMTbfPkk0/mhRdeYO7cubz44osMHz6cYcOGUV1dzQ033MCTTz7JoEGDUtpHqsr7yv/aa8F/CMR87uRGFuM1/GqyN5EghZrR+ZJLLuHWW29l48aNHDp0qPuKfdmyZXR2drJhwwaqqqqora0NnMY5UtBdwZ/+9Cfuuecefvvb3zJy5Eiuu+66pNvpad6zAQMGdL/v27dvytU+ibZ5wgknsGHDBp599lnmz5/PjBkz+Na3vsX69etZtWoVy5cv58c//jG//OUvU9pPKsr7yj8UgokTgeA+/2r4Fem9XMzoPGTIEM4880y++MUvRjX07t+/n6OOOoqqqipWr17Nzp07e9zOtGnTWLZsGQBbtmxh8+bNgDcd9ODBgxk+fDh79uzhueee6/6ZoUOHcuDAgcBtPfXUUxw8eJD33nuPFStW8LnPfS6j3zPRNnft2sWgQYOYNWsWc+bMYePGjbz77rvs37+fCy+8kIULF3Y/5zhbyvvKH2DkyO6303iRp7g06uMFC2DFinwXSqS05WJG56uuuorLLrssqufPNddcw+c//3kaGxupr6/nU5/6VI/buOmmm7j++uuZPHky9fX1nHrqqQBMmTKFU045hZNOOiluOuimpiYuuOACxo0bx+rVq7vTGxoauO6667q3ccMNN3DKKaekXMUDcPvtt3c36oL3qMigbba0tPD1r3+dPn36UFVVxX333ceBAweYOXMmhw8fxjnHD3/4w5T3m4qMpnTOpbSndI61eDHceCMAa5nK6fwGr9rHuzX8+MehF39LkbKjKZ1LVyGndC5+TU1QUwN4Db/1RN86qepHRCpR+Qd/6O7vD+rzLyIClRL8/dG+ANfyCLHTPajPv1S6Yq3+lcQy/ZtVRvCPGO3r9fmP7jGgPv9Syaqrq9m7d69OACXEOcfevXuprq5Oexvl39snzB/tC+rzLxJp/PjxtLe309nZWeiiSC9UV1czfvz4tH++coL/pEnd9TtNLGE+d7CPj4Zzhxt+9YB3qTRVVVVMmDCh0MWQPKuMah+ImuMf4uf5BzX8ikjlqJzgHwp1P94RwvP8O9TwKyKVqHKCP3Q/2B002ZuIVLbKCv633Ra1Op8747LoKV8iUgkqK/jHVP14k73ti8qiEb8iUgkqK/hDVNUPwDR+TWS9P6jhV0TKX+UF/5iqn48afj/y8st5LI+ISAFUXvAPhaLm+gnxMrX256gsHR2q+hGR8lZ5wR9g6tSo1fnuB6jqR0QqSWUG/5iqnyaWUNM3+uHI6vMvIuWsMoN/xOMdw8b2iZ7XRH3+RaScVWbwh6jHOwLc8kF8w++3v53H8oiI5FHlBv8vfSlqtYkljB0Q3ee/o0NX/yJSnio3+Ec83jHsu/3jR/xGPHtZRKRsZBT8zazGzH5hZm3+68gE+Y6Y2Sv+8nQm+8yqiC6fAE0H/oOaoe9Hpe3Zk88CiYjkR6ZX/vOAVc65icAqfz3IIedcvb9cnOE+syfi8Y5h04ZsjFpXw6+IlKNMg/9M4GH//cPAJRluL78iHu8YdpuL7+Cvyd5EpNxkGvyPds7tBvBfj0qQr9rMWs3sZTNLeIIwsyY/X2veHil3wglRq6E9TzG25u9RaZrsTUTKTdLgb2YvmNmWgGVmL/bzMedcI3A1sNDMPhGUyTm32DnX6JxrHDNmTC82n4FJk2ILwdRhW+OyacSviJSTpMHfOXeuc64uYFkJ7DGzcQD+61sJtrHLf/0j8CvglKz9BpmKebwjwG0jfhKXtmlTPgojIpIfmVb7PA3M9t/PBlbGZjCzkWY2wH8/GvgsEH9pXSgxc/wDhDpWxM78rKofESkrmQb/u4DzzKwNOM9fx8wazWyJn+dEoNXMXgVWA3c554on+EPcHP90dDB12O/isqnqR0TKhTnnkucqgMbGRtfa2pqfna1dC6efHp104hc5fduDUWk1NbB3b36KJCKSDjPb4Lex9qhyR/hGCpjoLdT1YmxtkPr8i0jZUPAPi5nojbY25l/xh7hs6vMvIuVAwT8sZqI3gKbX58RO/6OGXxEpCwr+YQGjfdm0iWnT4rM+8kh+iiQikisK/pFiHu/Izp3cdsFrcdn0gHcRKXUK/pFiHu8IEHruW3ENv6+8oqofESltCv6RAgZ8sWlT3DAAUJ9/ESltCv6xPvax6PU//zmw6kcPeBeRUqbgHytgorfQpv9Un38RKSsK/rGuvRbMotM6Opg/Pz6r+vyLSKlS8I8VCsGUKdFpO3YEPfJXff5FpGQp+Afp3z96/dVXYe3awD7/N9+cnyKJiGSTgn+Q2NG+zsEjjwT1BFW3TxEpSQr+QZqa4iZ6Y+vWwBohULdPESk9Cv6J9OsXvb5zJwD33RefVd0+RaTUKPgn8slPRq/7rbtB48DU7VNESo2CfyJBFfx+/Y66fYpIqVPwTyTBVA+Aun2KSMlT8O9JwFQP4Qgf1O1z3rw8lElEJAsU/HsSMNVDeDL/oFqhNWt09S8ipUHBvycJpnqA4Foh0NW/iJQGBf+eJJjqISyo4VdX/yJSChT8k0kw1QN4Db9Bg7405YOIFDsF/2QSTPUQFjToS1M+iEixU/BPJsFUD2GJpnyYPTvH5RIRyYCCfypip3p4/fWo1aCr/7Y2jfoVkeKl4J+K2KkeOjqiInuiq/+gBmERkWKg4J+KoE79Dz4YtRp09a85f0SkWCn4pyIUgvr66LT334/Lcs018T/6ta/lsFwiImlS8E9V7IiuiC6fYUuXwsCB0dkOHoTTTstt0UREekvBP1Vjx0avx3T5DPvKV+J/dP16Vf+ISHFR8E9VD1M9RGpuhjFj4n98zpwclUtEJA0ZBX8zqzGzX5hZm/86MkG+j5nZ82a2zcy2mlltJvstiCRTPURauTI+7cABOP/87BdLRCQdmV75zwNWOecmAqv89SCPAHc7504ETgXeynC/hRE71UOCobyJGn+ffx7mzs1R2UREeiHT4D8TeNh//zBwSWwGM5sE9HPO/QLAOfeuc+5ghvstjNipHiDh09uXLoVhw4Kza+oHESm0TIP/0c653QD+61EBeU4A3jGzJ81sk5ndbWZ9M9xvYTQ1xTf8bt+eMPvddwenX3FFFsskIpKGpMHfzF4wsy0By8wU99EP+BwwB/gMcBxwXYJ9NZlZq5m1dnZ2prj5PDvhhOj1rq6EWZuaYMaM+PT2dnX/FCk1a9fC9OkwYIDX9yOXy6BBua8iThr8nXPnOufqApaVwB4zGwfgvwbV5bcDm5xzf3TOdQFPAQ0J9rXYOdfonGscE9RlphjEPt0rySQ+LS0wfnx8+vr1agAWyZe1a73rtkwC8umne8/riBnfmROHDnlVxLk8AWRa7fM0EJ6/cjYQ0M+F3wIjzSwczc8GtgbkKw3XXhufFjPVQ6yf/Sw4/fnnYdasLJRJpAIsXgyjRqUfuNvaCv0b9N6TT+Zu25kG/7uA88ysDTjPX8fMGs1sCYBz7ghelc8qM3sNMOAnGe63cEKh+Cme//rXpD8SND0QwLJlOgFIZeptML/xRm++rEpy2WW527Y553K39Qw0Nja61tbWQhcj2GmnefU2YWbwm994Ub4Hs2Z5wT7IqafCunVZLKNIAS1e7M1qW2nBOlsGDvRmC2hu7v3PmtkG51xjsnwa4ZuOJE/3SmTp0uAGYPDOJePGZaFsInmQrA69XK/S+/TxOvw98ID3tc/VcvBgeoG/V79LbjdfppI83asnLS3eVX6Qjg6vlV/jACTfetsgWop16P36eYMvMwnKR47A7t1eCCh1Cv7pSvJ0r56sW5f4DuDQIe+LpZ5Ako5Zs7x/zXJtEB040Gs/Sydwf/CBd/ctHgX/dCV5ulcyLS2JG4HB6wnUr5+mg6hU6fZsWbbMuzotFb0N5vmoDqkUCv7pSuHpXsk0N8NLL8U/AyDsyBGvr2+fProTKAdz53p/60rp2ZJKYFcwLxwF/3Sl0eUz0WYOHoyfNSKSc96dgBlUValraDFK5Up9wQI4fLjQJc2eZHXoCuzFTcE/EyNjZrBua0u7tXb37sTtAJG6urxb+3BA6dtXdwXZlk6VSzlcqfe2QVR16KVNwT8TvZjlMxUtLV41UG9mtvjww4/uCiKXcj0pnH++Vw2WyTD9cg7kffrA8cd7/0dqEJWeaJBXpsaNi36i19ix3mV8hjRIRjIZ6COVS4O88mXq1Oj1Xvb6SaSpCfbu9a7IUqkOktJQU5P6ACHVmUsuKfhnKgu9fpJpafGCwUsvQX29d2svxSeV3i1795bHACEpfQojmQrq9bNrV852tWmT1wU0HExmzPDqqSW70hlMpCt1KSUK/tkQ2+unvT0rVT+paGnxGn1jA1G5nhT69PF+t2zNoaJALpVKwT8bgnr9ZLnqp7cSnRRKfTlyxPvdRCQzCv7Z0NQExx4bnZajqh8RkWxQ8M+W2OCfx6ofEZHeUvDPlqCqn4UL818OEZEUKPhnS1OT14k70p49hSmLiEgSCv7ZNG1a9Pq+far6EZGipOCfTUEDvu64I//lEBFJQsE/m0Kh+LmZd+7UcxlFpOgo+Gdb7Fw/kNFMnyIiuaDgn21BVT8vv5z/coiI9EDBP9tCIaitjU7r6FDVj4gUFQX/XJg/Pz5NVT8iUkQU/HMhqM//mjWFKYuISAAF/1yJ7fWjPv8iUkQU/HPlllvi09TnX0SKhIJ/rgRV/ajPv4gUCQX/XIqd7gHU8CsiRUHBP5eC+vyr4VdEioCCfy4F9flXw6+IFIGMgr+Z1ZjZL8yszX8dGZDnLDN7JWI5bGaXZLLfkhLU518NvyJSYJle+c8DVjnnJgKr/PUozrnVzrl651w9cDZwEHg+w/2WDjX8ikgRyjT4zwQe9t8/DCS7or8ceM45dzDD/ZYWNfyKSJHJNPgf7ZzbDeC/HpUk/5XAoxnus/So4VdEikzS4G9mL5jZloBlZm92ZGbjgJOBlh7yNJlZq5m1dnZ29mbzxU0NvyJSZJIGf+fcuc65uoBlJbDHD+rh4P5WD5u6AljhnPugh30tds41Oucax4wZ09vfpbgFNfwGpYmI5EGm1T5PA7P997OBlT3kvYpKrPIJa2qCIUOi03T1LyIFkmnwvws4z8zagPP8dcys0cyWhDOZWS3wD8CvM9xfabv55vg0Xf2LSAGYc67QZQjU2NjoWltbC12M7Bs0CA4dik574AHvzkBEJENmtsE515gsn0b45ttXvhKfpqt/EckzBf98a26GgQOj0/btg7lzC1MeEalICv6FEHT1v2CBRv2KSN4o+BdCc3P8lA8As2fHp4mI5ICCf6HceWd8Wlubun6KSF4o+BdKUxMcf3x8+pw5+S+LiFQcBf9CeuSR+LQDB2DWrPyXRUQqioJ/IYVCcM018enLlqnxV0RySsG/0JYuhWHD4tNn9mrePBGRXlHwLwZ33x2f1tkJ55+f/7KISEVQ8C8GTU1w6qnx6c8/r8FfIpITCv7FYt06GDw4Pl2Dv0QkBxT8i8m99wanX3hhfsshImVPwb+YNDXBjBnx6e+8A5Mm5b88IlK2FPyLTUsLnHhifPq2bWoAFpGsUfAvRlu3wogR8enPP68BYCKSFQr+xerZZ4PTly3TCUBEMqbgX6xCIbjttuDPdAIQkQwp+Bez5ubgBmDwTgBqAxCRNCn4F7uWluABYOC1AagXkIikQcG/FKxbl/gEsG0bjBuX3/KISMlT8C8V69YlrgLq6ICqKj0IRkRSpuBfSlpagqeABujqghtvhNNOy2+ZRKQkKfiXmqVLE/cCAli/HgYM0F2AiPRIwb8UNTfDSy/BwIHBn7//vncXoMZgEUlAwb9UhUJw8CB8/OOJ82zbBn36aEyAiMRR8C91O3YkbgcAcM4bE6CTgIhEUPAvB0uXetVAQfMBhYVPAmYwcaKeESBS4RT8y0UoBH/9q9cYbNZz3jfegNNPh+pqPSlMpEIp+Jeb5mb48MPEg8Ii/f3v3pPCdDcgUnEU/MvVunVeVdDEianlD98N9OmjOYNEKoCCfzkLheD11736/kSjg2M5580ZZOadCHRHIFKWMgr+ZlZjZr8wszb/dWSCfAvM7Hdmts3MFpklq5SWrGtp8QL7NdckbxMIc+6jOwIzGDVKg8dEykSmV/7zgFXOuYnAKn89ipmdDnwWmAzUAZ8Bpme4X0nX0qVem8ADD0BNTe9+dt8+b/CYGQwapMZikRKWafCfCTzsv38YuCQgjwOqgf7AAKAK2JPhfiVTTU2wd+9HdwN9+/bu5w8d+qixuKpKYwhESkymwf9o59xuAP/1qNgMzrm1wGpgt7+0OOe2ZbhfyaalS72J4cJtA72tlevq+mgMgdoJREpC0uBvZi+Y2ZaAZWYqOzCz44ETgfHAscDZZjYtQd4mM2s1s9bOzs7e/B6SLS0tXrWQc96Ygerq3v18bDtB377qPSRShJIGf+fcuc65uoBlJbDHzMYB+K9vBWziUuBl59y7zrl3geeAqQn2tdg51+icaxwzZkz6v5VkR3OzV72T7okAvBNJuPeQGo1Fikam1T5PA7P997OBlQF5/gxMN7N+ZlaF19irap9SE3kiSKexOCyy0VhtBSIFk2nwvws4z8zagPP8dcys0cyW+HkeB/4AvAa8CrzqnPt/Ge5XCimysbg3A8liRbYVmHnPIdDJQCQvzDlX6DIEamxsdK2trYUuhvTW3LmwaBEcPpz5tmpq4M47vZONiKTEzDY45xqT5dMIX8muyOqhdHsPhUVWEakXkUhWKfhLbmXaeygstheRBpqJZETBX/In8q4gk7aCsMiBZupWKtIrCv5SGJGTzmXrZBDbrVS9iUQSUvCX4hB7MsikiigstjeRqolEuin4S3GKbThOZ/6hWLHVRLozkAqm4C+lIXL+ofBAs7FjvV5A6Yq9M9AJQSqIgr+UpqYm2L0bjhzJTrfSsKATghqSpQwp+Ev5iOxWmo0G5LDYhuTworEHUsIU/KU8xTYgZzIfUSJBYw90YpASoeAvlSFyPqJs3xkE6enEoN5HUgQU/KUyxd4Z5OOEECu291HsnYOmv5YcUvAXCQs6IYQbkjPpVZQO56LnNkq0qDFa0qTgL5JMS0t0r6Jsjj3IVKLG6ERLv35QX6+2CFHwF0lb7NiD2BNDVVWhSxjvyBF49dWe2yKCFlVBlR0Ff5FcWLoU3n8/+MSQq95HuZRKFVRQu8X48XDTTbrTKEIK/iKFENv7KHbJxtxGheYcvPkm3H9/7+801HU25xT8RYpR7NxGiZZsjGouBal0ne3Nom62Cv4iJS1yVHOy5aWXvMbefPdcKkY9dbOtkLYP/ReIVIpQCDZtCu651NNSDlVQ2ZJO20eRVl8p+ItIz1KtgiqVHk+FlGr1VR6qpRT8RSQ3kvV46u2JpNBjKvIpXC2VwxOAgr+IFL+exlT0dimlbrZPPpmzTSv4i0hlSdbNtpjaPi67LGebVvAXEUlFum0f6ZxABg708jY35+zXMedczjaeicbGRtfa2lroYoiIlBQz2+Cca0yWT1f+IiIVSMFfRKQCKfiLiFQgBX8RkQqk4C8iUoEU/EVEKlDRdvU0s05gZwabGA28naXi5EKxlw+Kv4zFXj5QGbOh2MsHxVXGjzvnxiTLVLTBP1Nm1ppKX9dCKfbyQfGXsdjLBypjNhR7+aA0yhhL1T4iIhVIwV9EpAKVc/Av9sftFHv5oPh8CIHeAAAE/0lEQVTLWOzlA5UxG4q9fFAaZYxStnX+IiKSWDlf+YuISAJlF/zN7B/NbLuZvWFm8wpYjn8ws9Vmts3Mfmdmt/jpNWb2CzNr819H+ulmZov8cm82s4Y8lbOvmW0ys2f89Qlmts4v32Nm1t9PH+Cvv+F/Xpun8o0ws8fN7Pf+sQwV0zE0s6/5f98tZvaomVUX+hia2U/N7C0z2xKR1utjZmaz/fxtZjY7D2W82/87bzazFWY2IuKz+X4Zt5vZ+RHpOfm+B5Uv4rM5ZubMbLS/XpBjmDHnXNksQF/gD8BxQH/gVWBSgcoyDmjw3w8FXgcmAQuAeX76PKDZf38h8BxgwFRgXZ7KeSvwf4Bn/PWfAVf67+8HbvLf3wzc77+/EngsT+V7GLjBf98fGFEsxxA4FvgTMDDi2F1X6GMITAMagC0Rab06ZkAN8Ef/daT/fmSOyzgD6Oe/b44o4yT/uzwAmOB/x/vm8vseVD4//R+AFrwxSKMLeQwz/h0LXYCs/jIQAloi1ucD8wtdLr8sK4HzgO3AOD9tHLDdf/8AcFVE/u58OSzTeGAVcDbwjP/P+3bEF7D7ePr/8CH/fT8/n+W4fMP84Gox6UVxDPGC/1/8L3c//xieXwzHEKiNCay9OmbAVcADEelR+XJRxpjPLgWW+e+jvsfh45jr73tQ+YDHgSnADj4K/gU7hpks5VbtE/4yhrX7aQXl396fAqwDjnbO7QbwX4/ysxWi7AuB24AP/fVRwDvOua6AMnSXz/98v58/l44DOoGH/KqpJWY2mCI5hs65N4F7gD8Du/GOyQaK6xiG9faYFfq79EW8q2l6KEtey2hmFwNvOudejfmoKMrXW+UW/C0graDdmcxsCPAE8FXn3N96yhqQlrOym9k/AW855zakWIZCHNt+eLfe9znnTgHew6uySCTfx3AkMBOvKuIYYDBwQQ9lKLr/TxKXqWBlNbNvAl3AsnBSgrLkrYxmNgj4JvCtoI8TlKMY/97dyi34t+PVyYWNB3YVqCyYWRVe4F/mnHvST95jZuP8z8cBb/np+S77Z4GLzWwHsByv6mchMMLM+gWUobt8/ufDgX05LF94n+3OuXX++uN4J4NiOYbnAn9yznU65z4AngROp7iOYVhvj1lBvkt+o+g/Adc4v66kSMr4CbyT/Kv+d2Y8sNHMxhZJ+Xqt3IL/b4GJfm+L/niNak8XoiBmZsCDwDbn3L0RHz0NhFv9Z+O1BYTTr/V7DkwF9odv03PBOTffOTfeOVeLd5x+6Zy7BlgNXJ6gfOFyX+7nz+lVjHOuA/iLmX3STzoH2EqRHEO86p6pZjbI/3uHy1c0xzBCb49ZCzDDzEb6dzgz/LScMbN/BOYCFzvnDsaU/Uq/t9QEYCKwnjx+351zrznnjnLO1frfmXa8Dh0dFNEx7JVCNzpke8FreX8drxfANwtYjjPwbvE2A6/4y4V4dbyrgDb/tcbPb8D/9Mv9GtCYx7KeyUe9fY7D+2K9AfxfYICfXu2vv+F/flyeylYPtPrH8Sm8XhNFcwyB7wK/B7YA/xuvR0pBjyHwKF4bxAd4QepL6RwzvHr3N/zl+jyU8Q28OvLw9+X+iPzf9Mu4HbggIj0n3/eg8sV8voOPGnwLcgwzXTTCV0SkApVbtY+IiKRAwV9EpAIp+IuIVCAFfxGRCqTgLyJSgRT8RUQqkIK/iEgFUvAXEalA/x/YIEkWiAMSwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.log(run_hist_2.history[\"loss\"]),'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(np.log(run_hist_2.history[\"val_loss\"]),'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
